[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD student at the Center for Brain and Cognition, at Universitat Pompeu Fabra (Barcelona, Spain). I study how bilingual toddlers develop their vocabulary, using both experimental and observational techniques. Learning new things about programming languages brings me joy. I mainly use Twitter for sharing research outputs, programming tips or data visualisations. Don’t hesitate to get in touch on Twitter (@gongcastro) or at gonzalo.garciadecastro@upf.edu!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Getting the most out of logistic regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nrenv (o cómo usar paquetes de R sin ataques de pánico)\n\n\n\n\n\nrenv es un paquete de R que permite instalar paquetes de R gestionar sus versiones para proyectos de forma independiente. Aqui resumo para qué se utiliza y cómo funciona.\n\n\n\n\n\n\nFeb 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\n@psicotuiterbot: Un bot de Twitter para Psicotuiter\n\n\n\n\n\nHe creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON.\n\n\n\n\n\n\nDec 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\nCreando un paquete de R: una guía informal (I)\n\n\n\n\n\nEn el canal de Twitch de Alicia, Psicometries hicimos un directo en el que explicamos cómo se ha creado el paquete de R {psicotuiteR}, indicando cada paso lo mejor que hemos podido para que puedas replicarlo, contribuir al mismo paquete o incluso crear tu propio paquete en el futuro.\n\n\n\n\n\n\nNov 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nVisualising polynomial regression\n\n\n\n\n\nThe outputs of polynomial regression can be difficult to interpret. I generated some animated plots to see how model predictions change across different combinations of coefficients for 1st, 2nd, and 3rd degree polynomials.\n\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow similar is the word “mask” across languages?\n\n\n\n\n\nUsing the Levenshtein distance to quantify the orthographic and phonlogical similarity between translation equivalents of the word mask across multiple languages.\n\n\n\n\n\n\nNov 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nImporting data from multiple files simultaneously in R\n\n\n\n\n\nA comparison between base R and Tidyverse methods for importing data from multiple files\n\n\n\n\n\n\nJul 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nA primer on Mixed-Effects Models: Theory and practice\n\n\n\n\n\nSlides from a tutorial on mixed-effects models I presented to my research group.\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Click here for PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\nOctober 2018 - Present\nPhD, Biomedicine; Pompeu Fabra University (Barcelona, Spain); Supervisor: Núria Sebastian-Galles.\nOctober 2017 - July 2018\n\nMSc, Neurosciences; University of Barcelona (Barcelona, Spain). Dissertation: Phonemic Contrast Perception: A Segmentation Study on Monolingual and Bilingual Infants; Supervisors: Núria Sebastian-Galles and Chiara Santolin.\n\nSeptember 2013 - July 2017\n\nBSc, Psychology; University of Oviedo (Oviedo, Spain). Dissertation: Effects of Environmental Enrichment on Attention, Spatial Reference Memory, and Cytochrome C Oxidase Activity; Supervisors: Azucena Begega and Marcelino Cuesta."
  },
  {
    "objectID": "cv.html#contributions",
    "href": "cv.html#contributions",
    "title": "CV",
    "section": "Contributions",
    "text": "Contributions\n\nArticles\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., Saffran, J. (2020). Experience with research paradigms relates to infants’ direction of preference. Infancy, 00, 1–8. https://doi.org/10.1111/infa.12372 [preprint] [OSF] [code]\nSampedro-Piquero, P., Álvarez-Suárez, P., Moreno-Fernández, R., García-Castro, G., Cuesta, M., Begega, A. (2018). Environmental enrichment results in both brain connectivity efficiency and selective improvement in different behavioral tasks. Neuroscience, 388, 374-383. https://doi.org/10.1016/j.neuroscience.2018.07.036\n\n\nPreprints\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 4). Experience with research paradigms relates to infants’ direction of preference. https://doi.org/10.31234/osf.io/xgvbh"
  },
  {
    "objectID": "cv.html#proceedings",
    "href": "cv.html#proceedings",
    "title": "CV",
    "section": "Proceedings",
    "text": "Proceedings\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (Sempember, 2021). A study of lingusitic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presented at the Boston University Conference in Language Development, held online (Boston, United States).\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., Plunkett, K. (September, 2021). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Presentation at the Architectures and Mechanisms for Language Processing conference, held online (Paris, France). pdf scholar\n\nPosters\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (August, 2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presentation at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).se\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The impact of cognateness on bilingual lexical access: a longitudinal priming study. Poster presented at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (June, 2021). The role of cognateness in nonnative spoken word recognition. Poster presented at the XI International Symposium of Psycholinguistics (Madrid, Spain, held online).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (July, 2020). The role of lexical similarity on bilingual parallel activation: A priming study in toddlers. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nGarcia-Castro, G., Avila-Varela, D., and Sebastian-Galles, N. (July, 2020). Does phonological overlap across translation equivalents predict earlier age of acquisition?. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., and Plunkett, K. (August, 2019). The impact of phonology (cognateness) on the bilingual lexicon: Parallel cross-language phonological priming. Poster presented at the Lancaster Conference on Infant and Early Child Development (Lancaster, United Kingdom).\nGarcia-Castro, G., Marimon, M., Santolin, C., and Sebastian-Galles, N. (June, 2019). Encoding new word forms when contrastive phonemes are interchanged: A preliminary study on 8-month-old infants. Poster presented at the Workshop in Infant Language Development (Potsdam, Germany). https://doi.org/10.17605/OSF.IO/GYKUH\nGarcía-Castro, G., Álvarez-Suárez, P., García-Abad, N., Cuesta, M., and Begega, A. (June, 2017). Pro-cognitive effects of environmental enrichment on attention and spatial memory: hippocampal metabolic activity in a Wistar rat model. Poster presented at the 4th International Congress on Health and Aging Research (Murcia, Spain).\nGarcía-Abad, N., Santirso, M., García-Castro, G., and Álvarez-Suárez, P. (June, 2017). Efectos del omega-3 en las redes implicadas en la memoria de trabajo espacial en ratas Wistar. Poster presented at the 3rd National Congress of Psychology (Oviedo, Spain)\n\n\nScientific dissemination\nZacharaki, K., García-Castro, G. (2019, October). “Descubriendo la mente de los bebés” Talk presented at 13a Festa de la Ciència, Barcelona, Spain. [Link] [Video]\nGarcía-Castro, G., Avila-Varela (2019, October). “Estudiando la mente de los bebés: Desde el lenguaje hasta la lógica” Talk presented at Centre Cívic, Barcelona, Spain. [Link] [Video])\n\n\nRepositories\nGarcía-Castro, G., Santolin, C., Marimon, M., and Sebastian-Galles, N. (2019, October 22th). Segmentation: Catalan and Spanish natural speech segmentation at 8 months of age. https://doi.org/10.17605/OSF.IO/42GUP)\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 5). Flip: Experience with research paradigms relates to infants’ direction of preference. https://doi.org/10.17605/OSF.IO/G95UB"
  },
  {
    "objectID": "cv.html#others",
    "href": "cv.html#others",
    "title": "CV",
    "section": "Others",
    "text": "Others\n\nAnimal research\nResearch Staff Certificate for animal research, issued on 15/05/2018 by the Direcció General de Polítiques Ambientals i Medi Natural.\n\n\nSkills\n\nData processing: I particularly enjoy wrangling my way through messy data using the tidyverse family of packages. with some time, I can also google my way through Python.\nData analysis: Linear mixed models using R, both frequentist (lme4) and Bayesian (brms); reproducible reports using RMarkdown. I can also perform acoustic analysis on stimuli using Praat, and one of its R interfaces (PraatR).\nData visualisation using ggplot2. I can make animations using gganimate, and I’m currently learning to make maps, and Shiny Apps.\nDesigning experiments and questionnaires: I can program lab-based experiments using Matlab (PsychToolbox-3) and Python (Psychopy), online experiments using the PsychoPy/PsychoJS/Pavlovia workflow, and design reproducible online questionnaires using formR.\nGit/GitHub/Bitbucket\nJASP/SPSS"
  },
  {
    "objectID": "cv.html#languages",
    "href": "cv.html#languages",
    "title": "CV",
    "section": "Languages",
    "text": "Languages\n\nSpanish (native speaker)\nEnglish: C1\nFrench: C1"
  },
  {
    "objectID": "cv.html#short-courses",
    "href": "cv.html#short-courses",
    "title": "CV",
    "section": "Short courses",
    "text": "Short courses\nAugust 2018\n\nData Science: Multiple imputation in practice, Utrecht Summer School, Utrecht University. Introduction to Multiple Imputation as a powerful tool for dealing with missing data in experimental studies.\n\nAugust 2018\n\nData Science: Statistical Programming with R, Utrecht Summer School, Utrecht University.\n\nAugust 2016\n\nIntroduction to Neuroscience: From Molecule to Behavior, Radboud Summer School, Radboud University Nijmegen. Hands-on training at research labs of the Donders Institute for Brain and Cognition.\n\nAugust 2016\n\nHow to Improve the Quality and Translatability of Preclinical Animal Studies, Radboud Summer School, Radboud University Nijmegen. Course organized by SYRCLE institute, Radboud University Nijmegen and Radboud University Medical Centre. Theory and hands-on experience on Systematic Reviews and Meta-Analysis in preclinical animal studies. Overview of the main software available to conduct a Meta-Analysis and basic formation on academic reading and writing."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Language, statistics, R",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html",
    "href": "posts/animated-distributions/animated-distributions.html",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "",
    "text": "Visualising what different probability distributions look like under different parameters can be helpful when picking a likelihood function for you Bayesian analysis. I present some animations generated with Julia using Distributions.jl.\nIn the last couple of years I went down the rabbit hole of Bayesian statistics. Although I’ve made considerable progress, as compared to where I started, I still (and will) struggle understand some basic concepts beyond some shallow, abstract idea about what they mean. One of the first challenges I encountered in my first steps was becoming aware of how many probability distributions there are, and the fact that one should pick one should carefully chose which one to use when trying to estimate some parameter.\nBen Lambert’s excellent book A Student’s Guide to Bayesian Statistics offers a chart showing many of the most popular distributions, and how they relate to each other (pp. 145, see Figure 1 below for a similar chart). They then describe each distributions very plain terms, which is to be grateful for (many introductory books get lost in the details when describing likelihood distributions). However, when the moment arrives to pick a distribution in practice, I still struggle to see the risks and benefits of using each distribution. Will it adequately cover the sampling space of my parameter? Can I parameterise it so it allocates most of the probability around the regions I consider more likely? Am I actually able to interpret the values of the parameters of the distribution and map them to my research question?\nMy first step to answer these questions is to explore what a given distribution looks like under different parameters. This might give us a hint on the range of values it covers and the “shape” of the likelihood function. As I mentioned in previous posts, I struggle to grasp statistical/mathematical concepts in absence of a good visualisation. I generated some animations in Julia using the Distributions.jl package, which provides a substantial amount of implemented likelihood functions. The only reason I chose Julia is that I’m trying to learn it step by step and I found this silly project a nice opportunity to do so. You can see the code at the end of this article or on the accompanying GitHub repository. I posted this one Twitter (see below) and got a good response, so I decided to extend a bit the contents. Enjoy! :)"
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#normal-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#normal-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Normal distribution",
    "text": "Normal distribution\nWikipedia\nWell, we all know this one. This is a continuous distribution that covers the whole range of real numbers. Why is it used so often? Because most data we find in real life are frequently quite plausible under a normal distribution. Critically, the normal distributions makes very few assumptions about the data we are trying to model. Why is it so? As Richard McElreath explains in Statistical Rethinking1 (section 4.1), there are many ways a data generating mechanism might spit out normal data, even when each of the individual observations are not drawn from a normal distribution. This is because when the data are the result of adding (or multiplying), the resulting distribution tends to converge to a normal one. As McElreath points out, many phenomena we observe in nature is the result of adding multiple small factors together (e.g., someone´s height is the result of adding multiple genetic and environmental factors together). Basically, whenever you are not sure about what distribution suits your data better, the normal distribution might be a good first approximation.1 You can also take a look a his excellent lecture on his YouTube channel, which cover much of the contents in the book!\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\big(\\frac{x-\\mu}{\\sigma}\\big)^2}\n\\]\nWhere \\(f(x)\\) us the probability density function, is the standard deviation, and is the mean.\n\n\n\nFigure 2: The normal distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#gamma-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#gamma-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nWikipedia\n\\[\nf(x) = \\frac{\\lambda(\\lambda x)^{\\alpha - 1}  \\;e^{-\\lambda x}}{\\Gamma(\\alpha)}\n\\]\nThis distribution only covers positive values, and has two parameters. It is in important one, as many other distribution, like the exponential, are specific cases of this one. In applied contexts, this distribution is used to model waiting times, and the incidence of some diseases, among others. In general, any continuous variable whose mode is expected to be closer rather than farther from zero can potentially be modelled by a Gamma distribution.\n\n\n\nFigure 3: The Gamma distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#inverse-gamma-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#inverse-gamma-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Inverse-gamma distribution",
    "text": "Inverse-gamma distribution\nWikipedia\nThis distribution is a reciprocal transformation of the Gamma distribution, and has similar properties. In can’t find many uses for this distribution if not as a conjugate prior for the variance of a normal distribution.\n\\[\nf(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1}  \\;exp\\Bigg(-\\frac{\\beta}{x}\\Bigg)\n\\]\n\n\n\nFigure 4: The inverse-gamma distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#exponential-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#exponential-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nWikipedia\nThis distribution is continuous and cover only positive values. It places most of the likelihood around zero, which makes it very convenient for modelling small positive quantities such as small distances or time intervals or standard deviations. Its shape depends on only one parameter, \\(\\lambda\\), which makes it simpler to parameterise (as compared to others like the Beta distribution). It has an inconvenient, though: it is a bit difficult to interpret what the value of this parameter means in the context of our research question (i.e., theory). While one can interpret the mean and standard deviation of the normal distribution as where the most likely value of the distribution lies and its associated uncertainty, respectively, the \\(\\lambda\\) parameter is not that trial to interpret: it doesn’t relate to the most likely values of the distribution linearly, and it cannot be interpreted a a rate or any other occurrence metric. What works for me is to take a look at the resulting distribution and see whether it captures my expectations about the data I’m about to observe.\n\\[\nf(x) = \\lambda e^{-\\lambda x}\n\\]\n\n\n\nFigure 5: The exponential distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#student-t-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#student-t-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Student-t distribution",
    "text": "Student-t distribution\nWikipedia\n\\[\nf(x) = \\frac{\\Gamma((\\upsilon+1)/2)}{\\sqrt{\\upsilon \\pi}  \\;\\Gamma (\\upsilon /2)}\n\\]\n\n\n\nFigure 6: The Student’s t distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#beta-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#beta-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Beta distribution",
    "text": "Beta distribution\nWikipedia\n\\[\nf(x) = \\frac{x^{\\alpha-1} \\;(1-x) \\;x^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\n\n\n\nFigure 7: The Beta distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#cauchy-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#cauchy-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Cauchy distribution",
    "text": "Cauchy distribution\nWikipedia\n\\[\nf(x) = \\frac{1}{\\pi \\gamma \\Bigg[1 + \\big( \\frac{x-x_0}{\\gamma} \\big)^2 \\Bigg]}\n\\]\n\n\n\nFigure 8: The Cauchy distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#frechet-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#frechet-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Frechet distribution",
    "text": "Frechet distribution\nWikipedia\n\\[\nf(x) = \\alpha \\; x^{-1-\\alpha} e^{-x^{-\\alpha}}\n\\]\n\n\n\nFigure 9: The Frechet distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#pareto-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#pareto-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Pareto distribution",
    "text": "Pareto distribution\nWikipedia\n\\[\nf(x) = \\frac{\\alpha  \\;x_{m}^{\\alpha}}{x^{\\alpha+1}}  \\;\\text{for}  \\; x > x_m\n\\]\n\n\n\nFigure 10: The pareto distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#weibull-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#weibull-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Weibull distribution",
    "text": "Weibull distribution\nWikipedia\n\\[\nf(x) = \\lambda \\alpha(\\lambda x)^{\\alpha-1} \\; e^{-(\\lambda x)^{\\alpha}}\n\\]\n\n\n\nFigure 11: The Weibull distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#tri-weight-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#tri-weight-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Tri-weight distribution",
    "text": "Tri-weight distribution\nWikipedia\n\n\n\nFigure 12: The tri-weight distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#rayleigh-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#rayleigh-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Rayleigh distribution",
    "text": "Rayleigh distribution\nWikipedia\nSimilar to Gamma or Poisson, the Rayleigh likelihood function covers all positive values and relies on only one parameter, \\(\\sigma\\), to determine its shape and location. Lower values of \\(\\sigma\\) make the distribution lie closer to zero and be less disperse. Despite its seemingly less interesting appearance, this distributions is extensively used for modelling vibrational data such as water displacement, assessing the quality of railroads, or analysing MRI data2.2 See https://www.sciencedirect.com/topics/engineering/rayleigh-distribution\n\\[\nf(x) = \\frac{x}{\\sigma^2} \\; e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\n\n\nFigure 13: The Rayleigh distribution."
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#wigner-semi-circle-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#wigner-semi-circle-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Wigner semi-circle distribution",
    "text": "Wigner semi-circle distribution\nWikipedia\nThis distribution is, well, a semicircle when \\(R = 1\\), were \\(R\\) is the radius of the semi-circle. All values of the sampling space outside the [-R, R] interval have zero probability. I can’t say much more than that. I have no idea why this distribution even exists, and I have struggled to find documentation about it in the reasonably long period of time I searched for it. I suspect this distribution might be useful for geo-spatial modelling, where one wants to actually model the shape of an object. Otherwise, I’m lost.\n\\[\nf(x) = \\frac{2}{\\pi R^2} \\; \\sqrt{R^2 - x^2} \\\\ \\text{where} \\; \\pi \\; \\text{is the actual number } \\pi \\text{, not a parameter}\n\\]\n\n\n\nFigure 14: Wigner’s semicircle distribution.\n\n\nInterestingly, this distribution can be considered a particular case3 of the Beta distribution, so that where \\(\\alpha = \\beta = 3/2\\), then \\(X = 2RY – R\\).3 See https://handwiki.org/wiki/Wigner_semicircle_distribution"
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#triangular-distribution",
    "href": "posts/animated-distributions/animated-distributions.html#triangular-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Triangular distribution",
    "text": "Triangular distribution\nWikipedia\nAs a curiosity, there is such thing as a triangular distribution. It does look triangular, as can be seen in the Wikipedia article linked above. This distribution is not implemented yet in Distributions.jl. As with the semi-circle distribution, I struggle to see any context where this distribution might be useful beyond geo-spatial modelling.\n\\[\nf(x) = \\begin{cases}\n    0  & \\text{for}  \\; x < \\alpha, \\\\\n    \\frac{2(x-a)}{(b-a)(c-a)} & \\text{for} \\; a \\leq x < c, \\\\\n    \\frac{2}{b-a} &  \\text{for} \\; x = c, \\\\\n    \\frac{2(b-x)}{(b-a)(b-c)} & \\text{for} \\; c < x \\leq b, \\\\\n    0 & \\text{for} \\; b < x\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "posts/animated-distributions/animated-distributions.html#some-final-remarks",
    "href": "posts/animated-distributions/animated-distributions.html#some-final-remarks",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Some final remarks",
    "text": "Some final remarks\nSome well-known distributions are missing (e.g., Dirichlet, Wishart, Beta-binomial). This is because they have not yet been implemented in Distributions.jl, and I sadly lack the knowledge to contribute on this. Still feel free to explore the Wikipedia or other resources to get an idea of how these distributions look like in what contexts they are useful!"
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-el-paquete-psicotuiter",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-el-paquete-psicotuiter",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "¿Qué es el paquete {psicotuiteR}?",
    "text": "¿Qué es el paquete {psicotuiteR}?\n\n\nEl paquete psicotuiteR es un paquete muy simple que hemos creado para que la gente de la comunidad de #psicotuiter, en Twitter, pudiera experimentar con él añadiendo funciones o jugando con los datos que incluye. Podéis ver más información sobre el paquete en su página web. La comunidad es psicotuiter es un grupo de castellanoparlantes que hablan, entre otras cosas, sobre psicología y salud mental en Twitter."
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-un-paquete-de-r",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-un-paquete-de-r",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "¿Qué es un paquete de R?",
    "text": "¿Qué es un paquete de R?\nEn el manual R packages de Hadley Wickham y Jenny Bryan, se describe un paquete de R así:\n\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others.\n\nA mí me gusta definirlo como un grupo de funciones documentadas que se agrupan siguiendo el formato que el alto consejo jedi2 ha dictado.2 La gente que gestiona CRAN, vaya. Tienen cierta fama de boomers.\nPor cierto, R packages es el mejor recurso que existe en este momento para aprender a hacer paquetes de R. No hay nada en este tutorial que no esté incluido ahí–incluso mejor explicado–a excepción de algún que otro comentario autodespectivo al pie de página."
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#por-qué-hacer-un-paquete-de-r",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#por-qué-hacer-un-paquete-de-r",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "¿Por qué hacer un paquete de R?",
    "text": "¿Por qué hacer un paquete de R?\nCuando programamos, es común que necesitemos ejecutar la misma línea de código varias veces. Cuando esto ocurre, en lugar de escribir y ejecutar la misma línea de código una y otra vez en la consola de R, podemos escribir un script. Un script de R es un archivo con la extensión .R3 que contiene las diferentes líneas de código que queremos ejecutar.3 Por convención se suele usar la “R” mayúscula, aunque la mayoría de los sistemas operativos son indiferentes a que escribamos las extensiones en mayúsculas o minúsculas. Hagas lo que hagas, trata de ser consistente.\nA veces el mismo script contiene líneas de código muy parecidas que ejecutamos para aplicar, por ejemplo, la misma función sobre objetos diferentes. Por ejemplo:\nlm(x ~ y, data = df1)\nlm(x ~ y, data = df2)\nEn el bloque de código de arriba, df1 y df2 son objetos de tipo data.frame con variables similares pero diferentes observaciones. Idealmente, no deberíamos repetir la misma línea de código más de una vez. Digo idealmente porque la alternativa es escribir una función. No siempre merece la pena hacer una función, por mucho que la gente más purista insista. Si estás leyendo esto doy por hecho que te interesa hacer tu código más conciso y replicable.\nEn todo caso es generalmente recomendable definir una serie de funciones antes de trabajar sobre tus datos. Una función es un conjunto de comandos que se ejecutan en orden cuando damos la orden4. Estos comandos se agrupan bajo el nombre que asignemos a la función. Así podemos condensar nuestro código en funciones para que sea más conciso.4 Definición mediocre pero obligada. Lo siento.\nEn un último nivel de abstracción, nivel cerebro galáctico, está agrupar nuestras funciones en un paquete. Hacer esto tiene unos cuantos beneficios:\n\nNo tendremos que definir las funciones cada vez que abramos un script: bastará con cargar el paquete y sus funciones estarán disponibles para usarlas.\nSerá más fácil compartir nuestro código con otras personas: es común que nuestras funciones requieran, tener instalados ciertos paquetes externos. Por ejemplo, si mi función usa la función mutate del paquete dplyr, quien quiera usar mi función deberá tener instalado dplyr (a veces es inluso necesario tener instalada la misma versión del paquete). Este es uno de las principales amenazas a la reproducibilidad de nuestro código. Un paquete de R, sin embargo, se asegura que, durante la instalación, se instalen las dependencias necesarias en el ordenador de la otra persona,\nPodremos documentar las funciones fácilmente (se acabaron los comentarios escuestos e indescifrables en nuestros scripts) y hacer esta documentación accesible a quien use nuestro paquete al ejecutar ?mifuncion (puedes ejecutar ?mean para ver un ejemplo de cómo se verá nuestra documentación).\n\nHay más motivos por los que puede ser buena idea crear un paquete de R, como por ejemplo trabajar con tu propio código de forma más cómoda y reproducible5, para compartir y documentar bases de datos (a veces nuestro paquete no incluirá ninguna función, sino únicamente unos datos y su documentación) o para aprender R más a fondo y sus entresijos. Yo he aprendido más intentando hacer paquetes de R que en cualquer tutorial.5 Tu yo futuro te lo agradecerá y si algún día se puede viajar en el tiempo lo harás para darte un beso en la frente por ello."
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-necesito-para-hacer-un-paquete-de-r",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-necesito-para-hacer-un-paquete-de-r",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "¿Qué necesito para hacer un paquete de R?",
    "text": "¿Qué necesito para hacer un paquete de R?\nPara seguir este tutorial, y en general para crear un paquete de R necesitaremos instalar en nuestro ordenador6:6 En el momento de escribir este tutorial yo estoy usando R 4.0.4, RStudio 1.4.1103, rmarkdown 2.11.1, devtools 2.4.2 y usethis 2.0.1. Echa un vistazo a esta guía para ver cómo instalar una versión específica de un paquete de R.\n\nR\nRStudio\nLos siguientes paquetes de R: devtools, usethis y rmarkdown.\n\nPuedes instalar estos paquetes así:\ninstall.packages(\"devtools\", \"usethis\", \"rmarkdown\")\nCuando tengamos todo instalado, reiniciaremos nuestra sesión de RStudio y después cargaremos devtools y usethis (usaremos rmarkdown más adelante):\nlibrary(devtools)\nlibrary(usethis)"
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#inicializando-el-paquete",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#inicializando-el-paquete",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "Inicializando el paquete",
    "text": "Inicializando el paquete\nSalvo contadas excepciones7, las personas de bien utilizaremos RStudio para programar en R, en lugar de usar únicamente la consola como hacen les psicópatas. Trabajar en un proyecto de RStudio nos facilitará mucho la vida al hacer un paquete de R y deberías hacerlo casi siempre que programas en R (como mínimo, te ahorrará mucho tiempo buscando documentos dentro de carpetas)8.7 Por ejemplo, si tienes el suficiente tiempo libre como para configurar una sesión de R medianamente funcional en Visual Studio Code.8 Nunca está demás ver este tutorial de Danielle Navarro sobre cómo organizar un repositorio\nPara crear un paquete tenemos dos opciones: hacerlo a través de RStudio (ver tutorial) o usando usethis en nuestra consola. A mi me gusta la segunda opción:\ncreate_package(path = \"psicotuiteR\") # abre nueva ventana\nEste comando creará una nueva carpeta. Lo hará dentro de tu repositorio de inicio, el cual puedes consultar ejecutando getwd() en tu consola (asumiendo que nos has cambiado el directorio por tu cuenta previamente). Esta carpeta contendrá varios archivos y una carpeta:\n.gitignore\n.Rbuildignore\nDESCRIPTION\nNAMESPACE\npsicotuiteR.Rproj\nR\n\n\n\n\n\n\nImportant\n\n\n\nLos nombres de la mayoría de archivos y carpetas en este directorio son importantes. Trata de no cambiarlos si no es imprescindible.\n\n\nUno de los archivos en la carpeta tiene el mismo nombre que el paquete y la extensión .Rproj. Cada vez que queramos trabajar en nuestro paquete es recomendable abrir la sesión de RStudio haciendo doble click sobre el archivo .Rproj. Veremos qué son el resto de archivos más adelante.\nPues bien: técnicamente, ¡ya tenemos un paquete de R! Si ejecutamos el código de abajo, el paquete se instalará como si fuera uno más en nuestro directorio de paquetes de R.\ndevtools::install()\nEncontrarás la carpeta de instalación junto a la de los demás paquetes que hays instalado en tu ordenador. Puedes consultar dónde se instalan tus paquetes de R ejecutando .libPaths() en tu consola. La primera ruta que aparezca será donde encontrarás tu paquete instalado (en mi caso, encontraré la carpeta ~/Documents/R/win-library/4.0/psicotuiteR).\nLa función que hemos ejecutado, install() del paquete devtools simula lo que otra persona haría al ejecutar la función install.packages() si nuestro paquete estuviera disponible en CRAN. Ahora mismo, podríamos cargar nuestro paquete con library(psicotuiteR) y trabajar con él. Por supuesto, nuestro paquete aún está vacío. En las próximas secciones veremos cómo añadir funciones, entre otras cosas, a nuestro paquete.\nAntes de hacer eso, ahí va un truco: cuando añadimos o hacemos cambios en el paquete, necesitaremos actualizar nuestra sesión y cargar el paquete de nuevo y volver a ejecutar las funciones para comprobar que todo funciona correctamente. En lugar de ejecutar install() cada vez que queremos ver si nuestro paquete funciona, podemos ejecutar load_all() (también del paquete devtools) sin siquera reiniciar la sesión y así cargar de nuevo el paquete actualizado como si alguien hubiera cargado el paquete usando library(). Los contenidos del paquete que se cargarán usando load_all() son los que hay en el nuestro repositorio (desde el que estamos desarrollando el paquete), y no desde el directorio en el que se instala el paquete si ejecutamos install(). ¡Esto es mucho más rápido y eficiente!\nOtro truco: puesto que vamos a utilizar las funciones de los paquetes devtools y usethis a menudo–y por lo tanto vamos a necesitar cargar estos paquetes mediante library(devtools) y library(usethis) cada vez que iniciemos una sesión de R– podría ser recomendable añadir esos dos comandos a nuestro archivo .Rprofile (ver esta guía para más información sobre .Rprofile. Las líneas de código que contenga ese archivo serán ejecutadas en cada inicio de sesión que hagamos en nuestro proyecto de RStudio. Podemos hacer esto usando las siguientes funciones de usethis:\nlibrary(usethis)\nuse_usethis() \nuse_devtools()\nEsto creará un archivo llamado .Rprofile en nuestro directorio y añadirá, entre otras cosas muy útiles, los comandos library(usethis) y library(devtools)."
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#description",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#description",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "DESCRIPTION",
    "text": "DESCRIPTION\nEl primer archivo que vamos a explicar se llama DESCRIPTION. La mayor parte de la información general (o meta-información) de nuestro paquete se encuentra en este archivo: autoría, ajustes generales, dependencias, etc. Puedes consultar el DESCRIPTION de psicotuiteR para hacerte una idea de cómo se ve cuando está editado. ¡Presta especial atención a cómo hemos indicado la autoría!\nDESCRIPTION es uno de los pocos archivos que tendremos que editar tanto a mano como mediante otras funciones del paquete usethis. Tendremos que cambiar el título, autoría y descripción del paquete a mano (además de algún otro campo), mientras que, por ejemplo, los campos Imports, Depends y Suggests serán rellenados más adelante mediante la función use_package(), de usethis. Cuando hablemos de dependencias, más adelante, comentaremos un par de cosas saobre estos tres campos."
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#funciones-en-r",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#funciones-en-r",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "Funciones en R",
    "text": "Funciones en R\nLas funciones de R son el cuerpo principal de un paquete de R. Contienen el código que se ejecutará en nuestras funciones y su correspondiente documentación. Las funciones de por sí no tienen gran misterio. Las puedes hacer más simples o más complejas. Generalmente, por motivos de legibilidad suele ser mejor mantener nuestras funciones lo más simples que podamos. Es mejor escribir muchas funciones que hacen tareas pequeñas que pocas funciones que lo hacen todo. No tienes por qué hacer disponibles todas las funciones que escribas: puedes mantener algunas funciones para uso interno dentro de otras funciones que sí están disponibles al público (ahora veremos cómo). Hay muchos tutoriales para aprender a hacer funciones en R. Por ejemplo este. Merece la pena ganar algo de confianza en poder hacer funciones de R: empodera mucho y te hace entender muchos de los errores que te encontrarás a lo largo de tu vida programando en R. Ahí va un ejemplo de función muy simple:\nprint_name <- function(\n    author = \"Gon\"\n){\n    print(author)\n}\nEsas líneas de código definen la función print_name(). Esta función incluye un argumento llamado author, que, por defecto, toma el valor \"Gon\". Si definimos la función y ejecutamos print_name() en nuestra consola de R, nos devolverá el valor `“Gon”.\nGuardaremos esta función en un archivo con la extensión .R dentro de la carpeta R/ en nuestro repositorio principal (el nombre que pongamos a este archivo da igual, pero trata de que sea informativo de su contenido). Yo llamaré a este archivo print_name.R. Ahora nuestro directorio se ve así:\n.gitignore\n.Rbuildignore\nDESCRIPTION\nNAMESPACE\npsicotuiteR.Rproj\nR\n    |-print_name.R"
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#documentación",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#documentación",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "Documentación",
    "text": "Documentación\nAhora vamos a documentar esa función. R utiliza un tipo de lenguaje parecido a LaTeX para escribir la documentación de un paquete. Este lenguaje se llama R documentation. Técnicamente, podríamos escribir toda la documentación de cada función de nuestro paquete en este lenguaje y guardar cada archivo en la carpeta man/ con la extensión .Rd. En esta carpeta es donde R espera encontrar la documentación. Gracias a Dios (y a la buena voluntad de la comunidad de R), podemos incluir los contenidos de esos archivos como si fueran comentarios en nuestros scripts de R, encima de cada función. La función document() de devtools se encargará de generar todos los .Rd necesarios en la carpeta man/ por nosotrxs. Volveremos a esto más adelante. Por ahora, observa un ejemplo de función documentada:\n#' Print author\n#' @export print_name\n#' @usage print_author()\n#' @import dplyr\n#' @importFrom tidyr drop_na\n#' @description Print the name of the author of the package we are developing\n#' @param author Name of the package author\n#' @author Gonzalo Garcia-Castro\nprint_name <- function(\n    author = \"Gon\"\n){\n    print(author)\n}\nEn la parte de abajo encontramos las mismas líneas de código que hemos visto antes. En la parte de arriba encontramos la documentación de la función. Estas líneas de código están precedidas por el símbolo #'. La apóstrofe indica que no es un comentario cualquiera, sino parte de la documentación de la función. En la mayoría de estas líneas indicamos mediante el símbolo @ qué campo estamos describiendo (autoría, descripción, un argumento, etc.).\nPor ejemplo, la línea de abajo indica que estamos describiendo uno de los argumentos de la función (por alguna razón que desconozco, se ha decidido refererirse a los argumentos como “@param” y no como @arg“). La primera línea se asume que es el título de la función y por eso no hace falta indicar @title antes de \"Print author\".\n#' @param author Name of the package author\nUna vez hayamos rellenado la documentación de nuestra función, ejecutaremos document() y se generará automáticamente un archivo llamado print_name.Rd en la carpeta man/. Podemos comprobar que la documentación se ha guardado correctamente ejecutando ?print_name. Se debería abrir la ventana Help en uno de los paneles de RStudio. Algunos consejos cuando rellenes la documentación de tus funciones:\n\nExplica las cosas con claridad, pero no tengas miedo de extenderte o repetir las cosas. Más documentación siempre es mejor que menos documentación, siempre que se expliquen las cosas con claridad y se mantenga cierto sistema en la estructura de la documentación.\nEcha un vistazo a la documentación de las funciones princpiales de tus paquetes favoritos. Es la mejor forma de saber cómo documentar un función y qué campos son los más importantes.\n\nViñetas y artículos\nUna forma más elaborada de documentar un paquete es crear viñetas. Una viñeta (o vignette, en inglés) es un documento en el que se explica con detalle cómo se trabaja con el paquete, como si fuera un tutorial. Las viñetas son particularmente útiles para quienes usan el paquete por primera vez, y deberían ilustrar, como mínimo, algún ejemplo sobre cómo se usan las funciones. Un buen ejemplo de viñeta es esta, del paquete {dplyr}, en la que indican cómo usar la función group_by(). Para crear una viñeta ejecutaremos el siguiente comando:\nuse_vignette(\n  name = \"print-name\" # así se llamará el archivo,\n  title = \"Imprimiendo un nombre\" # así se titulará la viñeta\n)\nEste comando creará una carpeta llamada vignettes/ y creará un archivo con la extensión .Rmd (Rmarkdown). Los archivos Rmarkdown son una mezcla entre un archivo Markdown (que tienen la extensión .md) y un script de R. Markdown es un lenguaje de edición de textos bastante sencillo (desde luego más sencillo que LaTeX). Rmarkdown permite intercalar bloques de código de R en medio del texto. Cuando renderizamos el documento, esos bloques de código se ejecutan y el resultado se incluye como texto o imagen. Este tipo de documentos son muy útiles para crear informes y actualizar los datos que incluyen con sólo un click. Si quieres aprender a user Rmarkdown (100% recomendado), el manual R Markdown: The Definitive Guide de Yihui Xie es el mejor recurso. Una vez escribamos nuestra viñeta podremos incluirla en nuestra documentación ejecutando:\nbuild_vignettes()\ndocument()\nComo dato curioso, este mismo tutorial que estás leyendo es una viñeta incluida en el paquete psicotuiteR. Muy meta todo, ¿verdad?"
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#dependencias-y-otras-pesadillas",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#dependencias-y-otras-pesadillas",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "Dependencias y otras pesadillas",
    "text": "Dependencias y otras pesadillas\nCon frecuencia nuestras funciones de R dependerán, a su vez, de funciones de otros paquetes. Considera la siguiente función:\n#' Una función que añade una variable\n#' @param x Una serie de caracteres\n#' @returns El número incluido en x\nextrae_numeros <- function(x){\n  y <- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nComo indica la documentación, esta función devuelve los números incluidos en x, una serie de caracteres que introducimos como argumento, en formato numérico. Para ello utiliza la función str_extract(). Esta función pertenece al paquete {stringr}. Tal y como está escrita esta función, cuando instalemos el paquete y la ejecutemos no dará un error: R nos indicará que la función str_extract no existe. Podría ocurrírsenos usar algo como stringr::str_extract o library(stringr) dentro de la función. Pero esto no es buena idea porque muchas veces no funcionará. Necesitamos incluir la función str_extract y stringr como dependencias de nuestro paquete, para que cuando alguien instale el paquete esa función se encuentre disponible para el paquete cuando ejecutemos nuestra función extrae_numeros(). Para indicar una dependencia, debemos hacerlo en dos pasos:\n\nEjecutar use_package(\"stringr\") para incluir stringr en la lista de paquetes que deben instalarse junto al nuestro. Al hacerlo, verás que stringr ha sido incluido en el campo Imports del archivo DESCRIPTION.\nIncluir la función str_extract del paquete stringr en la documentación de la función de la siguiente forma:\n\n#' @importFrom stringr str_extract\nAhora nuestra función debería verse así:\n#' Una función que añade una variable\n#' @param x Una serie de caracteres\n#' @importFrom stringr str_extract\n#' @returns El número incluido en x\nextrae_numeros <- function(x){\n  y <- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nLa función str_extract estará disponible y nuestra función podrá ejecutarse sin problemas. Otra opción sería importar el paquete stringr al completo, con todas sus funciones. En lugar de indicar (importFrom?) con el nombre del paquete y la función que queremos importar, indicaríamos @import y solamente nombre del paquete: #' @import stringr. Sin embargo, casi siempre es mejor indicar las dependencias una a una, aunque sea repetitivo: así será más fácil detectar de dónde viene cada función que usamos. Si vamos a usar varias funciones del mismo paquete, podemos indicarlas una debajo de otra:\n#' @importFrom stringr str_extract\n#' @importFrom stringr str_detect\n#' @importFrom stringr str_replace\nSólo en el caso de que utilicemos muchísimas funciones del mismo paquete en nuestra función tendrá cierto sentido importar el paquete al completo. Las dependencias de nuestro paquete sólo estará disponibles cuando ejecutemos document() y se actualice la documentación. Esto es porque document() no sólo construye la documentación del paquete (los archivos .Rd en man/), sino que también modifica el archivo NAMESPACE para enumerar las dependencias. Echa un vistazo a NAMESPACE de psicotuiteR. Este archivo ha sido generado al ejecutar document(). Una de nuestras funciones incluye #' @import janitor clean_names y por tanto esta función ha sido incluida en el NAMESPACE.\nHemos aprendido a indicar dependencias de otros paquetes en nuestro código através de la documentación: funciones que nuestras propias funciones necesitan. Para hacer nuestras funciones disponibles en el paquete, necesitaremos incluirlas también en el NAMESPACE, pero no como dependencias, sino como exportaciones: en lugar de incluirlas en la documentación usando @importFrom, lo haremos usando #' @export. Volviendo al ejemplo anterior: #' @export extrae_numeros. Nuestra función se ver ahora así:\n#' Una función que añade una variable\n#' @export extrae_numeros\n#' @param x Una serie de caracteres\n#' @importFrom stringr str_extract\n#' @returns El número incluido en x\nextrae_numeros <- function(x){\n  y <- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nLa segunda línea indica que está función debe estar disponible en nuestro paquete con el nombre extrae_numeros. Aunque es posible exportar la función con un nombre diferente al que le hemos asignado en el script (R nos dará un aviso, pero no un error), es importante que así sea.\nPodrías preguntarte: ¿qué sentido tiene crear funciones si no van a estar disponibles para quien use el paquete? Pues porque a veces es recomendable hacer funciones pequeñas para uso interno que, aunque su funcion no es de gran interés para el público, ayuden a otras funciones más complejas que sí tienen sentido que use el público. Sea como sea, técnicamente sí se puede acceder a este tipo de funciones usando el operador :::.\nSi quieres hacer la prueba, ve a la consola de R y compara las sugerencias que salen cuando escribes usethis:: y usethis:::. Todas esas funciones que salen en el segundo caso no están disponibles cuando cargamos el paquete usando library() porque las personas que han creado el paquete usethis han considerado que no las necesitamos, aunque sí las necesitan las funciones que sí usamos.\nEn resumen, cuando ejecutemos document(), se incluirán en el NAMESPACE las funciones que hayamos indicado en la documentación mediante #' @export. Nunca modificaremos NAMESPACE archivo a mano, sino que cambiaremos sus contenidos modificando la documentación de nuestras funciones en el script de R y luego ejecutaremos document(). Con toda probabilidad, la mayoría de los problemas que vas a encontrar al desarrollar un paquete de R (también los más frustrantes) se deban a sus dependencias. Con el tiempo aprenderás a detectar estos problemas y entender por qué ocurren. Te damos un abrazo por adelantado: been there, done that.\nHay dos dependencias un poco especiales: si usamos pipes, “pipas”–o como quiera Dios que se traduzca al español–en nuestras funciones, en lugar de indicar # @importFrom dplyr %>% en cada función, podremos ejecutar use_pipe() y esta función se encargará de incluir esta dependencia en la documentación por nosotros. Lo hará en un script llamado con el nombre de nuestro paquete, en la carpeta R/, que creará automáticamente. Si alguna de nuestras funciones devuelve un objeto en forma de data.frame y queremos usar la función tibble, del paquete tibble para que quede mejor, también podemos usar la función use_tibble(), que hará lo mismo que use_pipe(), aladiendo tibble a la lista de dependencias.\nPor último, es recomendable mantener las dependencias de paquetes al mínimo. Cada vez que incluimos una dependencia corremos el riesgo de que alguno de los paquetes de los que dependemos cambie de versión y nuestro paquete deje de funcionar porque una de las funciones de ese paquete que utilizamos ha cambiado. CRAN impone un límite de dependencias en 20."
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#datos-internos-datos-externos-datos-brutos",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#datos-internos-datos-externos-datos-brutos",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "Datos internos, datos externos, datos brutos",
    "text": "Datos internos, datos externos, datos brutos\nMuchos paquetes incluyen objetos del tipo data.frame. Por ejemplo, el paquete {dplyr} contiene el objeto starwars. Este objeto es un data.frame con información sobre muchos personajes del universo de Star Wars. Podemos acceder a este objeto así dplyr::starwars.\n\n\n# A tibble: 6 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n2 C-3PO           167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n3 R2-D2            96    32 <NA>    white,… red        33   none  mascu… Naboo  \n4 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n5 Leia Organa     150    49 brown   light   brown      19   fema… femin… Aldera…\n6 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n# … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n#   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\nEste objeto está documentado: si ejecutamos ?dplyr::starwars se abrirá el panel de documentación de RStudio. Este tipo de objetos son muy útiles para mostrar ejemplos de uso de las funciones de un paquete. Algunos paquetes incluso han sido diseñados con el único propósito de compartir una base de datos documentada, como por ejemplo el paquete {palmerpenguins}, que apenas contiene el objeto penguins: una base de datos sobre la anatomía demlos pingüinos de la Isla de Palmer. En esta sección veremos cómo incluir una base de datos como ésta en nuestro paqeute de R y cómo documentarla.\nDatos internos y externos\nPodemos incluir un data.frame por defecto en nuestro paquete de dos formas: como un objeto interno o como un objeto externo. Hacerlo de la primera forma es el equivalente a crear una función sin exportarla al NAMESPACE: nos sirve para utilizarla dentro de las funciones internas del paquete, pero no será inmediatamente visible para quien cargue el paquete usando library (aunque igualmente podrá hacerlo usando el operador :::). Cuando incluimos el data.frame como objeto externo, sí se podrá acceder a él cuando cargemos el paquete (o mediante el operador ::).\nPara guardar un data.frame de cualquiera de las dos formas, primero debemos tenerlo definido entre las variables de nuestra sesión de R. Por ejemplo, supón que hemos definido el data.frame clima (disponible en el paquete psicotuiteR):\n\n\n# A tibble: 6 × 37\n     id lugar  genero   psico1 psico2 psico3 tw1   tw2   tw_pers tw_di…¹ tw_an…²\n  <int> <chr>  <chr>    <chr>   <int>  <int> <chr> <chr>   <int>   <int>   <int>\n1     1 Europa Mujer    Posgr…      3      4 Mas … Vari…       4       3       0\n2     2 Europa Hombre   Posgr…      1      5 Mas … Vari…       4       2       3\n3     3 Europa No bina… Posgr…      4      3 Meno… Vari…       4       2       1\n4     4 Europa Hombre   Posgr…      4      3 Mas … Vari…       5       5       0\n5     5 Europa Hombre   Posgr…      5      1 Mas … Vari…       4       2       3\n6     6 Europa Hombre   Profe…      5      3 Mas … Vari…       1       4       0\n# … with 26 more variables: tw_tuits <int>, tw_hilos_div <int>,\n#   tw_hilos_ot <int>, tw_rt <int>, tw_like <int>, tw_resp <int>,\n#   exp_tw1 <int>, exp_tw2 <int>, exp_tw3 <int>, exp_tw4 <int>, comp1 <chr>,\n#   comp2_verg <int>, comp2_quediran <int>, comp2_calidad <int>,\n#   comp2_error <int>, comp2_conf <int>, comp2_nunca <int>, comp3_divulg <int>,\n#   comp3_apren <int>, comp3_deb_cien <int>, comp3_deb_poli <int>,\n#   comp3_pers <int>, comp3_nunca <int>, psico_tw1 <chr>, psico_tw2 <int>, …\n\n\nUtilizaremos la función use_data() del paquete usethis para incluirlo en nuestro paquete. Si queremos exportarlo al NAMESPACE especificaremos internal = FALSE en los argumentos de la función (opción por defecto). Si no queremos exportarlo especificaremos internal = TRUE:\nuse_data(clima) # objeto externo\nuse_data(clima, internal = TRUE) # objeto interno\nEsta función creará (en caso de que no exita ya) una carpeta en nuestro directorio llamada data/ y guardará el data.frame como un archivo .rds (como un objeto de R), en nuestro caso lo guardará con el nombre clima.rds. En el caso de que el objeto ya exista, debemos incluir overwrite = TRUE en los argumentos para que nos permita sobreescribirlo.\nuse_data() también creará un archivo llamado clima.R en la carpeta R. Como la extensión indica, este archivo es un script de R como el que usamos para las funciones. Debemos documentar nuestros datos en este archivo usando Roxigen, tal y como hacemos para las funciones. Echa un vistazo al archivo clima.R del paquete psicotuiteR. Verás que sólo contiene documentación, excepto por la línea final, que contiene \"clima\". No necesita nada más que el nombre del objeto bajo el cual exportaremos el data.frame. De lo demás se encargará, como siempre la función document().\nDatos brutos\nHay una forma más de incluir datos, aun más reproducible que la anterior: incluir los datos brutos, como un archivo .csv, .txt, .tsv, .xlsx, etc. También podemos incluir en nuestro paquete el script de R con el código de que hemos usando para procesar los datos contenidos en el archivo y para llegar a la forma final del objeto que guaradamos mediante use_data(). La función use_raw_data() del paquete usethis se encarga de esto.\nuse_raw_data()\nEsta función creará dos carpetas (en caso de que no existan ya): inst/ y data-raw/. La carpeta inst/ (abreviatura de installation) de un paquete de R incluye archivos externos (por ejemplo, .txt, scripts de Python, Stan, C++…) que queremos que estén disponibles cuando alguien cargue el paquete, pero no son archivos que normalmente se incluyan en un paquete. Es recomendable guardar el archivo con nuestros datos brutos en la carpeta inst/. En la carpeta data-raw/ se creará un archivo con la extensión .R. En este archivo (que se abrirá automáticamente cuando ejecutemos use_data_raw()) escribiremos el código que procesa los datos y los deja como queremos que se guarden en el paquete. La última línea del script (incluida por defecto) guarda el objeto resultante como datos externos mediante la función use_data(). Así, cada vez que queramos actualizar el objeto, sólo tendremos que ejecutar este código.\nArchivos externos e inst/\nComo hemos mencionado, cualquier archivo que queramos incluir en nuestro paquete y no tenga la extensión .R o .rds, debería estar dentro de la carpeta inst/. Cuando alguien instale el paquete, los archivos de esta carpeta se moverán al directorio principal del paquete. Puedes hacer la prueba ejecutando la función install() de devtools. Cuando lo hayas hecho ve a la carpeta del paquete (recuerda que los paquetes se instalan en la primera ruta que indique .libPaths()). Cualquier archivo que hayas dejado en la carpeta inst/ aparecerá en el directorio principal ahora. A veces querremos acceder a estos archivos dentro de nuestras funciones. Esto puede ser un poco complicado. La práctica más recomendable es hacerlo usando la función:\nsystem.file(\"clima.csv\", package = \"psicotuiteR\")`\nEn la línea de código anterior, estaremos accediendo al archivo clima.csv, que hemos incluido en la carpeta inst/, pero que al instalar el paquete se encontrará en ~/Documents/R/win-library/4.0 (al menos en mi ordenador).\n\n\n\n\n\n\nImportant\n\n\n\nSi la función no encuentra ese archivo devolverá \"\", en lugar de un error. Esto puede llevar a confusión. Si quieres que la función devuelva un error si no encuentra el archivo, añade mustWork = TRUE a los argumentos:\nsystem.file(\"clima.csv\", package = \"psicotuiteR\", mustWork = TRUE)`"
  },
  {
    "objectID": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#manteniendo-y-compartiendo-el-paquete",
    "href": "posts/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#manteniendo-y-compartiendo-el-paquete",
    "title": "Creando un paquete de R: una guía informal (I)",
    "section": "Manteniendo y compartiendo el paquete",
    "text": "Manteniendo y compartiendo el paquete\nGit y GitHub\nUn paquete de R puede volverse complejo en poco tiempo: scripts con muchas funciones, funciones que dependen de otras funciones, funciones dependen de funciones de otros paquetes… Es fácil liarla. Git es una buena herramienta para controlar cómo va cambiando el paquete. Te permite llevar la cuenta de cómo ha cambiado cada archivo dentro del paquete, poder volver a una versión específica del mismo archivo, o tener diferentes versiones del mismo paquete funcionando a la vez de forma independiente. Este último punto es especialmente útil si queremos “jugar” con una versión de prueba del paquete mientras otras personas se pueden descargar una versión estable del mismo. Todas estas utilidades se conocen como control de versiones, una versión sofisticada y menos cortoplacista de crear un millón de archivos de similar contenido y nombres incrementalmente más creativos con el fin de no perder contenido.\nPor otro lado, GitHub es una red social que permite almacenar y compartir repositorios mediante control de versiones mediante Git9. Varias personas pueden acceder al repositorio cuando está alojado en GitHub (un paquete de R, en nuestro caso) y sugerir cambios como si estuvieran trabajando sobre dicho paquete en un sólo ordenador. Por desgracia, el uso de git o GitHub está fuera del alcance de este tutorial por varios motivos10. El mejor manual que conozco (y también el más accesible) para aprender a usar Git y GitHub (especialmente para quien ya trabaja en R) es Happy Git and GitHub for the useR, de Jenny Bryan. Aprender Git no siempre es fácil pero siempre merece la pena11.9 No es la única plataforma disponible para hacer esto: Gitlab y Bitbucket, entre otras, hacen lo mismo, aunque son menos populares. Además, por si esto calma alguna conciencia inquita, no son propiedad de Microsoft, al contrario de GitHub.10 El primero de todos siendo que no tengo la confianza suficiente como para hacerlo (yo mismo la lío con Git cada día). El segundo es que aunque stuviera esa confianza, me daría infinita pereza hacerlo. Hacedme caso y echad un vistazo el libro que recomiendo.11 En mi honesta, humilde, ignorable opinión.\nGit y GitHub cumplen una función muy especial para quienes hacemos un paquete en R: la función install_github, del paquete {devtools}12, permite instalar un paquete sin necesidad de que esté publicado en CRAN. Hablaremos en otro tutorial sobre CRAN, pero por ahora nos interesa saber que podemos compartir cualquier paquete a través de GitHub usando install_github, pero para poder instalarlo usando install.packages, como normalmente hacemos, ese paquete necesita estar publicado en CRAN. Publicar en CRAN requiere un proceso de revisión que en ocasiones es díficil solventar (y a veces innecesario). Para compartir nuestro paquete sin necesidad de pasar por ese calvario, lo haremos a través de GitHub.12 Más que un paquete de R, devtools es una collección de funciones de otros paquetes que han sido agrupadas por su utilidad a la hora de desarrollar paquetes (“devtools” es la abreviatura de developer tools). La función install_github pertenece, originalmente, al paquete {remotes}\nPara hacerlo, primero debemos crear un usuario de GitHub, crear un nuevo repositorio, hacer click en el botón verde que dice Code y finalmente copiar el enlace que aparece.\nEn nuestra sesión de R, ejecutamos las siguientes líneas de código:\nuse_git() # esta línea inicializa Git en el repositorio\nuse_git_remote(name = \"origin\", url = \"https://github.com/gongcastro/psicotuiteR.git\") # sustituye ese link por el que hayas copiado de GitHub\nuse_github_ignore() # crea un archivo llamado .gitignore que indica a Git qué archivos ignorar\ngit_vaccinate() # añade más cosas a .gitignore para evitar subir información sensible a GitHub\nPoniendo a prueba el paquete con {testthat} y test()\nLos cambios que introducimos en nuestras funciones de R pueden provocar que fallos que a veces no detectamos inmediatamente. Algunos fallos no producen un error, sino que hacen que nuestras funciones se comporten de forma diferente a la que esperamos. Por ejemplo, un data.frame que devuelve nuestra función podría contener una variable con una clase character en lugar de logical. Este comportamiento indeseado podría pasar desapercibido cuando probemos las funciones que hemos cambiado. Para detectar estos problemas debemos poner a prueba todo el código cada vez que hacemos cambios. Hacer esto de forma manual cada vez puede ser muy tedioso. El paquete {testthat} se encarga de hacer esto por nosotros.\nSi ejecutamos use_testhat, se creará una carpeta llamada tests en nuestro repositorio principal. Dentro de esta carpeta, hay otra carpeta llamada testthat. Cualquier script de R que guardemos en esa carpeta se ejecutará automáticamente cuando ejecutemos test() en nuestra consola. Estos scripts deberían seguir tener el siguiente contenido:\ntest_that(\"Los datos de clima se cargan correctamente\", {expect_error(data(\"clima\"), NA)})\nEn esta línea de código estamos creando un test mediante la función test_that (del paquete {testhat}). Primero incluimos un mensaje que indique qué estamos “testeando” en específico (en este caso, que podemos cargar el dataset clima sin error). La función expect_error ejecuta el código de dentro, y evalúa si el resultado se corresponde con lo que indiquemos en el segundo argumento (en nuestro caso NA, que significa que no hay fallo). Para ver con más detalle cómo poner a prueba el paquete que has creado y aprender buenas prácticas en este tema puedes ver la documentación del paquete {testhat}."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#tldr",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#tldr",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "TLDR",
    "text": "TLDR\n\nWe need to import several CSV or TXT files and merge them into one data frame in R. Regardless of what function we use to import the files, vectorising the operation using purrr::map in combination with do.call or dplyr::bind_rows is the most time-efficient method (~25 ms importing 50 files with 10,000 rows each), compared to for loops (~220ms) or using lapply (~123 ms). data.table::fread is the fastest function for importing data. Importing TXT files is slightly faster than importing CSV files."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Why this post",
    "text": "Why this post\nTo analyse data in any programming environment, one must first import some data. Sometimes, the data we want to analyse are distributed across several files in the same folder. I work with eye-tracking data from toddlers. This means that I work with multiple files that have many rows. At 120 Hz sampling frequency, we take ~8.33 samples per second. A session for one participants can take up to 10 minutes. So these files are somewhat big. These data also tend to be messy, requiring a lot of preprocessing. This means that I need to import the same large files many times during the same R session when wrangling my way through the data, which takes a few seconds. After some iterations, it can be annoying. I have decided to invest all my lost time into analysing what method for importing and merging large files is the fastest in R so that the universe and I are even again.\nBelow I provide several options for importing data from the different files, using base R and tidyverse, among other tools. I will compare how long it takes to import and merge data using each method under different circumstances. You can find the whole code here in case you want to take a look, reproduce it or play with it1.1 Ironically, this code is super inefficient and messy. It takes ages to run, and has been written by copy-pasting multiple times. I didn’t feel like doing anything more elegant. Also, I don’t know how. Help yourself."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "How can I import large files and merge them?",
    "text": "How can I import large files and merge them?\nSo we have some files in a folder. All files have the same number of columns, the same column names, and are in the same format. I assume that data are tabular (i.e., in the shape of a rectangle defined by rows and columns). I also assume that data are stored as Comma-Separated Values (.csv) or Tab-separated Text (.txt or .tsv), as these formats are the most reproducible.\nWe to import all files and bind their rows together to form a unique long data frame. There are multiple combinations of functions we can use. Each function comes with a different package and does the job in different ways. Next, I will show some suggestions, but first let’s create some data. We are creating 50 datasets with 10 columns and 10,000 rows in .txt format. The variables included are numeric and made of 0s and 1s. There is also a column that identifies the data set. These files are created in a temporary directory using the temp.dir function for reproducibility. After closing you R session, this directory and all of its contents will disappear.\n\n\n\nBase R: for loops\nfor loops are one of the fundamental skills in many programming languages. The idea behind for loops is quite intuitive: take a vector or list of length n, and apply a series of functions to each element in order. First, to element 1 and then to element 2, and so on, until we get to element n. Then, the loop ends. We will first make a vector with the paths of our files, and then apply the read.delim function to each element of the vector (i.e., to each path). Every time we import a file, we store the resulting data frame as an element of a list. After the loop finishes, we merge the rows of all element of the list using a combination of the functions do.call and rbind.\n\n\n\nBase R: lapply\n\nWe will use the functions read.delim and read.csv in combination with the function lapply. The former are well known. The later is part of a family of functions (together with sapply, mapply, and some others I can’t remember) that take two arguments: a list and a function, which will be applied over each element of the list in parallel (i.e., in a vectorised way).\n\n\n\nTidyverse\nThe tidyverse is a family of packages that suggests a workflow when working in R. The use of pipes (%>%) is one of its signature moves, which allow you to chain several operations applied on the same object within the same block of code. In contrast, base R makes you choose between applying several functions to the same object in different blocks of code, or applying those functions in a nested way, so that the first functions you read are those applied the last to your object (e.g., do.call(rbind, as.list(data.frame(x = \"this is annoying\", y = 1:100)))). We will use a combination of the dplyr and purrr packages to import the files listed in a vector, using read.delim and bind_rows.\n\n\n\ndata.table\nThe function rbindlist function from the package data.table also allows to merge the datasets contained in a list. In combination with fread (from the same package), it can be very fast."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "What method is the fastest?",
    "text": "What method is the fastest?\nI will compare how long each combination of importing, vectorising, and merging functions needs to import 50 data sets with 10 columns and 10,000 rows each. Additionally, I will compare the performance of each method when working with CSV (.csv) and TSV (.txt) files. For each method, I will repeat the process 100 times, measuring how long it takes from the moment we list the extant files in the folder to the moment we finish merging the data sets. Here are the results:\n\n\n\n\nFigure 1: Mean time (and standard deviation) for each combination of methods and file formats across 100 replications\n\n\n\n\nFor more detail:\n\n\n\n\n\n\n\nTable 1:  Execution times \n  \n\nTime taken to import and merge\n    \n\n50 datasets with 10 columns and 10,000 rows each\n    \n\n\n\n\n      package\n      \n        for loop\n      \n      \n        lapply\n      \n      \n        purrr::map\n      \n    \n\nM\n      SD\n      M\n      SD\n      M\n      SD\n    \n\n\n\ndo.call - .csv\n    \n\n\nbase\n1.34\n0.06\n1.11\n0.07\n0.15\n0.03\n\n\n\ndata.table\n1.33\n0.39\n0.88\n0.04\n0.17\n0.02\n\n\n\nreadr\n1.38\n0.05\n1.07\n0.13\n0.15\n0.02\n\n\ndplyr::bind_rows - .csv\n    \n\n\nbase\n1.35\n0.16\n0.99\n0.14\n0.15\n0.01\n\n\n\ndata.table\n1.14\n0.04\n0.84\n0.06\n0.16\n0.01\n\n\n\nreadr\n1.25\n0.14\n0.91\n0.04\n0.14\n0.00\n\n\ndo.call - .txt\n    \n\n\nbase\n1.18\n0.04\n0.88\n0.04\n0.17\n0.02\n\n\n\ndata.table\n1.17\n0.04\n0.80\n0.05\n0.15\n0.02\n\n\n\nreadr\n1.18\n0.05\n0.80\n0.05\n0.15\n0.02\n\n\ndplyr::bind_rows - .txt\n    \n\n\nbase\n1.13\n0.03\n0.84\n0.06\n0.20\n0.02\n\n\n\ndata.table\n1.19\n0.13\n0.77\n0.03\n0.14\n0.01\n\n\n\nreadr\n1.13\n0.04\n0.77\n0.03\n0.14\n0.01\n\n\nMean\n—\n1.23\n0.10\n0.89\n0.06\n0.15\n0.02\n\n\n\n\n\n\n\nFigure 1 and Table Table 1 show the detailed timings The grand mean average time taken by all methods is ~2.12 seconds, but there are some differences.\n\nIt doesn’t really matter what function we use to merge data sets: both do.call and dplyr::bind_rows perform roughly similarly.\nWhat makes the biggest difference is what function we use to vectorise the importing operation across file names to import them. purrr::map is the fastest. Incredibly, is takes less than 0.3 seconds in all conditions. It is also the least sensitive to the format of the files and the function we use to import them.\nThe next vectorising function in terms of temporal efficiency is lapply, which takes ~1.5 seconds. It performs slightly better when working with .txt files, in that when working with .csv files its performance depends on what method we use to import them: data.table::fread is much faster than its base and readr competitors. This post by Daniele Cook sheds some light into the advantage of data.table over other importing functions, also covering the vroom package, which this post doesn’t cover.\nUsing for loops looks like the least efficient method for iterating across data sets when importing data. It also shows a similar profile than lapply: data.table::fread performs a bit better than the rest."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#conclusion",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#conclusion",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Conclusion",
    "text": "Conclusion\nUnder the scenario under which I have simulated the data, it seems that using purrr::map in combination with do.call or dplyr::bind_rows to merge data sets is the most efficient method in terms of time. When using said combination, it doesn’t matter what function we use to import files, but data.table::fread seems like the best choice, as it is also the most flexible (take a look at the documentation of data.table to see all the features it offers).\nIf I have time, I may add another two dimensions: number of rows in the files and number of files, although I dare say similar results are to be expected. If anything, I would say that differences may become greater as file size and number of files increase. Also, it would be interesting to test if pre-allocating the elements of the vector in the for loop speeds up the process (see here what I mean). We shall see.\nHope this was useful, if not interesting!"
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#code",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#code",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#session-info",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#session-info",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.8.0          ggsci_2.9         forcats_0.5.2     stringr_1.5.0    \n [5] readr_2.1.3       tidyr_1.2.1       tibble_3.1.8      ggplot2_3.4.0    \n [9] tidyverse_1.3.2   data.table_1.14.6 purrr_1.0.0       dplyr_1.0.10     \n[13] quarto_1.2       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9          lubridate_1.9.0     ps_1.7.2           \n [4] assertthat_0.2.1    digest_0.6.31       utf8_1.2.2         \n [7] R6_2.5.1            cellranger_1.1.0    backports_1.4.1    \n[10] reprex_2.0.2        evaluate_0.19       httr_1.4.4         \n[13] pillar_1.8.1        rlang_1.0.6         googlesheets4_1.0.1\n[16] readxl_1.4.1        rstudioapi_0.14     rmarkdown_2.19     \n[19] labeling_0.4.2      googledrive_2.0.0   munsell_0.5.0      \n[22] broom_1.0.2         compiler_4.2.2      modelr_0.1.10      \n[25] xfun_0.36           pkgconfig_2.0.3     htmltools_0.5.4    \n[28] tidyselect_1.2.0    fansi_1.0.3         crayon_1.5.2       \n[31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n[34] later_1.3.0         commonmark_1.8.1    grid_4.2.2         \n[37] jsonlite_1.8.4      gtable_0.3.1        lifecycle_1.0.3    \n[40] DBI_1.1.3           magrittr_2.0.3      scales_1.2.1       \n[43] cli_3.5.0           stringi_1.7.8       farver_2.1.1       \n[46] renv_0.15.4         fs_1.5.2            xml2_1.3.3         \n[49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.5.1        \n[52] tools_4.2.2         glue_1.6.2          hms_1.1.2          \n[55] processx_3.8.0      fastmap_1.1.0       yaml_2.3.6         \n[58] timechange_0.1.1    colorspace_2.0-3    gargle_1.2.1       \n[61] rvest_1.0.3         knitr_1.41          haven_2.5.1        \n[64] sass_0.4.4"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "title": "How similar is the word “mask” across languages?",
    "section": "",
    "text": "The ubiquity of masks has given psycholinguists a frequent-ish stimulus to use in experiments. This word is more form-similar across languages than one may think. I gathered a big-ish dataset with translation equivalents of the word mask across ~110 languages. I tweeted about this today, and wanted to dedicate some more lines to nuance.\nHere’s the data:\nTo compute the similarity of each pair of translation equivalents, I followed Floccia et al.’s (2018) procedure. For each pair of translation equivalents, I computed their Levenshtein distance as the number of insertions, deletions and replacements a string character has to go through to become identical to the other, and then divided this value by the number of characters of the longest of the two strings, so that all values range between 0 and 1. To compute the Levenshtein distance, I used the stringdist() function of the stringdist R package."
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Orthographic distance",
    "text": "Orthographic distance\nI first computed the orthographic distance between each pair of translation equivalents. Since some word forms make use of different alphabets, I first romanised all word forms. By romanised, I mean that I searched for the transcription of each word form in the Roman alphabet, and used it as input to compute the Levenshtein distance for each pair of translation equivalents. Here’s how orthographically similar (the romanisations of) the translations of mask are (N = 110 pairs):"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Phonological distance",
    "text": "Phonological distance\nThe phonological similarity/distance may be more informative. This time I searched for or generated with the help of a native speaker a phonological IPA transcription of each word-form. I then used this transcription as input to compute the phonological similarity of each pair of translation equivalents. A pitfall in this process is the fact that phonemes are almost never identical across languages, so even the common phoneme /m/ could vary slightly on its pronunciation in two languages. If this difference is encoded in the IPA transcription (as different characters), the Levenshtein distance will be inflated. For this reason, I simplified some IPA transcriptions to preserve this similarity. I also removed tones. This is terribly wrong from a linguistics perspective, but it’s the only way I see to be able to play with some reliable data. Also I’m no linguist, so you have no power here.\n\nHere’s the same analysis performed on phonological transcriptions of a subset of those languages (N = 75 pairs, those I could find a reliable IPA transcription for or could find help from a native speaker):"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "title": "How similar is the word “mask” across languages?",
    "section": "Onsets",
    "text": "Onsets\nMost of the times, the phonological overlap comes from onset graphemes/phonemes. This is how many word-forms start with each onset:"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "title": "How similar is the word “mask” across languages?",
    "section": "Some disclaimers:",
    "text": "Some disclaimers:\nI tried ensuring that words referred to surgical masks (instead of other types of masks) with help from native speakers. Wrong translations may still have slipped in (or be just wrong). I wish I had time to double-check all of them (I did this for fun).\nThis analysis is probably affected by selection bias. I suspect many dissimilar translations are missing due to not being included in the translation apps I used (e.g. Google Translate). Feel free to contribute missing entries or make corrections!"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "title": "How similar is the word “mask” across languages?",
    "section": "Code and data",
    "text": "Code and data"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "title": "How similar is the word “mask” across languages?",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] htmltools_0.5.4  kableExtra_1.3.4 knitr_1.41       gt_0.8.0        \n [5] readxl_1.4.1     forcats_0.5.2    stringr_1.5.0    dplyr_1.0.10    \n [9] purrr_1.0.0      readr_2.1.3      tidyr_1.2.1      tibble_3.1.8    \n[13] ggplot2_3.4.0    tidyverse_1.3.2  quarto_1.2      \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.4          sass_0.4.4          jsonlite_1.8.4     \n [4] viridisLite_0.4.1   modelr_0.1.10       assertthat_0.2.1   \n [7] renv_0.16.0         googlesheets4_1.0.1 cellranger_1.1.0   \n[10] yaml_2.3.6          pillar_1.8.1        backports_1.4.1    \n[13] glue_1.6.2          digest_0.6.31       rvest_1.0.3        \n[16] colorspace_2.0-3    pkgconfig_2.0.3     broom_1.0.2        \n[19] haven_2.5.1         scales_1.2.1        webshot_0.5.4      \n[22] processx_3.8.0      svglite_2.1.0       stringdist_0.9.10  \n[25] later_1.3.0         tzdb_0.3.0          timechange_0.1.1   \n[28] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[31] withr_2.5.0         cli_3.6.0           magrittr_2.0.3     \n[34] crayon_1.5.2        evaluate_0.19       ps_1.7.2           \n[37] fs_1.5.2            fansi_1.0.3         xml2_1.3.3         \n[40] tools_4.2.2         hms_1.1.2           gargle_1.2.1       \n[43] lifecycle_1.0.3     munsell_0.5.0       reprex_2.0.2       \n[46] compiler_4.2.2      systemfonts_1.0.4   rlang_1.0.6        \n[49] grid_4.2.2          rstudioapi_0.14     rmarkdown_2.19     \n[52] gtable_0.3.1        DBI_1.1.3           R6_2.5.1           \n[55] lubridate_1.9.0     fastmap_1.1.0       utf8_1.2.2         \n[58] commonmark_1.8.1    stringi_1.7.8       parallel_4.2.2     \n[61] Rcpp_1.0.9          vctrs_0.5.1         dbplyr_2.2.1       \n[64] tidyselect_1.2.0    xfun_0.36"
  },
  {
    "objectID": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "href": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "title": "A primer on Mixed-Effects Models: Theory and practice",
    "section": "",
    "text": "When I started my PhD I had to get familiar with Linear Mixed Models very well very quickly. Then I was asked to present what I learnt and prepare this informal tutorial for my colleagues, which I presented on March 10th, 2020 (yes, early pandemic :confounded:). This post shares the result.\nThis is not supposed to be taken as a formal guide to linear mixed-effects models, but rather as a semi-coherent compilation of notes and self-suggestions that I considered worth sharing with my lab mates during my early stages of in the PhD. Here are the slides:\nWhat I should be taken more seriously are (1) the references I suggest in the first slides (which I consider some of the best resources available to learn linear mixed-effects models), and (2) the memes, which I personally curated and even created to sweeten up the dreadful incoherence of the content of some of the slides (sorry about that).\nBefore the presentation I tweeted one of the animations I generated for it.\nThis tweet got some attention (for my usual numbers) and many kind folks have asked for the R code or the GIF file of the specific animation included in the tweet, so here they are (you’ll also find them in the GitHub repository) in a perhaps more comfortable format, ready to be cloned or downloaded):"
  },
  {
    "objectID": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "href": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "title": "A primer on Mixed-Effects Models: Theory and practice",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] xaringanExtra_0.7.0 quarto_1.2         \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9      ps_1.7.2        digest_0.6.31   later_1.3.0    \n [5] lifecycle_1.0.3 jsonlite_1.8.4  magrittr_2.0.3  evaluate_0.19  \n [9] stringi_1.7.8   rlang_1.0.6     cli_3.6.0       renv_0.16.0    \n[13] rstudioapi_0.14 vctrs_0.5.1     rmarkdown_2.19  tools_4.2.2    \n[17] stringr_1.5.0   glue_1.6.2      compiler_4.2.2  xfun_0.36      \n[21] yaml_2.3.6      fastmap_1.1.0   processx_3.8.0  htmltools_0.5.4\n[25] knitr_1.41"
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html",
    "href": "posts/psicotuiterbot/psicotuiterbot.html",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "",
    "text": "He creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON."
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "href": "posts/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Escribiendo el código",
    "text": "Escribiendo el código\nUn bot de Twitter no es más que un código que se ejecuta automáticamente de forma periódica (ej., cada 15 minutos) y realiza una acción en Twitter a través de una cuenta (hacer un RT o responder a un tweet). Para poder realizar esta acción a través del código, es necesario tener acceso a la API de Twitter. API es un acrónimo para Application Programming Interface y como dice su nombre, es una plataforma desde la que podemos interactuar con una aplicación (en este caso Twitter) a través de programación con una serie de comandos que el equipo de Twitter ha diseñado la API ha definido.\nNunca había hecho un bot de Twitter, pero sabía de la existencia de bastantes tutoriales para hacerlo. La mayoría de los bots de Twitter (y por tanto de los tutoriales) están escritos en Python y JavaScript, pero yo me encuentro algo más cómodo con R. Además gran parte de Psicotuiter (especialmente quienes están relacionades con la metodología) también está más familiarizado con R. Mi intención era hacer el bot lo más trasparente y accesible para la comunidad, así que me decanté por R. En un pricipio seguí el tutorial de Matt Dray, en el que utiliza el paquete de R {rtweet} para interactuar con la API de Twitter. Hay más tutoriales que usan rtweet para crear un bot con R. Pero a diferencia de otros, este tutorial explicaba cómo usar GitHub actions para ejecutar el código de forma periódica. Ahora explico esto último. Mientras tanto, vamos al código de R.\nPrimero creé un repositorio de GitHub donde alojar el código (GitHub es como un Google Drive especializado en código donde además podemos hacer control de versiones de los archivos que subimos). Puedes echar un vistazo al código de R en la carpeta R/. El código principalmente recoge los últimos tweets que se han escrito en las últimos 6 horas mencionando #psicotuiter o #psicotwitter y los retuitea. No me meteré en detalle a explicar cómo funciona el código, pero aquí va un pequeño resumen del programa. Si tienes curiosidad te recomiendo explorar el repositorio de GitHub, que contiene todo lo necesario para hacer funcionar el bot:\nPrimero cargamos el paquete de R {dplyr}, que utilizamos en bastantes ocasiones en el programa, ajustamos un pequeño detalle relacionado con el comportamiento de la API de Twitter, e instalamos los paquetes necesarios (en caso de que hayan cambiado) usando el pqeute de R {renv} (si no lo conoces y te interesa la reproducibilidad computacional tienes que echarle un vistazo).\n# bot R code\nlibrary(dplyr)\noptions(httr_oob_default = TRUE)\n# restore packages\nrenv::restore()\nA continuación extraemos las credenciales que necesitamos para acceder a la API de Twitter (las paso a como variables de entorno para evitar hacerlas públicas, ya que eso dería acceso a cualquiera a la cuenta de Twitter del bot).\n# authenticate Twitter API\nmy_token <- rtweet::create_token(\n    app = \"psicotuiterbot\",  # the name of the Twitter app\n    consumer_key = Sys.getenv(\"TWITTER_CONSUMER_API_KEY\"),\n    consumer_secret = Sys.getenv(\"TWITTER_CONSUMER_API_KEY_SECRET\"),\n    access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n    access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"), \n    set_renv = FALSE\n)\nLuego extraemos los tweets usando rtweet, filtramos los que sean relevantes y no contengan posible contenido ofensivo (y algún otro filtro más).\nlibrary(dplyr)\n# define hashtags\nhashtags_vct <- c(\"#psicotuiter\", \"#psicotwitter\", \"#Psicotuiter\", \"#Psicotwitter\", \"#PsicoTuiter\", \"#PsicoTwitter\")\nhashtags <- paste(hashtags_vct, collapse = \" OR \")\nhate_words <- unlist(strsplit(Sys.getenv(\"HATE_WORDS\"), \" \")) # words banned from psicotuiterbot (separated by a space)\nblocked_accounts <- unlist(strsplit(Sys.getenv(\"BLOCKED_ACCOUNTS\"), \" \")) # accounts banned from psicotuiterbot (separated by a space)\ntime_interval <- lubridate::now(tzone = \"UCT\")-lubridate::minutes(120)\n# get mentions to #psicotuiter and others\nall_tweets <- rtweet::search_tweets(\n    hashtags, \n    type = \"recent\", \n    token = my_token, \n    include_rts = FALSE, \n    tzone = \"CET\"\n) \nstatus_ids <- all_tweets %>% \n    filter(\n        !(screen_name %in% gsub(\"@\", \"\", blocked_accounts)),\n        created_at >= time_interval, # 15 min\n        !grepl(paste(hate_words, collapse = \"|\"), text), # filter out hate words\n        stringr::str_count(text, \"#\") < 4, # no more than 3 hashtags\n        lang %in% c(\"es\", \"und\") # in Spanish or undefined language\n    ) %>% \n    pull(status_id)\n# get request ID\nrequest_tweets <- rtweet::get_mentions(\n    token = my_token, \n    tzone = \"CET\"\n) \nFinalmente, hacemos RT uno a uno usando rtweet (si ya habíamos hecho RT a uno de ellos simplemente se ignora).\nif (nrow(request_tweets) > 0) {\n    request_ids <- request_tweets %>% \n        filter(\n            created_at >= time_interval, # 15 min\n            grepl(\"@psicotuiterbot\", text),\n            grepl(\"rt|RT|Rt\", text),\n            !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n        ) %>% \n        pull(status_in_reply_to_status_id)\n    \n    # get requested IDS\n    if (length(request_ids) > 0) {\n        requested_ids <- rtweet::lookup_statuses(request_ids, token = my_token) %>% \n            filter(\n                !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n            ) %>% \n            pull(status_id)\n    } else {\n        requested_ids <- NULL\n    }\n} else {\n    requested_ids <- NULL\n}\n# RT all IDs\nif (length(status_ids) > 0){\n    for (i in 1:length(status_ids)){\n        rtweet::post_tweet(\n            retweet_id = unique(status_ids)[i], # vector with IDs\n            token = my_token\n        )\n    }\n    print(paste0(length(status_ids), \" RT(s): \", paste(status_ids, collapse = \", \")))\n} else {\n    print(\"No tweets to RT\")\n}\nRecientemente incluí un pequeño bloque de código para permitir que la gente solicitase un RT para tuits que no mencionaban #psicotuiter, pero podrían ser de interés para la comunidad. Para hacerlo solo hay que responder al tuit en cuestión mencionando a @psicotuiterbot junto con la palabra RT (ej., “RT @psicotuiterbot por favor”).\n# tweet requests\nif (length(requested_ids) > 0){\n    for (i in 1:length(requested_ids)){\n        rtweet::post_tweet(\n            retweet_id = unique(requested_ids)[i], # vector with IDs\n            token = my_token\n        )\n    }\n    print(paste0(length(requested_ids), \" request(s) posted: \", paste(requested_ids, collapse = \", \")))\n} else {\n    print(\"No requests\")\n}"
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html#ejecutando-el-código",
    "href": "posts/psicotuiterbot/psicotuiterbot.html#ejecutando-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Ejecutando el código",
    "text": "Ejecutando el código\nLo que hace “bot” a un bot es que no requiere intervención manual para que realice la acción que deseamos. Hay muchas opciones para conseguir esto, pero casi todas tienen un inconveniente: necesitamos que una máquina (ordenador/servidor, móvil, etc.) esté encendido en el momento en el que queremos ejecutar nuestro código. En nuestro caso necesitamos que se ejecute cada 15 minutos, lo que implica que deberíamos tener un dispositivo conectado a la corrriente y funcionando todo el día. Tener mi ordenador personal haciendo esto no es una opción viable. Me encontré con dos alternativas.\n\nPrimer intento (sale mal): GitHub actions\nComo mencioné antes, en el tutorial de Matt Dray se ilustra cómo usar GitHub Actions para ejecutar nuestro código una vez está alojado en un repositorio de GitHub. GitHub Actions es un servicio que ofrece GitHub que permite ejecutar ciertos comandos en determinadas condiciones o cada cierto tiempo usando un servidor que ponen a nuestra disposición (con ciertos límites). Este grupo de comandos se denominan workflows o flujos de trabajo, y si los incluimos en una carpeta de nuestro repositorio llamada .github/workflows/ siguiendo cierto formato en un archivo YAML (.yml), GitHub se encargará de ejecutarlo sin nuestra intervención. Hay buenos tutoriales sobre cómo y cuándo escribir workflows para GutHub Actions. El workflow principal era inicialmente este:\nname: bot\non:\n  push:\n    branches:\n      - main # run every time there is a push to main branch\n      - test\njobs:\n  psicotuiterbot-post:\n    runs-on: macOS-latest\n    env: #  twitter API keys (used to authenticate) defined in the gh actions environment\n      TWITTER_CONSUMER_API_KEY: ${{ secrets.TWITTER_CONSUMER_API_KEY }}\n      TWITTER_CONSUMER_API_KEY_SECRET: ${{ secrets.TWITTER_CONSUMER_API_KEY_SECRET }}\n      TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}\n      TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}\n      \n    steps:\n      - uses: actions/checkout@v2\n      - uses: r-lib/actions/setup-r@v1\n      - uses: r-lib/actions/setup-renv@v1\n        with:\n          cache-version: 1\n      - name: Restore packages using renv\n        shell: Rscript {0}\n        run: |\n          if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::restore()\n      - name: Create and post tweet\n        run: Rscript R/bot.R\nLa gran ventaja de usar este sistema es que no necesitamos usar nuestro ordenador personal, ya que usamos el que que GitHub nos asigna (un servidor no deja de ser un ordenador). Pero tiene varios inconvenientes. El primero es que el proceso de establecer un workflow en GitHub Actions suele requerir varios intentos (en mi caso muchos). Esto suele deberse a problemas de reproducibilidad computacional: el código funciona correctamente en mi ordenador porque en él tengo instalado todo el sofware del que depende. Cuando uso el servidor de GitHub, el sistema operativo suele necesitar que instalemos estas dependencias antes de ejecutar el código. GitHub Actions permite cierta flexibilidad a la hora de seleccionar el software que viene instalado en el sistema operativo que vamos a usar (ej., R, compiladores de C++, dependencias de Linux, etc.). El problema es que muchas veces ni siquiera somos conscientes de cuántas dependencias requiere nuestro código. Con paciencia y muchas búsquedas de Google es posible solventar este problema.\nUn segundo inconveniente que encontré a la hora de implementar el bot en GitHub Actions tiene que ver con los tiempos: instalar todas las dependencias del código en el servidor cada 15 minutos (la configuración se pierde casi totalmente tras cada ejecución) es poco eficiente. Instalar las dependencias puede tardar más de 10 minutos (en el caso de este bot). Esto puede además hacer fallar en ocasiones el flujo de trabajo. GitHub Actions tampoco es lo más consistente del mundo, aunque no deja de ser gratis."
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "href": "posts/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Segundo intento (sale bien): Raspberry Pi",
    "text": "Segundo intento (sale bien): Raspberry Pi\nTras varios problemas en la ejecución del bot a través de GitHub Actions, decidí cambiar de método. Por razones ajenas al bot, hacía unos meses que tenía muerta de aburrimiento una Raspberry Pi 4 que compré con un amigo para jugar con ella. Este dispositivo es un mini-ordenador relativamente barato (~40€) que salió al mercado como herramienta educativa para enseñar a programar (ej., róbotica para niñes) pero que poco a poco ha ido tomando espacio en lugares de producción. Tiene mil posibilidades por su simplicidad y, en nuestro caso, por su bajo consumo: tener una Raspberry Pi funcionando todo el día apenas tiene impacto sobre el consumo de luz.\n\n\n\nAquí está alojado el @psicotuiterbot\n\n\nPrimero instalé el código en la Raspberry con sus dependencias: básicamente, cloné el repositorio de GitHub en una carpeta dentro de home/Documents/. Para ejecutar el código cada 15 minutos utilicé una función muy útil que incluye Linux (sistema operativo con el que funciona la Raspberry) llamado CRON. Simplemente consiste en un archivo en el que incluimos una serie de comandos que queremos que se ejecuten de forma periódica, junto con una código que indica la periodicidad de la ejcución de este comando. Aquí tienes unos ejemplos. Incluí cuatro comandos (cada uno en su propio archivo con extensión .sh, que denota comandos de Linux):\n# descarga el código de GitHub, por si ha habido cambios\ngit pull origin main\n# ejecuta el código principal del bot\nTZ=\"Spain/Madrid\" Rscript -e 'source(\"R/bot.R\")'\n# guarda los tweets detectados en un archivo y crea un gráfico\nRscript -e 'source(\"R/counts.R\")'\nrm Rplots.pdf\n# sube los nuevos datos a GitHub\ngit add .\ngit commit -m \"Update repository\"\ngit push\nEstos comandos se ejecutan en este orden cada 15 minutos."
  },
  {
    "objectID": "posts/renv-package/renv-package.html",
    "href": "posts/renv-package/renv-package.html",
    "title": "renv (o cómo usar paquetes de R sin ataques de pánico)",
    "section": "",
    "text": "A veces necesitamos instalar versiones diferentes del mismo paquete de R en proyectos diferentes. El paquete {renv} nos permite almacenar los paquetes de R de cada proyecto de forma independiente, evitando posibles conflictos entre proyectos. De paso, incrementará la reproducibilidad computacional de nuestro código.\n\n\n\nEl problema\nPonte en la siguiente situación: tienes entre manos un proyecto de R que necesita varios paquetes. Cada uno de estos paquetes depende, a su vez, de terceros paquetes. De hecho, dos paquetes pueden depender del mismo paquete, o incluso de versiones diferentes del mismo paquete. Si hay mala suerte, una de las versiones no será lo suficientemente reciente como para funcionar correctamente con ambos paquetes. Resultado: uno de los dos paquetes no funcionará.\n\n\n\nUsuarie de R promedio después de tirar dos horas a la basura intentando instalar los paquetes que necesita para trabajar en un proyecto de R que no tocaba desde hacía cuatro meses.\n\n\nEste problema se extiende al caso de que necesitemos versiones diferentes del mismo paquete en proyectos de R diferentes en los que estamos trabajando de forma simultánea en el mismo ordenador. Para hacerlos funcionar necesitaríamos instalar de nuevo la versión correspondiente del mismo paquete cada vez que cambiemos de proyecto.\n\n\nInstalando paquetes de R\nEn resumidas cuentas, cada vez que instalamos o actualizamos un paquete de R, lo hacemos para todos nuestros proyectos de R de forma global. Esto se debe a que por defecto R busca todos los paquete de R en la misma carpeta. Para ver dónde instala R tus paquetes puedes ejecutar el siguiente comando:\n.libPaths()\nEste comando te mostrará el directorio o directorios donde R instala sus paquetes por defecto. Si hay más de un directorio significa que, en caso de que no sea posible encontrar un paquete en el primer directorio, R lo buscará en el segundo, tercero, etc., hasta que te devuelva un error indicando que no has instalado ese paquete.\nSi accedes al primer directorio que muestra .libPaths() verás que cada paquete tiene una carpeta. Cada carpeta incluye el código de R, los datos y la documentación asociada a cada paquete (entre otras cosas). Cada vez que instalamos o actualizamos un paquete, se crea o reemplaza su carpeta correspondiente en nuestro directorio, es decir, en nuestra “biblioteca global” de paquetes de R.\n\n\n\nAsí es como tienes tu carpeta de paquetes de R. Que lo sé yo. Que te he visto. Vergüenza me daría a mí.\n\n\nHemos visto que esto no es ideal. ¿No será mejor tener una carpeta diferente para cada proyecto en la que instalamos sus paquetes de forma independente, sin afectar a los paquetes de otros proyectos? Sí. De hecho este procedimiento es estándar en otros lenguajes de programación como JavaScript1 o Julia 2. Existe un paquete de R que nos permite hacer esto: renv. Veamos cómo funciona.1 Echa un vistazo a este post de Nikola Đuza: Ride Down Into JavaScript Dependency Hell2 Echa un vistazo a este post de Bogumił Kamiński: My practices for managing project dependencies in Julia\n\n\nUsando renv\nPrimero hay que instalar renv. Como está incluido en el CRAN, podemos hacerlo usando install.packages():\ninstall.packages(\"renv\")\nAhora abrimos una sesión de R en la carpeta de nuestro proyecto. Digamos que nuestra carpeta tiene la siguiente estructura:\ndata\n |-some-data.csv\ndocs\n |-index.Rmd\n |-index.html\nR\n |-main.R\n |-functions.R\n.Rprofile\nAsí es la estructura de la mayoría de carpetas de mis proyectos de R. Tiene su razón de ser, pero eso es material para otro post. Lo importante es cómo cambiará esta estructura en unos momentos. La documentación de renv incluye los pasos para usar renv, pero explicaré los principales. Primero inicializaremos renv en nuestra consola de R:\nrenv::init()\nEsto creará una carpeta (renv) y un archivo (renv.lock) nuevos en nuestro directorio:\n.Rprofile\ndata\n    |-some-data.csv\ndocs\n    |-index.Rmd\n    |-index.html\nR\n    |-main.R\n    |-functions.R\nrenv\n    |-.gitignore\n    |-activate.R\n    |-library\n        |-...\n    |-local\n        |-...\n    |-settings.dcf\nrenv.lock\nNo necesitaremos modificar ni consultar nunca ningunos de los archivos creados, pero vamos a curiosear un poco. Al usar init(), renv ha echado un vistazo a los scripts de R de la carpeta (achivos con la extensión .R, como main.R y functions.R), y ha detectado los paquetes que necesita nuestro código para ejecutarse (puedes consultar las dependencias de tu proyecto usando renv::dependencies()). Por ejemplo, si main.R incluye library(dplyr) o dplyr::mutate(), detectará el paquete dplyr como una dependencia.\n\n\n\nrenv detectando tus dependencias.\n\n\nA continuación, renv ha instalado todas los paquetes necesarios en renv/library/. Si comparas esa carpeta con el directorio mostrado en .libPaths() (como hicimos hace un momento), verás que ambas carpetas son muy parecidas. Eso es porque ahora R buscará los paquetes que necesites en esa carpeta, y no en la “biblioteca global” de paquetes de R. Esa es la magia de renv: podrás instalar y actualizar paquetes de R de forma independiente para cada uno de tus proyectos. Para instalar nuevos paquetes deberás hacerlo usando la función renv::install(). Por ejemplo:\nrenv::install(\"tidyr\")\nEsta función es el equivalente a install.packages() en renv. esta función asumirá que el paquete que quieres se encuentra en CRAN y será allí donde lo buscará. Si el paquete que quieres instalar se encuentra alojado en otro sitio (o quieres instalar una versión experimental del mismo, en un repositorio de GitHub, por ejemplo), puedes indicar el repositorio de la siguiente forma:\nrenv::install(\"crsh/papaja\")\nSi echas un vistazo a renv.lock verás que incluye una lista de todas las dependencias de tu proyecto, en un formato un poco raro, con muchos paréntesis, y la extensión .lock. No necesitas entender este archivo, sólo que sigue un formato parecido al que usan otros leguajes de programación para hacer lo mismo. Es el equivalente al archivo package-lock.json de un proyecto de JavaScript o al archivo Manifest.toml de un proyecto de Julia. Si te fijas, verás que simplemente incluye información mínima para cada paquete: nombre, versión, origen y un código que lo identifica. Por ejemplo, el renv.lock de nuestro proyecto incluye lo siguiente:\n{\n  \"R\": {\n    \"Version\": \"4.0.4\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"dplyr\": {\n      \"Package\": \"dplyr\",\n      \"Version\": \"1.0.8\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"ef47665e64228a17609d6df877bf86f2\"\n    },\n    \"papaja\": {\n      \"Package\": \"papaja\",\n      \"Version\": \"0.1.0.9997\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"crsh\",\n      \"RemoteRepo\": \"papaja\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a231c3628ccf24359cc17f11a5bbc743e3fed920\",\n      \"Remotes\": \"tidymodels/broom\",\n      \"Hash\": \"3df0637229690f807616c46d3ff77113\"\n    },\n    \"tidyr\": {\n      \"Package\": \"tidyr\",\n      \"Version\": \"1.2.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"d8b95b7fee945d7da6888cf7eb71a49c\"\n    },\n  }\n}\nUna ventaja enorme de usar renv es que si descargas o copias y pegas esta carpeta en un ordenador diferente (en el que posiblemente tengas una colección de paquetes diferente a la del ordenador donde trabajaste con el proyecto por última vez), renv podrá consultar este archivo para instalar por tí los paquetes necesarios en sus versiones correspondientes. Esto se puede hacer usando el comando:\nrenv::restore()\nImportante: cuando instales nuevos paquetes usando renv::install(), el archivo renv.lock no se actualizará de forma automática. Para incluir los nuevos paquetes en este archivo, tendremos que usar el siguiente comando:\nrenv::snapshot()\nComo podrás imaginar, poder instalar los paquetes que necesita un proyecto en su versión adecuada resuelve uno de los problemas más frecuentes que amenazan la reproducibilidad computacional de nuestros proyectos.\n\n\n\nImaginando un mundo donde todo el mundo se preocupa lo suficiente por la reproducibilidad computacional de sus proyectos.\n\n\n\n\nConclusiones\nTe recomiendo empezar a usar renv en algún proyecto “de juguete” con el que puedas experimentar, e ir poco a poco incorporando esta rutina en tus proyectos por el bien de tu salud mental y de la de les demás. :smile:"
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "title": "Visualising polynomial regression",
    "section": "",
    "text": "The outputs of polynomial regression can be difficult to interpret. I generated some animated plots to see how model predictions change across different combinations of coefficients for 1st, 2nd, and 3rd degree polynomials."
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#why-polynomials",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#why-polynomials",
    "title": "Visualising polynomial regression",
    "section": "Why polynomials",
    "text": "Why polynomials\nWhen modelling data using regression, sometimes the relationship between input variables and output variables is not very well captured by a straight line. A standard linear model is defined by the equation\n\\[y_i = \\beta_{0} + \\beta_{1}x_{i}\\]\nwhere \\(\\beta_{0}\\) is the intercept (the value of the input variable \\(x\\) where the output variable \\(y=0\\)), and where \\(\\beta_{1}\\) is the coefficient of the input variable (how much \\(y\\) increases for every unit increase in \\(x\\)). To illustrate this, let’s imagine we are curious abut what proportion of the students in a classroom are paying attention, and how this proportion changes as minutes pass. We could formalise our model as\n\\(y_i = \\beta_{0} + \\beta_{1} Time_i\\)\nLet’s generate some data to illustrate this example. Let’s say that, at the beginning of the lesson, almost 100% of the students are paying attention, but that after some time stop paying attention. Right before the end of the class, students start paying attention again.\n\nThe attention paid by the students did not decay linearly, but first dropped and rose up again, following a curvilinear trend. In these cases, we may want to perform some transformation on some input variables to account for this non-linear relationship. One of these transformations are polynomial transformations. In this context, when we talk about applying a polynomial function to a set of values, we usually mean exponentiating it by a positive number larger than 1. The power by which we exponentiate our variable defines the degree of the polynomial we are obtaining. Exponentiating our variable to the power of 2 will give us its second-degree polynomial. Exponentiating it by 3 will give us its third-degree polynomial, and so on. Back to our classroom example, we could add a new term to our regression equation: the second-degree polynomial of the input variable \\(Time\\), or even a third degree polynomial if we wanted to test to what extend our model follows a more complex pattern. Our regression trend will not be linear any more, but curvilinear. Let’s take a look at the anatomy of polynomials from a visual (and very informal perspective). Our model would look like this:\n\\[\ny_i = \\beta_{0} + \\beta_{1} Time_i + \\beta_{2} Time_{i}^2 + \\beta_{3} Time_{i}^3\n\\]\nAdding polynomial terms to our regression offers much flexibility to researchers when modelling this kind of associations between input and output variables. This practice is, for example, common in Cognitive Science when analysing repeated measures data such as eye-tracking data, where we register what participants fixated in a screen during a trial under several conditions. Polynomial regression could be considered as of the main techniques in the more general category of Growth Curve Analyis (GCA) methods. If you are interested in learning GCA, you should take a look at Daniel Mirman’s “Growth Curve Analysis and Visualization Using R” [book].\nPowerful as this technique is, it presents some pitfalls, especially to newbies like me. For instance, interpreting the outputs of a regression model that includes polynomials can tricky. In our example, depending on the values of the coefficients \\(\\beta_{1}\\), \\(\\beta_2\\) and \\(\\beta_3\\)–the first-degree and second-degree polynomials of \\(Time\\)–the shape of the resulting curve will be different. The combination of values that these two coefficient can take is infinite, and so is the number of potential shapes our curve can adopt. Interpreting how the values of these coefficients affect the shape of our model, and more importantly, their interaction with other predictors of interest in the model can be difficult without any kind of visualisation. The aim of this post is to visualise how the regression lines of a regression model changes with the degree of its polynomials. For computational constraints, and to make visualisation easier, I will only cover one, two, and three-degree polynomials. I will generate plots for multiple combinations of the coefficients of these polynomials using the base R function poly() to generate polynomials, the R package ggplot2() to generate plots, and the gganimate R package to animate the plots. I will briefly describe what is going on in each plot, but I hope the figures are themselves more informative than anything I can say about them!"
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "title": "Visualising polynomial regression",
    "section": "Intercept",
    "text": "Intercept\nFirst, let’s start with how the value of the intercept (\\(\\beta_0\\)) changes the regression line for polynomials of different degree (1st, 2nd, and 3rd). I set the rest of the coefficients to arbitrary values for simplicity (\\(\\beta_1 = \\beta_2 = \\beta_3 = 1\\)). As you can see, regardless of the order of the polynomials involved in the model, increasing the intercept makes the line be higher in the Y-axis, and decreasing the value of the intercept makes the line be lower in the Y-axis. Simple as that.\n\nThe interpretation of the intercept is similar to how we interpret it in standard linear regression models. It tells us the value of \\(y\\) when all predictors are set to 0 (in our case \\(Time = 0\\)). As we will discuss later, what that means in practice depends on what that zero means for the other coefficients, that is, how we coded them. For now, let’s continue adding more terms to the equation."
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Linear term: adding a 1st-order polynomial",
    "text": "Linear term: adding a 1st-order polynomial\nNow let’s see how a linear model (with only a 1st degree polynomial) changes as we vary the value of \\(\\beta_1\\), the coefficient of the linear term \\(Time\\). As you can see, nothing special happens, the line just gets steeper, meaning that for every unit increase in \\(x\\), \\(y\\) increases (or decreases, depending on the sign) in \\(\\beta_1\\) units. When the coefficient equals zero, there is no increase nor decrease in \\(y\\) for any change in \\(x\\).\n\nWhen \\(\\beta_1=0\\), the resulting line is completely horizontal, parallel to the X-axis. This is what a model with just an intercept (\\(y = \\beta_{0}\\)) would look like. We generalise this to say that the linear model we just visualised is exactly the same as adding a 2nd and a 3rd degree polynomial to the model with their correspondent coefficients set to zero (\\(\\beta_2 = 0\\) and \\(\\beta_3 = 0\\), respectively)."
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Quadratic: adding a 2nd-order polynomial",
    "text": "Quadratic: adding a 2nd-order polynomial\nNow things get a bit more interesting. When we add a second degree polynomial (\\(Time^2\\)), the line is not linear any more. If the coefficient of the 2nd-order polynomial (\\(\\beta_2\\)) is positive, the curve will go down and up in that order. When \\(\\beta_2 < 0\\), the curve goes up and then down. When \\(\\beta_2 = 0\\), the curve turns out the be a line whose slope is defined by \\(\\beta_1\\), just like in the previous example.\n\nImportantly, varying the value of the coefficient of 1st-order polynomials (\\(\\beta_1\\)) also changes the shape of the curve: more positive values of \\(\\beta_1\\) make the curve “fold” at higher values of \\(x\\). As you can see, when \\(\\beta_1 < 0\\) (left panel, in blue), the point at which the curve starts increasing or decreasing occurs more to the left. When \\(\\beta_2 > 0\\), this change occurs more to the right."
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Cubic: adding a 3rd-order polynomial",
    "text": "Cubic: adding a 3rd-order polynomial\nFinally, let’s complicate things a bit more by adding a third-order polynomial. Now the curve will “fold” two times. The magnitude of \\(\\beta_3\\) (the coefficient of the 3rd-degree polynomial) determines how distant both folding points are in the y-axis. When \\(\\beta_3\\) is close to zero, both folding points get closer, resembling the shape we’ve seen in a model with just a 2nd-degree polynomial. In fact, when \\(\\beta_3 = 0\\), we get the same plot (compare the panel to the right-upper corner to the plot in the previous section). The sign of \\(\\beta_3\\) also determines whether the curve goes down-up-down or up-down-up: down-up-down if \\(\\beta_3 < 0\\), and up-down-up if \\(\\beta_3 > 0\\).\nThe magnitude of \\(\\beta_2\\) (the coefficient of the 2rd-degree polynomial) determines the location of the mid-point between both folding points. For more positive values of \\(\\beta_2\\) this point is located higher in the y-axis, while for more negative values of \\(\\beta_2\\), this point is located lower in the y-axis. This value is a bit difficult to put in perspective in our practical example. Probably \\(\\beta_1\\) is more informative: \\(\\beta_1\\) changes the value of \\(x\\) at which the curve folds. More negative values of \\(\\beta_1\\) make the curve fold at lower values of \\(x\\), while more positive values of \\(\\beta_1\\) make the curve fold at higher values of \\(x\\)."
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "title": "Visualising polynomial regression",
    "section": "Conclusion",
    "text": "Conclusion\nThere are way more things to say about polynomial regression, and it’s more than likely that I sacrifice accuracy for simplicity. After all, the aim of generating these animations was helping myself understand the outputs of polynomial models a bit more easily in the future. I hope it helps others too. If you consider something is misleading or inaccurate, please let me know! I’m the first interested in getting it right. Cheers!"
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "title": "Visualising polynomial regression",
    "section": "Just the code",
    "text": "Just the code"
  },
  {
    "objectID": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "href": "posts/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "title": "Visualising polynomial regression",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] quarto_1.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9      ps_1.7.2        digest_0.6.31   later_1.3.0    \n [5] lifecycle_1.0.3 jsonlite_1.8.4  magrittr_2.0.3  evaluate_0.19  \n [9] stringi_1.7.8   rlang_1.0.6     cli_3.6.0       renv_0.16.0    \n[13] rstudioapi_0.14 vctrs_0.5.1     rmarkdown_2.19  tools_4.2.2    \n[17] stringr_1.5.0   glue_1.6.2      compiler_4.2.2  xfun_0.36      \n[21] yaml_2.3.6      fastmap_1.1.0   processx_3.8.0  htmltools_0.5.4\n[25] knitr_1.41"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html",
    "href": "posts/logistic-regression/logistic-regression.html",
    "title": "Getting the most out of logistic regression",
    "section": "",
    "text": "Codelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(scales)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(ggdist)\nlibrary(palmerpenguins)\n\ntheme_set(theme_minimal() +\n              theme(\n                  axis.line = element_line(colour = \"black\", size = 0.65),\n                  panel.grid = element_blank()\n              ))\n\nclrs <- c(\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\")"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#marginal-effects",
    "href": "posts/logistic-regression/logistic-regression.html#marginal-effects",
    "title": "Getting the most out of logistic regression",
    "section": "Marginal effects",
    "text": "Marginal effects\nDefining marginal effect is tricky. As it happens with many concepts and labels in statistics, the same label may be used to refer to different concepts, and several labels may be used interchangeably to refer to the same concept. Each subfield seems of science seems to use a somewhat intrinsic lexicon, which sometimes leads to some confusion. I will adopt the terminology in the documentation of the marginaleffects R package (arel-bundock2023marginaleffects?), in which a marginal effect is defined in the context of regression as:\n\n[…] the association between a change in a regressor \\(x\\) and a change in the response \\(y\\). Put differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor \\(x\\).\n\nAccording to this definition, calculating the marginal effect of our predictor of interest flipper_length_mm means extracting its slope for a specific value of the predictor. For linear regression models, this is trivial: since the relationship between the predictor and the response variable is assumed linear, the slope is considered constant across the whole range of the values of the predictor, and therefore the its marginal effect is identical for all of them. We can prove this by taking a look at the estimates of our model in the logit scale, which is linear. Let’s say that we are interested in finding out the slope of flipper_length_mm for its average, 190.1027397. A slope is just a difference. And a difference is a derivative. And the linear regression function, \\(y = \\beta_0 + \\beta_1 x\\), is a function that can be derived (see Equation 10).\n\\[\n\\begin{aligned}\ny &= \\beta_0 + \\beta_1 \\times \\text{Flipper length} \\\\\ny' &= \\beta_1 & \\text{First derivative}\n\\end{aligned}\n\\tag{10}\\]\nThe derivative of the linear regression equation is a constant! This constant corresponds to the regression coefficient of flipper_length_mm. This means that the difference in probability of being a male penguin is going to be same for two penguins whose flippers are 180 mm and 185 mm, respectively, and for two penguins whose flippers are 190 and 195 mm, respectively. Take a look at Figure 6. The difference in chances of being male between each pair of penguins, in the logit scale, is the same: 0.64, which corresponds to five times the flipper_length_mm regression coefficient because in both cases the difference in flipper length is not 1 mm but 5 mm (\\(5 \\times 0.1271 = 0.635\\)). but, again, any value in the logit scale is difficult to interpret by itself, so we are interested in translating this to the scale of probabilities.\n\nCode# predictions of model with unstandardised age\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = predict(fit, ., type = \"link\"))\n\nggplot(my_data, aes(flipper_length_mm, sex_pred_logit)) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred_logit),\n              size = 1,\n              colour = clrs[1]) +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(round(point_preds[2,2]-point_preds[1,2], 3), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(round(point_preds[4,2]-point_preds[3,2], 3), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male) [logit scale]\",\n         title = \"<span style = 'color:#003f5c;'>Slope (logit scale)</span>\") +\n    guides(linetype = \"none\") +\n    \n    \n    ggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    geom_point(colour = NA) +\n    geom_hline(yintercept = coef(fit)[\"flipper_length_mm\"],\n               size = 1, colour = \"#ffa600\") +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length [logit scale])\",\n         title = \"<span style = 'color:#ffa600;'>Derivative (logit scale)</span>\") +\n    scale_y_continuous(limits = c(0.05, 0.2)) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\nFigure 6: ?(caption)\n\n\n\n\nWhat marginal effects translate to for logistic regression\nWe have seen that regression coefficients do not behave identically for different values of their predictors when transformed to probabilities. The exact point of flipper_length_mm at which we calculate its marginal effect matters. This is because the derivative of the logistic function, which describes the behaviour of the probability scale we just moved to, is no longer a constant. See Equation 11.\n\\[\n\\begin{aligned}\ny &= \\frac{1}{1 + e^{(-\\beta x)}} & \\text{Logistic function} \\\\\ny' &= \\frac{\\beta_1 · e^{\\beta_0 + \\beta_1 · \\ \\text{Flipper length}} }{(1 + e^{-(\\beta_0 + \\beta_1 · \\ \\text{Flipper length})})^2} & \\text{Derivative}\n\\end{aligned}\n\\tag{11}\\]\nThe derivative of the logistic function still considers the value of the predictor (\\(\\text{Flipper length}\\)), which means that the value of the derivative changes depending on such value. Let’s try to visualise this. First, we are going to implement the derivative of the logistic function as an R function that computes it for the fit model:\n\nCodelogistic_derivative <- function(object, x, value) {\n    slope <- coef(object)[x]\n    intercept <- coef(object)[\"(Intercept)\"]\n    numerator <- slope * exp(-(intercept + (slope * value)))\n    denominator <- (1 + exp(-(intercept + (slope * value))))^2\n    y <- numerator/denominator\n    names(y) <- NULL\n    return(y)\n}\n\n\nFigure 7 shows how the derivative changes for each value of the predictor. As you can see, the difference in probability of being male is largest at around 190 mm, while such difference decreases as flipper_length_mm shifts away from 190 mm.\n\nCode# model predictions (on the probability scale by default)\nmy_data <- mutate(my_data,\n                  derivative = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = plogis(predict(fit, .)),\n           derivate = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[2,2]-point_preds[1,2]), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[4,2]-point_preds[3,2]), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male)\",\n         title = \"<span style = 'color:#003f5c;'>Logistic curve</span>,\n        <span style = 'color:#ff6361;'>intercept</span>\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    \n    ggplot(my_data, aes(flipper_length_mm, \n                        logistic_derivative(fit, \"flipper_length_mm\", value = flipper_length_mm))) +\n    geom_point(colour = NA) +\n    geom_line(size = 1, colour = \"#ffa600\") +\n    geom_segment(aes(x = flipper_length_mm[which.max(derivative)],\n                     xend = flipper_length_mm[which.max(derivative)],\n                     y = min(derivative), yend = max(derivative)),\n                 colour = \"grey\", linetype = \"dotted\") +\n    annotate(geom = \"label\", label = paste0(\"Max = \", percent(max(my_data$derivative))),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = max(my_data$derivative)*0.5) +\n    \n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length [logit scale])\",\n         title = \"<span style = 'color:#ffa600;'>Derivative</span>\") +\n    scale_y_continuous(labels = percent) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\nFigure 7: ?(caption)\n\n\n\n\nThe maximum change in male probability is 3%, which occurs at around 190 mm flipper length. There is a smarter way of calculating the maximum slope of flipper_length_mm. This value will always occur at the mid-point of the logistic curve, and it turns out that to find the derivative of the logistic function at the mid-point (i.e., for \\(x = x_0\\), go back to ?@eq-logistic for a reminder), we only need to find \\(\\beta /4\\), where \\(\\beta\\) is the regression coefficient of our predictor of interest! This is called the divide-by-four-rule, and is a simple trick to report the coefficients of a logistic regression model in the scale of probabilities, and for meaningful values of the predictors (the value at which the slope is maximum). This way, we could just divide the regression coefficient of flipper_length_mm by four to get the maximum probability difference of being male between two penguins with flipper lengths \\(x\\) and \\(x + 1\\).\n\n\n\n\n\n\nThe divide-by-four rule\n\n\n\nWe have seen that dividing the coefficients of a logistic regression model (in the logit scale) gets us the maximum slope of the predictor in the probability scape. We mentioned that this has to do with the derivative of the logistic function at the mid-point. But since we dropped that term some equations ago after setting it at zero, it is no longer clear how one would derive the logistic function in such way that the divide-by-four-rule holds. Let’s go back to Equation 1, when the mid-point still appeared in our equation. We derive this formula:\n\\[\n\\begin{aligned}\n\\text{logistic(x)} &= \\frac{1}{1 + e^{(-\\beta \\ · \\ (x-x_0))}} & \\text{Logistic function}\\\\\n\\text{logistic'(x)} &= \\frac{\\beta · e^{-\\beta (x-x_0)}}{(1 + e^{-\\beta (x-x_0)})^2} & \\text{Derivative}\n\\end{aligned}\n\\]\nAnd since \\(x = x_0\\), \\(x-x_0 = 0\\). We can simplify the derivative now, knowing that \\(e^0 = 1\\).\n\\[\n\\begin{aligned}\n\\text{logistic'(x)} &= \\frac{\\beta · e^0}{(1 + e^0)^2} \\\\\n&= \\frac{\\beta}{(1 + 1)^2} \\\\\n&= \\frac{\\beta}{4} \\\\\n\\end{aligned}\n\\]\nTake a look at the previously cited blog post by TJ Mahr for a derivation that also includes the asymptote term in the logistic function.\n\n\nLet’s put the divide-by-four rule. The output of the logistic_derivative() that we defined before should, when solved for the mid-point of the logistic curve, return an equivalent value to \\(\\beta_1 / 4\\), where \\(\\beta_1 = 0.1271495\\), and therefore return something close to 0.0317874. We don’t know what value of flipper_length_mm corresponds to the mid-point. In Figure 7 A, we calculated it my finding the value of flipper_length_mm for which logistic_derivative() returned the maximum value:\n\nderivative_values <- logistic_derivative(fit, \"flipper_length_mm\", my_data$flipper_length_mm)\nmy_data$flipper_length_mm[which.max(derivative_values)]\n\n[1] 190\n\n\nFrom our data, we find that the maximum slope of flipper_length_mm occurs at 190. But finding the mid-point this way requires us to have already calculated the derivative of the logistic function. There is an alternative way to compute this mid-point from the estimated coefficients of the regression model. An additional benefit of computing the mid-point this way, is that we are doing so by relying on model-projections, and therefore in a way that does not entirely rely on the range of values of the predictor for which we have computed the derivative of the logistic function. This method consists in ?@eq-midpoint-coefs.\n\\[\nx_0 = -\\beta_0 / \\beta_1\n\\]\nWhere \\(\\beta\\) is the intercept of the regression model, and \\(\\beta_1\\) is the regression coefficient of the predictor we are calculating the mid-point for. We can implement this formula in R as:\n\n# get point in x at the inflection point (where y = 0.5)\nget_mid <- function(x) {\n    coefs <- coef(x)[-1]\n    mid <- coef(x)[1]/-coefs\n    return(mid)\n}\n\nget_mid(fit)\n\n(Intercept) \n   190.0882 \n\n\nUsing this function, we find that the mid-point is located at flipper length 190.0882 mm, pretty close to what we had estimated from our data. This value can sometimes be extremely interesting! A personal experience: in my PhD, I investigated how the age at which children learn particular words is affected by participant-level and word-level characteristics. I used logistic regression to model the probability of a given child having learnt a word, adjusting for my predictors of interest, the most important of them being the age of the child. Older children are more likely to have lear a given word than younger children. My model returned, among others, a coefficient for age in the logit scale, but I wasn’t specially intersted in it, even after having transformed it to the probability scale. I was, however, more interested in finding the value of age at which most children were learning each word, which corresponded to the mid-point of the logistic curve for the age predictor!\nNow that we have calculated the mid-point of our logistic function, we can finally compare the divide-by-four rule against the actual value of the derivative of the logistic function. If we solve Equation 11 for \\(x_0 = 190.0882\\) using logistic_derivative(fit, \"flipper_length_mm\", 190.0882), we get that the mid-point is located at 0.0317874. If we use the divide-by-four rule, we get that coef(fit)[\"flipper_length_mm\"] / 4. Exactly the same!"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#centering-a-predictor",
    "href": "posts/logistic-regression/logistic-regression.html#centering-a-predictor",
    "title": "Getting the most out of logistic regression",
    "section": "Centering a predictor",
    "text": "Centering a predictor"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#standardising-a-predictor",
    "href": "posts/logistic-regression/logistic-regression.html#standardising-a-predictor",
    "title": "Getting the most out of logistic regression",
    "section": "Standardising a predictor",
    "text": "Standardising a predictor"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#multiple-predictors",
    "href": "posts/logistic-regression/logistic-regression.html#multiple-predictors",
    "title": "Getting the most out of logistic regression",
    "section": "Multiple predictors",
    "text": "Multiple predictors"
  }
]