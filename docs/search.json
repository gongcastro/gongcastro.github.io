[
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Gonzalo García-Castro",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "title": "Visualising polynomial regression",
    "section": "",
    "text": "When modelling data using regression, sometimes the relationship between input variables and output variables is not very well captured by a straight line. A standard linear model is defined by the equation\n\\[y_i = \\beta_{0} + \\beta_{1}x_{i}\\]\nwhere \\(\\beta_{0}\\) is the intercept (the value of the input variable \\(x\\) where the output variable \\(y=0\\)), and where \\(\\beta_{1}\\) is the coefficient of the input variable (how much \\(y\\) increases for every unit increase in \\(x\\)). To illustrate this, let’s imagine we are curious abut what proportion of the students in a classroom are paying attention, and how this proportion changes as minutes pass. We could formalise our model as\n\\(y_i = \\beta_{0} + \\beta_{1} Time_i\\)\nLet’s generate some data to illustrate this example. Let’s say that, at the beginning of the lesson, almost 100% of the students are paying attention, but that after some time stop paying attention. Right before the end of the class, students start paying attention again.\nThe attention paid by the students did not decay linearly, but first dropped and rose up again, following a curvilinear trend. In these cases, we may want to perform some transformation on some input variables to account for this non-linear relationship. One of these transformations are polynomial transformations. In this context, when we talk about applying a polynomial function to a set of values, we usually mean exponentiating it by a positive number larger than 1. The power by which we exponentiate our variable defines the degree of the polynomial we are obtaining. Exponentiating our variable to the power of 2 will give us its second-degree polynomial. Exponentiating it by 3 will give us its third-degree polynomial, and so on. Back to our classroom example, we could add a new term to our regression equation: the second-degree polynomial of the input variable \\(Time\\), or even a third degree polynomial if we wanted to test to what extend our model follows a more complex pattern. Our regression trend will not be linear any more, but curvilinear. Let’s take a look at the anatomy of polynomials from a visual (and very informal perspective). Our model would look like this:\n\\[\ny_i = \\beta_{0} + \\beta_{1} Time_i + \\beta_{2} Time_{i}^2 + \\beta_{3} Time_{i}^3\n\\]\nAdding polynomial terms to our regression offers much flexibility to researchers when modelling this kind of associations between input and output variables. This practice is, for example, common in Cognitive Science when analysing repeated measures data such as eye-tracking data, where we register what participants fixated in a screen during a trial under several conditions. Polynomial regression could be considered as of the main techniques in the more general category of Growth Curve Analyis (GCA) methods. If you are interested in learning GCA, you should take a look at Daniel Mirman’s “Growth Curve Analysis and Visualization Using R” [book].\nPowerful as this technique is, it presents some pitfalls, especially to newbies like me. For instance, interpreting the outputs of a regression model that includes polynomials can tricky. In our example, depending on the values of the coefficients \\(\\beta_{1}\\), \\(\\beta_2\\) and \\(\\beta_3\\)–the first-degree and second-degree polynomials of \\(Time\\)–the shape of the resulting curve will be different. The combination of values that these two coefficient can take is infinite, and so is the number of potential shapes our curve can adopt. Interpreting how the values of these coefficients affect the shape of our model, and more importantly, their interaction with other predictors of interest in the model can be difficult without any kind of visualisation. The aim of this post is to visualise how the regression lines of a regression model changes with the degree of its polynomials. For computational constraints, and to make visualisation easier, I will only cover one, two, and three-degree polynomials. I will generate plots for multiple combinations of the coefficients of these polynomials using the base R function poly() to generate polynomials, the R package ggplot2() to generate plots, and the gganimate R package to animate the plots. I will briefly describe what is going on in each plot, but I hope the figures are themselves more informative than anything I can say about them!"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "title": "Visualising polynomial regression",
    "section": "Intercept",
    "text": "Intercept\nFirst, let’s start with how the value of the intercept (\\(\\beta_0\\)) changes the regression line for polynomials of different degree (1st, 2nd, and 3rd). I set the rest of the coefficients to arbitrary values for simplicity (\\(\\beta_1 = \\beta_2 = \\beta_3 = 1\\)). As you can see, regardless of the order of the polynomials involved in the model, increasing the intercept makes the line be higher in the Y-axis, and decreasing the value of the intercept makes the line be lower in the Y-axis. Simple as that.\n\nThe interpretation of the intercept is similar to how we interpret it in standard linear regression models. It tells us the value of \\(y\\) when all predictors are set to 0 (in our case \\(Time = 0\\)). As we will discuss later, what that means in practice depends on what that zero means for the other coefficients, that is, how we coded them. For now, let’s continue adding more terms to the equation."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Linear term: adding a 1st-order polynomial",
    "text": "Linear term: adding a 1st-order polynomial\nNow let’s see how a linear model (with only a 1st degree polynomial) changes as we vary the value of \\(\\beta_1\\), the coefficient of the linear term \\(Time\\). As you can see, nothing special happens, the line just gets steeper, meaning that for every unit increase in \\(x\\), \\(y\\) increases (or decreases, depending on the sign) in \\(\\beta_1\\) units. When the coefficient equals zero, there is no increase nor decrease in \\(y\\) for any change in \\(x\\).\n\nWhen \\(\\beta_1=0\\), the resulting line is completely horizontal, parallel to the X-axis. This is what a model with just an intercept (\\(y = \\beta_{0}\\)) would look like. We generalise this to say that the linear model we just visualised is exactly the same as adding a 2nd and a 3rd degree polynomial to the model with their correspondent coefficients set to zero (\\(\\beta_2 = 0\\) and \\(\\beta_3 = 0\\), respectively)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Quadratic: adding a 2nd-order polynomial",
    "text": "Quadratic: adding a 2nd-order polynomial\nNow things get a bit more interesting. When we add a second degree polynomial (\\(Time^2\\)), the line is not linear any more. If the coefficient of the 2nd-order polynomial (\\(\\beta_2\\)) is positive, the curve will go down and up in that order. When \\(\\beta_2 &lt; 0\\), the curve goes up and then down. When \\(\\beta_2 = 0\\), the curve turns out the be a line whose slope is defined by \\(\\beta_1\\), just like in the previous example.\n\nImportantly, varying the value of the coefficient of 1st-order polynomials (\\(\\beta_1\\)) also changes the shape of the curve: more positive values of \\(\\beta_1\\) make the curve “fold” at higher values of \\(x\\). As you can see, when \\(\\beta_1 &lt; 0\\) (left panel, in blue), the point at which the curve starts increasing or decreasing occurs more to the left. When \\(\\beta_2 &gt; 0\\), this change occurs more to the right."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Cubic: adding a 3rd-order polynomial",
    "text": "Cubic: adding a 3rd-order polynomial\nFinally, let’s complicate things a bit more by adding a third-order polynomial. Now the curve will “fold” two times. The magnitude of \\(\\beta_3\\) (the coefficient of the 3rd-degree polynomial) determines how distant both folding points are in the y-axis. When \\(\\beta_3\\) is close to zero, both folding points get closer, resembling the shape we’ve seen in a model with just a 2nd-degree polynomial. In fact, when \\(\\beta_3 = 0\\), we get the same plot (compare the panel to the right-upper corner to the plot in the previous section). The sign of \\(\\beta_3\\) also determines whether the curve goes down-up-down or up-down-up: down-up-down if \\(\\beta_3 &lt; 0\\), and up-down-up if \\(\\beta_3 &gt; 0\\).\nThe magnitude of \\(\\beta_2\\) (the coefficient of the 2rd-degree polynomial) determines the location of the mid-point between both folding points. For more positive values of \\(\\beta_2\\) this point is located higher in the y-axis, while for more negative values of \\(\\beta_2\\), this point is located lower in the y-axis. This value is a bit difficult to put in perspective in our practical example. Probably \\(\\beta_1\\) is more informative: \\(\\beta_1\\) changes the value of \\(x\\) at which the curve folds. More negative values of \\(\\beta_1\\) make the curve fold at lower values of \\(x\\), while more positive values of \\(\\beta_1\\) make the curve fold at higher values of \\(x\\)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "title": "Visualising polynomial regression",
    "section": "Conclusion",
    "text": "Conclusion\nThere are way more things to say about polynomial regression, and it’s more than likely that I sacrifice accuracy for simplicity. After all, the aim of generating these animations was helping myself understand the outputs of polynomial models a bit more easily in the future. I hope it helps others too. If you consider something is misleading or inaccurate, please let me know! I’m the first interested in getting it right. Cheers!"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "title": "Visualising polynomial regression",
    "section": "Just the code",
    "text": "Just the code"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "title": "Visualising polynomial regression",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] quarto_1.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9      ps_1.7.2        digest_0.6.31   later_1.3.0    \n [5] lifecycle_1.0.3 jsonlite_1.8.4  magrittr_2.0.3  evaluate_0.19  \n [9] stringi_1.7.8   rlang_1.0.6     cli_3.6.0       renv_0.16.0    \n[13] rstudioapi_0.14 vctrs_0.5.1     rmarkdown_2.19  tools_4.2.2    \n[17] stringr_1.5.0   glue_1.6.2      compiler_4.2.2  xfun_0.36      \n[21] yaml_2.3.6      fastmap_1.1.0   processx_3.8.0  htmltools_0.5.4\n[25] knitr_1.41"
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "",
    "text": "Install decktape and adapt this command to download your slides—which should be deployed on GitHub Pages—as PDF in your local machine:\ndecktape &lt;slides-url&gt; &lt;slides-file&gt;.pdf\n\n\n\n\n\n\n\n\n\n\n\nAn embarrasing confession\n\n\n\nSo it turns out one can download their RevealJS slides (in a HTML file) without using decktape, just by following the instructions in the Quarto documentation. In my defence, for whatever reason this did not work for me, whereas decktape did the job. I probably skipped a critical step.\nI’m too lazy to re-orient or rewrite this post, so I’ll publish it any way. Read it as if it were the panicky reflections of a sleepless PhD student who got overconfident about their programming skills the night before their conference presentation. decktape might be still a convenient option to do the same job in a programmatic way, so there it goes."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#tldr",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#tldr",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "",
    "text": "Install decktape and adapt this command to download your slides—which should be deployed on GitHub Pages—as PDF in your local machine:\ndecktape &lt;slides-url&gt; &lt;slides-file&gt;.pdf\n\n\n\n\n\n\n\n\n\n\n\nAn embarrasing confession\n\n\n\nSo it turns out one can download their RevealJS slides (in a HTML file) without using decktape, just by following the instructions in the Quarto documentation. In my defence, for whatever reason this did not work for me, whereas decktape did the job. I probably skipped a critical step.\nI’m too lazy to re-orient or rewrite this post, so I’ll publish it any way. Read it as if it were the panicky reflections of a sleepless PhD student who got overconfident about their programming skills the night before their conference presentation. decktape might be still a convenient option to do the same job in a programmatic way, so there it goes."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#some-context",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#some-context",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "Some context",
    "text": "Some context\nLast week, I presented an oral communication at the International Symposium of Psycholinguistics. I feel comfortable using Quarto, and I had already given a couple of presentations generating slides from Quarto before, so I decided to take the next step and use Quarto for this one as well.\nI chose RevealJS as the output format of my slides: it looks beautiful, you don’t need much knowledge of HTML or CSS to fix the layout of the slides, and it has a nice presenter view mode. In the end this is what my slides looked like:\n\n\n\n\n\n\n\n\nI’m quite happy with how they look, so I even made a Quarto extension to apply this format to future presentations more conveniently.\nThe main inconvenience of RevealJS (or any HTML output format) is that presenting from a computer difference from the one in which you created the slides requires moving a whole folder, as opposed to just moving the .pptx or .pdf file to the new machine.\nIf one moves only the resulting index.html that contains the slides, the presentation will lose all of its nice formatting, and won’t show any images or additional resources. From previous (bad) experiences, I knew that this could be a problem, and given that I would certainly have to present from the conference room’s machine, I decided to use GitHub Pages1 to deploy my slides on a website.\n1 Here is a nice tutorials on how to deploy your slides on GitHub PagesMy plan was to access the URL of the slides whenever I was about to present. But the day before the presentation, I started worrying about the internet not working and the conference venue at the wrong time I needed to export the slides to a PDF. A PDF never (well, rarely) fails."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#the-problem",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#the-problem",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "The problem",
    "text": "The problem\nExporting Quarto slides to PDF is not as simple as just adding the pdf: default line to the YAML header of the file, under format:, like this:\nformat:\n  revealjs: default\n  pdf: default\nThis will render the slides as a PDF document, not as slides. Also, much of the nice formatting would be lost, as the .scss file that contained the styling only applies to HTML.\nQuarto offers a PDF output format for slides, Beamer, which is pretty popular among LaTeX users. But again, generating the presentation in Beamer format would require re-doing the formatting from scratch, and doing so in LaTeX, which I’m not totally comfortable using. This was not an option. Neither it was to use the Power Point output format, as not only the format was lost, but also prevented any possibility of customising the layout of the slides.\n\n\n\nYou may recognise this aesthetic from Beamer slides. I personally don’t like it a lot, but I acknowledge the convenience of Beamer for LaTeX conoisseurs. I wish I was one of them. I’m not."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#from-revealjs-to-pdf-printing-option-in-chrome",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#from-revealjs-to-pdf-printing-option-in-chrome",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "From RevealJS to PDF: printing option in Chrome",
    "text": "From RevealJS to PDF: printing option in Chrome\nI came across the Print to PDF section of the Presenting Slides article in the Quarto documentation:\n\n\n\nA snapshot of the instructions to print a RevealJS presentation to PDF in the Quarto documentation (see Chrome’s printing menu on the right, prompted byu Ctrl + P.\n\n\nAlthough this looked promising, the output of this method was not what I expected. The layout had been moved around a bit, and some formatting options were not preserved. I this presentation was aimed at my colleagues, it would have been ok-ish, but it was not for a conference presentation.\nAt this point I was already considering staying up all night re-doing the presentation in Power Point, and forgetting about Quarto for a while. In the last minute, when reading the RevealJS documentation, I found the suggestion of using decktape:\n\n\n\nMy salvation\n\n\nI was not very hopeful about this. decktape is written in JavaScript, which I’m not very familiar with, and I feared wasting too much time trying to make it work instead of just doing everything from scratch in Power Point. But it worked!. These are the steps I followed, according to the dektape docs.\n\nInstall Node.js\nInstall npm from the Node.js console:\n\nnpm install -g npm\nThen following the instructions in the decktape repository:\n\nInstall decktape from the Node.js console:\n\nnpm install -g decktape\n\nFinally, using the decktape command to download my slides deployed on Github Pages as a PDF:\n\ndecktape https://gongcastro.github.io/ips_2023_trajectories slides.pdf\nAnd it did the job, preserving the layout, SCSS formatting, and even the links!"
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html",
    "href": "blog/psicotuiterbot/psicotuiterbot.html",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "",
    "text": "Follow @psicotuiterbot"
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Escribiendo el código",
    "text": "Escribiendo el código\nUn bot de Twitter no es más que un código que se ejecuta automáticamente de forma periódica (ej., cada 15 minutos) y realiza una acción en Twitter a través de una cuenta (hacer un RT o responder a un tweet). Para poder realizar esta acción a través del código, es necesario tener acceso a la API de Twitter. API es un acrónimo para Application Programming Interface y como dice su nombre, es una plataforma desde la que podemos interactuar con una aplicación (en este caso Twitter) a través de programación con una serie de comandos que el equipo de Twitter ha diseñado la API ha definido.\nNunca había hecho un bot de Twitter, pero sabía de la existencia de bastantes tutoriales para hacerlo. La mayoría de los bots de Twitter (y por tanto de los tutoriales) están escritos en Python y JavaScript, pero yo me encuentro algo más cómodo con R. Además gran parte de Psicotuiter (especialmente quienes están relacionades con la metodología) también está más familiarizado con R. Mi intención era hacer el bot lo más trasparente y accesible para la comunidad, así que me decanté por R. En un pricipio seguí el tutorial de Matt Dray, en el que utiliza el paquete de R {rtweet} para interactuar con la API de Twitter. Hay más tutoriales que usan rtweet para crear un bot con R. Pero a diferencia de otros, este tutorial explicaba cómo usar GitHub actions para ejecutar el código de forma periódica. Ahora explico esto último. Mientras tanto, vamos al código de R.\nPrimero creé un repositorio de GitHub donde alojar el código (GitHub es como un Google Drive especializado en código donde además podemos hacer control de versiones de los archivos que subimos). Puedes echar un vistazo al código de R en la carpeta R/. El código principalmente recoge los últimos tweets que se han escrito en las últimos 6 horas mencionando #psicotuiter o #psicotwitter y los retuitea. No me meteré en detalle a explicar cómo funciona el código, pero aquí va un pequeño resumen del programa. Si tienes curiosidad te recomiendo explorar el repositorio de GitHub, que contiene todo lo necesario para hacer funcionar el bot:\nPrimero cargamos el paquete de R {dplyr}, que utilizamos en bastantes ocasiones en el programa, ajustamos un pequeño detalle relacionado con el comportamiento de la API de Twitter, e instalamos los paquetes necesarios (en caso de que hayan cambiado) usando el pqeute de R {renv} (si no lo conoces y te interesa la reproducibilidad computacional tienes que echarle un vistazo).\n# bot R code\nlibrary(dplyr)\noptions(httr_oob_default = TRUE)\n# restore packages\nrenv::restore()\nA continuación extraemos las credenciales que necesitamos para acceder a la API de Twitter (las paso a como variables de entorno para evitar hacerlas públicas, ya que eso dería acceso a cualquiera a la cuenta de Twitter del bot).\n# authenticate Twitter API\nmy_token &lt;- rtweet::create_token(\n    app = \"psicotuiterbot\",  # the name of the Twitter app\n    consumer_key = Sys.getenv(\"TWITTER_CONSUMER_API_KEY\"),\n    consumer_secret = Sys.getenv(\"TWITTER_CONSUMER_API_KEY_SECRET\"),\n    access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n    access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"), \n    set_renv = FALSE\n)\nLuego extraemos los tweets usando rtweet, filtramos los que sean relevantes y no contengan posible contenido ofensivo (y algún otro filtro más).\nlibrary(dplyr)\n# define hashtags\nhashtags_vct &lt;- c(\"#psicotuiter\", \"#psicotwitter\", \"#Psicotuiter\", \"#Psicotwitter\", \"#PsicoTuiter\", \"#PsicoTwitter\")\nhashtags &lt;- paste(hashtags_vct, collapse = \" OR \")\nhate_words &lt;- unlist(strsplit(Sys.getenv(\"HATE_WORDS\"), \" \")) # words banned from psicotuiterbot (separated by a space)\nblocked_accounts &lt;- unlist(strsplit(Sys.getenv(\"BLOCKED_ACCOUNTS\"), \" \")) # accounts banned from psicotuiterbot (separated by a space)\ntime_interval &lt;- lubridate::now(tzone = \"UCT\")-lubridate::minutes(120)\n# get mentions to #psicotuiter and others\nall_tweets &lt;- rtweet::search_tweets(\n    hashtags, \n    type = \"recent\", \n    token = my_token, \n    include_rts = FALSE, \n    tzone = \"CET\"\n) \nstatus_ids &lt;- all_tweets %&gt;% \n    filter(\n        !(screen_name %in% gsub(\"@\", \"\", blocked_accounts)),\n        created_at &gt;= time_interval, # 15 min\n        !grepl(paste(hate_words, collapse = \"|\"), text), # filter out hate words\n        stringr::str_count(text, \"#\") &lt; 4, # no more than 3 hashtags\n        lang %in% c(\"es\", \"und\") # in Spanish or undefined language\n    ) %&gt;% \n    pull(status_id)\n# get request ID\nrequest_tweets &lt;- rtweet::get_mentions(\n    token = my_token, \n    tzone = \"CET\"\n) \nFinalmente, hacemos RT uno a uno usando rtweet (si ya habíamos hecho RT a uno de ellos simplemente se ignora).\nif (nrow(request_tweets) &gt; 0) {\n    request_ids &lt;- request_tweets %&gt;% \n        filter(\n            created_at &gt;= time_interval, # 15 min\n            grepl(\"@psicotuiterbot\", text),\n            grepl(\"rt|RT|Rt\", text),\n            !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n        ) %&gt;% \n        pull(status_in_reply_to_status_id)\n    \n    # get requested IDS\n    if (length(request_ids) &gt; 0) {\n        requested_ids &lt;- rtweet::lookup_statuses(request_ids, token = my_token) %&gt;% \nposterior_preds &lt;- expand.grid(any1 = seq(min(dat$any1), max(dat$any1), by = 0.25),\n                               any2 = seq(min(dat$any2), max(dat$any2), by = 0.25),\n                               mes = unique(dat$mes)) %&gt;% \n  add_fitted_draws(fit3, n = 10) %&gt;% \n  ungroup() %&gt;% \n  left_join(select(dat, any, mes, temperatura) \n            mutate(intercept = fixef(fit3)$Estimate[1])\n            rowwise(y = fixef(fit3)[1,1] + any2)\n            ggplot(posterior_preds, aes(any1, .value, colour = any1, group = interaction(any2, .draw))) +\n              facet_wrap(~mes) +\n              geom_point(size = 0.1)\n            \n            \n                     if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::restore()\n      - name: Create and post tweet\n        run: Rscript R/bot.R\nLa gran ventaja de usar este sistema es que no necesitamos usar nuestro ordenador personal, ya que usamos el que que GitHub nos asigna (un servidor no deja de ser un ordenador). Pero tiene varios inconvenientes. El primero es que el proceso de establecer un workflow en GitHub Actions suele requerir varios intentos (en mi caso muchos). Esto suele deberse a problemas de reproducibilidad computacional: el código funciona correctamente en mi ordenador porque en él tengo instalado todo el sofware del que depende. Cuando uso el servidor de GitHub, el sistema operativo suele necesitar que instalemos estas dependencias antes de ejecutar el código. GitHub Actions permite cierta flexibilidad a la hora de seleccionar el software que viene instalado en el sistema operativo que vamos a usar (ej., R, compiladores de C++, dependencias de Linux, etc.). El problema es que muchas veces ni siquiera somos conscientes de cuántas dependencias requiere nuestro código. Con paciencia y muchas búsquedas de Google es posible solventar este problema.\nUn segundo inconveniente que encontré a la hora de implementar el bot en GitHub Actions tiene que ver con los tiempos: instalar todas las dependencias del código en el servidor cada 15 minutos (la configuración se pierde casi totalmente tras cada ejecución) es poco eficiente. Instalar las dependencias puede tardar más de 10 minutos (en el caso de este bot). Esto puede además hacer fallar en ocasiones el flujo de trabajo. GitHub Actions tampoco es lo más consistente del mundo, aunque no deja de ser gratis."
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Segundo intento (sale bien): Raspberry Pi",
    "text": "Segundo intento (sale bien): Raspberry Pi\nTras varios problemas en la ejecución del bot a través de GitHub Actions, decidí cambiar de método. Por razones ajenas al bot, hacía unos meses que tenía muerta de aburrimiento una Raspberry Pi 4 que compré con un amigo para jugar con ella. Este dispositivo es un mini-ordenador relativamente barato (~40€) que salió al mercado como herramienta educativa para enseñar a programar (ej., róbotica para niñes) pero que poco a poco ha ido tomando espacio en lugares de producción. Tiene mil posibilidades por su simplicidad y, en nuestro caso, por su bajo consumo: tener una Raspberry Pi funcionando todo el día apenas tiene impacto sobre el consumo de luz.\n\n\n\nAquí está alojado el @psicotuiterbot\n\n\nPrimero instalé el código en la Raspberry con sus dependencias: básicamente, cloné el repositorio de GitHub en una carpeta dentro de home/Documents/. Para ejecutar el código cada 15 minutos utilicé una función muy útil que incluye Linux (sistema operativo con el que funciona la Raspberry) llamado CRON. Simplemente consiste en un archivo en el que incluimos una serie de comandos que queremos que se ejecuten de forma periódica, junto con una código que indica la periodicidad de la ejcución de este comando. Aquí tienes unos ejemplos. Incluí cuatro comandos (cada uno en su propio archivo con extensión .sh, que denota comandos de Linux):\n# descarga el código de GitHub, por si ha habido cambios\ngit pull origin main\n# ejecuta el código principal del bot\nTZ=\"Spain/Madrid\" Rscript -e 'source(\"R/bot.R\")'\n# guarda los tweets detectados en un archivo y crea un gráfico\nRscript -e 'source(\"R/counts.R\")'\nrm Rplots.pdf\n# sube los nuevos datos a GitHub\ngit add .\ngit commit -m \"Update repository\"\ngit push\nEstos comandos se ejecutan en este orden cada 15 minutos."
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "title": "How similar is the word “mask” across languages?",
    "section": "",
    "text": "The ubiquity of masks has given psycholinguists a frequent-ish stimulus to use in experiments. This word is more form-similar across languages than one may think. I gathered a big-ish dataset with translation equivalents of the word mask across ~110 languages. I tweeted about this today, and wanted to dedicate some more lines to nuance.\nHere’s the data:\nLanguage\nOrthography\nOrthography\n(romanisation)\nPhonology (IPA)\n\n\n\n\nAfrikaans\nmasker\nmasker\n/mɑːsk/\n\n\nAlbanian\nmaskë\nmaskë\n/maskaɾiʝa/\n\n\nAmharic\nጭምብል\nch'imibili\n/tʃ'imibili/\n\n\nArabic\nقناع\nqunae\n/qinaːʕ/\n\n\nArmenian\nդիմակ\ndimak\n/dimɑk/\n\n\nAzerbaijani\nmaska\nmaska\n/maska/\n\n\nBangla\nমুখোশ\nmukhosh\n-\n\n\nBasque\nmaskara\nmaskara\n/maskaɾa/\n\n\nBelarusian\nмаска\nmaska\n/maska/\n\n\nBengali\nমাস্ক\nmāska\n/mask/\n\n\nBosnian\nmaska\nmaska\n/maska/\n\n\nBulgarian\nмаска\nmaska\n/maskə/\n\n\nCatalan\nmascareta\nmascareta\n/məskəɾɛtə/\n\n\nCebuano\nmaskara\nmaskara\n/maskaɾa/\n\n\nChichewa\namabisa\namabisa\n/amaɓisa/\n\n\nChinese Simplified\n手术口罩\nshǒushù kǒuzhào\n/ʂə̌uʂʷù kʰə̌uʈʂàu/\n\n\nChinese Traditional\n手術口罩\nshǒushù kǒuzhào\n/ʂə̌uʂʷù kʰə̌uʈʂàu/\n\n\nCorsican\nmaschera\nmaschera\n/maskeɾa/\n\n\nCroatian\nmaska\nmaska\n/màska/\n\n\nCzech\nmaska\nmaska\n/maska/\n\n\nDanish\nmaske\nmaske\n/masgə  /\n\n\nDutch\nmasker\nmasker\n/mɑskər/\n\n\nEnglish\nmask\nmask\n/mɑːsk/\n\n\nEsperanto\nmasko\nmasko\n/masko/\n\n\nEstonian\nnaamio\nnaamio\n/naːmio/\n\n\nTagalog (Filipino)\nmaskara\nmaskara\n/maskaɾa/\n\n\nFinnish (Suomi)\nnaamio\nnaamio\n/nɑːmio/\n\n\nFinnish (Suomi)\nmaskara\nmaskara\n/mɑskɑrɑ/\n\n\nFinnish (Suomi)\nripsiväri\nripsiväri\n/ripsiˌʋæri/\n\n\nFrench\nmasque\nmasque\n/mɑːsk/\n\n\nFrisian\nmasker\nmasker\n/masker/\n\n\nGalician\nmáscara\nmáscara\n/maskaɾa/\n\n\nGeorgian\nნიღაბი\nnighabi\n/niɣɑbi/\n\n\nGerman\nMaske\nmaske\n/maskə/\n\n\nGreek\nμάσκα\nmáska\n/maska/\n\n\nGujarati\nમહોરું\nmahorũ\n-\n\n\nHausa\nabin rufe fuska\nabin rufe fuska\n-\n\n\nHawaiian\npale maka\npale maka\n-\n\n\nHebrew\nמסכה\nmasekháh\n-\n\n\nHindi\nमुखौटा\nmukhauta\n-\n\n\nHungarian\nmaszk\nmaszk\n/mɒsk/\n\n\nIcelandic\ngríma\ngríma\n/kriːma/\n\n\nIgbo\nmkpu\nmkpu\n/mkk͡p~ɓ̥u/\n\n\nIndonesian\ntopeng\ntopeng\n/topɛŋ/\n\n\nIndonesian\nmasker\nmasker\n/maskər/\n\n\nIrish\nmasc\nmasc\n/mɑːsk/\n\n\nItalian\nmaschera\nmascherina\n/maskeɾina/\n\n\nJapanese\nマスク\nmasuku\n/masɯkɯ/\n\n\nJavanese\nmask\nmask\n-\n\n\nKannada\nಮಸುಕು\nmukhavāḍa\n-\n\n\nKazakh\nмаска\nmaska\n/maska/\n\n\nKhmer\nរបាំង\nrbang\n-\n\n\nKinyarwanda\nagapfukamunwa\nagapfukamunwa\n/agapfukamuŋwa/\n\n\nKorean\n마스크\nmaseukeu\n/ma̠sʰɯkxɯ/\n\n\nKurdish (Kurmanji)\nberrû\nberrû\n/beru/\n\n\nKyrgyz\nмаска\nmaska\n-\n\n\nLao\nຫນ້າກາກ\nnā kāk\n-\n\n\nLatvian\nmaska\nmaska\n/maska/\n\n\nLithuanian\nkaukė\nkaukė\n/kâˑʊ̯ke/\n\n\nLuxembourgish\nmask\nmask\n/mask/\n\n\nMacedonian\nмаска\nmaska\n/maska/\n\n\nMalay\ntopeng\ntopeng\n/topɛŋ/\n\n\nMalay\nکدوق\nkedok\n/kedok/\n\n\nMalayalam\nമാസ്ക്\nmāsk\n-\n\n\nMaltese\nmaskra\nmaskra\n/maskra/\n\n\nMāori \nmaruhā\nmaruhā\n/maɾuha/\n\n\nMarathi\nलपवू\nlapavū\n-\n\n\nMongolian\nмаск\nmask\n/mask/\n\n\nMyanmar (Burmese)\nမျက်နှာဖုံး\nmyakhnahpum\n/mjɛʔn̥əpʰóʊɴ/\n\n\nNepali\nमुखवटा\nmukhavaṭā\n-\n\n\nNorwegian\nmaskara\nmaskara\n/maskara/\n\n\nOdia\nମାସ୍କ\nmāska\n-\n\n\nPashto\nماسک\nmâsk-hâ\n-\n\n\nPersian (Farsi)\nنقاب زدن\nmask\n/mask/\n\n\nPolish\nmaska\nmaska\n/maskɔ̃/\n\n\nPortuguese\nmáscara\nmáscara\n/maskaɾa/\n\n\nPunjabi\nਮਾਸਕ\nmāsaka\n-\n\n\nRomanian\nmasca\nmasca\n/maska/\n\n\nRussian\nмаскировать\nmaskirovat'\n/məskʲɪrɐˈvatʲ/\n\n\nSamoan\nufimata\nufimata\n-\n\n\nScots Gaelic\nmasg\nmasg\n/masɡ/\n\n\nSerbian\nмаскa\nmaska\n-\n\n\nSesotho\npata\npata\n-\n\n\nShona\nchifukidzo\nchifukidzo\n-\n\n\nSindhi\nماسڪ\nnutarian\n-\n\n\nSinhala\nවෙස්මුහුණ\nvesmuhuṇa\n-\n\n\nSlovak\nmaskovať\nmaskovať\n/maskovat/\n\n\nSlovenian\nmaska\nmaska\n/maska/\n\n\nSomali\nmaaskaro\nmaaskaro\n-\n\n\nSpanish\nmáscara\nmáscara\n/maskaɾa/\n\n\nSundanese\ntopéng\ntopéng\n/topɛŋ/\n\n\nSwahili\nmask\nmask\n/mask/\n\n\nSwedish (Svenska)\nmask\nmask\n/mask/\n\n\nTajik\nниқоб\nniqoʙ\n-\n\n\nTamil\nமுகமூடி\nmukamūṭi\n-\n\n\nTatar\nмаска\nmaska\n/maska/\n\n\nTelugu\nముసుగు\nmusugu\n-\n\n\nThai\nหน้ากาก\nnâakàak\n/naː˥˩kaːk̚˨˩/\n\n\nTurkish\nmaske\nmaske\n/mask̟ʰe/\n\n\nTurkmen\nmaska\nmaska\n-\n\n\nUkrainian\nмаскувати\nmaskuvaty\n/mɑskɐ/\n\n\nUrdu\nماسک\nmask\n-\n\n\nUyghur\nmask\nmask\n-\n\n\nUzbek\nniqob\nniqob\n-\n\n\nVietnamese\nkhẩu trang\nkhẩu trang\n/kʰəw˨˩˦ ʈaːŋ˧˧/\n\n\nWelsh\nmwgwd\nmwgwd\n/mʊɡʊd/\n\n\nXhosa\nimaski\nimaski\n-\n\n\nYiddish\nמאַסקע\nmaske\n-\n\n\nYoruba\nboju-abẹ\nboju-abẹ\n-\n\n\nZulu\nimaski\nimaski\n-\n\n\n\nhttps://drive.google.com/file/d/18SeJTiM2-JXR9SOqEg22wdkvNL3OxG3u/view?usp=sharing\nTo compute the similarity of each pair of translation equivalents, I followed Floccia et al.’s (2018) procedure. For each pair of translation equivalents, I computed their Levenshtein distance as the number of insertions, deletions and replacements a string character has to go through to become identical to the other, and then divided this value by the number of characters of the longest of the two strings, so that all values range between 0 and 1. To compute the Levenshtein distance, I used the stringdist() function of the stringdist R package."
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Orthographic distance",
    "text": "Orthographic distance\nI first computed the orthographic distance between each pair of translation equivalents. Since some word forms make use of different alphabets, I first romanised all word forms. By romanised, I mean that I searched for the transcription of each word form in the Roman alphabet, and used it as input to compute the Levenshtein distance for each pair of translation equivalents. Here’s how orthographically similar (the romanisations of) the translations of mask are (N = 110 pairs):"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Phonological distance",
    "text": "Phonological distance\nThe phonological similarity/distance may be more informative. This time I searched for or generated with the help of a native speaker a phonological IPA transcription of each word-form. I then used this transcription as input to compute the phonological similarity of each pair of translation equivalents. A pitfall in this process is the fact that phonemes are almost never identical across languages, so even the common phoneme /m/ could vary slightly on its pronunciation in two languages. If this difference is encoded in the IPA transcription (as different characters), the Levenshtein distance will be inflated. For this reason, I simplified some IPA transcriptions to preserve this similarity. I also removed tones. This is terribly wrong from a linguistics perspective, but it’s the only way I see to be able to play with some reliable data. Also I’m no linguist, so you have no power here.\n\nHere’s the same analysis performed on phonological transcriptions of a subset of those languages (N = 75 pairs, those I could find a reliable IPA transcription for or could find help from a native speaker):"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "title": "How similar is the word “mask” across languages?",
    "section": "Onsets",
    "text": "Onsets\nMost of the times, the phonological overlap comes from onset graphemes/phonemes. This is how many word-forms start with each onset:"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "title": "How similar is the word “mask” across languages?",
    "section": "Some disclaimers:",
    "text": "Some disclaimers:\nI tried ensuring that words referred to surgical masks (instead of other types of masks) with help from native speakers. Wrong translations may still have slipped in (or be just wrong). I wish I had time to double-check all of them (I did this for fun).\nThis analysis is probably affected by selection bias. I suspect many dissimilar translations are missing due to not being included in the translation apps I used (e.g. Google Translate). Feel free to contribute missing entries or make corrections!"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "title": "How similar is the word “mask” across languages?",
    "section": "Code and data",
    "text": "Code and data"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "title": "How similar is the word “mask” across languages?",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] htmltools_0.5.4  kableExtra_1.3.4 knitr_1.41       gt_0.8.0        \n [5] readxl_1.4.1     forcats_0.5.2    stringr_1.5.0    dplyr_1.0.10    \n [9] purrr_1.0.0      readr_2.1.3      tidyr_1.2.1      tibble_3.1.8    \n[13] ggplot2_3.4.0    tidyverse_1.3.2  quarto_1.2      \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.4          sass_0.4.4          jsonlite_1.8.4     \n [4] viridisLite_0.4.1   modelr_0.1.10       assertthat_0.2.1   \n [7] renv_0.16.0         googlesheets4_1.0.1 cellranger_1.1.0   \n[10] yaml_2.3.6          pillar_1.8.1        backports_1.4.1    \n[13] glue_1.6.2          digest_0.6.31       rvest_1.0.3        \n[16] colorspace_2.0-3    pkgconfig_2.0.3     broom_1.0.2        \n[19] haven_2.5.1         scales_1.2.1        webshot_0.5.4      \n[22] processx_3.8.0      svglite_2.1.0       stringdist_0.9.10  \n[25] later_1.3.0         tzdb_0.3.0          timechange_0.1.1   \n[28] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[31] withr_2.5.0         cli_3.6.0           magrittr_2.0.3     \n[34] crayon_1.5.2        evaluate_0.19       ps_1.7.2           \n[37] fs_1.5.2            fansi_1.0.3         xml2_1.3.3         \n[40] tools_4.2.2         hms_1.1.2           gargle_1.2.1       \n[43] lifecycle_1.0.3     munsell_0.5.0       reprex_2.0.2       \n[46] compiler_4.2.2      systemfonts_1.0.4   rlang_1.0.6        \n[49] grid_4.2.2          rstudioapi_0.14     rmarkdown_2.19     \n[52] gtable_0.3.1        DBI_1.1.3           R6_2.5.1           \n[55] lubridate_1.9.0     fastmap_1.1.0       utf8_1.2.2         \n[58] commonmark_1.8.1    stringi_1.7.8       parallel_4.2.2     \n[61] Rcpp_1.0.9          vctrs_0.5.1         dbplyr_2.2.1       \n[64] tidyselect_1.2.0    xfun_0.36"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#tldr",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#tldr",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "TLDR",
    "text": "TLDR\n\nWe need to import several CSV or TXT files and merge them into one data frame in R. Regardless of what function we use to import the files, vectorising the operation using purrr::map in combination with do.call or dplyr::bind_rows is the most time-efficient method (~25 ms importing 50 files with 10,000 rows each), compared to for loops (~220ms) or using lapply (~123 ms). data.table::fread is the fastest function for importing data. Importing TXT files is slightly faster than importing CSV files."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Why this post",
    "text": "Why this post\nTo analyse data in any programming environment, one must first import some data. Sometimes, the data we want to analyse are distributed across several files in the same folder. I work with eye-tracking data from toddlers. This means that I work with multiple files that have many rows. At 120 Hz sampling frequency, we take ~8.33 samples per second. A session for one participants can take up to 10 minutes. So these files are somewhat big. These data also tend to be messy, requiring a lot of preprocessing. This means that I need to import the same large files many times during the same R session when wrangling my way through the data, which takes a few seconds. After some iterations, it can be annoying. I have decided to invest all my lost time into analysing what method for importing and merging large files is the fastest in R so that the universe and I are even again.\nBelow I provide several options for importing data from the different files, using base R and tidyverse, among other tools. I will compare how long it takes to import and merge data using each method under different circumstances. You can find the whole code here in case you want to take a look, reproduce it or play with it1.\n1 Ironically, this code is super inefficient and messy. It takes ages to run, and has been written by copy-pasting multiple times. I didn’t feel like doing anything more elegant. Also, I don’t know how. Help yourself."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "How can I import large files and merge them?",
    "text": "How can I import large files and merge them?\nSo we have some files in a folder. All files have the same number of columns, the same column names, and are in the same format. I assume that data are tabular (i.e., in the shape of a rectangle defined by rows and columns). I also assume that data are stored as Comma-Separated Values (.csv) or Tab-separated Text (.txt or .tsv), as these formats are the most reproducible.\nWe to import all files and bind their rows together to form a unique long data frame. There are multiple combinations of functions we can use. Each function comes with a different package and does the job in different ways. Next, I will show some suggestions, but first let’s create some data. We are creating 50 datasets with 10 columns and 10,000 rows in .txt format. The variables included are numeric and made of 0s and 1s. There is also a column that identifies the data set. These files are created in a temporary directory using the temp.dir function for reproducibility. After closing you R session, this directory and all of its contents will disappear.\n\nBase R: for loops\nfor loops are one of the fundamental skills in many programming languages. The idea behind for loops is quite intuitive: take a vector or list of length n, and apply a series of functions to each element in order. First, to element 1 and then to element 2, and so on, until we get to element n. Then, the loop ends. We will first make a vector with the paths of our files, and then apply the read.delim function to each element of the vector (i.e., to each path). Every time we import a file, we store the resulting data frame as an element of a list. After the loop finishes, we merge the rows of all element of the list using a combination of the functions do.call and rbind.\n\n\nBase R: lapply\nWe will use the functions read.delim and read.csv in combination with the function lapply. The former are well known. The later is part of a family of functions (together with sapply, mapply, and some others I can’t remember) that take two arguments: a list and a function, which will be applied over each element of the list in parallel (i.e., in a vectorised way).\n\n\nTidyverse\nThe tidyverse is a family of packages that suggests a workflow when working in R. The use of pipes (%&gt;%) is one of its signature moves, which allow you to chain several operations applied on the same object within the same block of code. In contrast, base R makes you choose between applying several functions to the same object in different blocks of code, or applying those functions in a nested way, so that the first functions you read are those applied the last to your object (e.g., do.call(rbind, as.list(data.frame(x = \"this is annoying\", y = 1:100)))). We will use a combination of the dplyr and purrr packages to import the files listed in a vector, using read.delim and bind_rows.\n\n\ndata.table\nThe function rbindlist function from the package data.table also allows to merge the datasets contained in a list. In combination with fread (from the same package), it can be very fast."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "What method is the fastest?",
    "text": "What method is the fastest?\nI will compare how long each combination of importing, vectorising, and merging functions needs to import 50 data sets with 10 columns and 10,000 rows each. Additionally, I will compare the performance of each method when working with CSV (.csv) and TSV (.txt) files. For each method, I will repeat the process 100 times, measuring how long it takes from the moment we list the extant files in the folder to the moment we finish merging the data sets. Here are the results:\n\n\n\n\n\n\n\n\nFigure 1: Mean time (and standard deviation) for each combination of methods and file formats across 100 replications\n\n\n\n\n\nFor more detail:\n\n\n\n\nTable 1: Execution times\n\n\n\n\n\n\n  \n    \n      Time taken to import and merge\n    \n    \n      50 datasets with 10 columns and 10,000 rows each\n    \n  \n  \n    \n      \n      package\n      \n        for loop\n      \n      \n        lapply\n      \n      \n        purrr::map\n      \n    \n    \n      M\n      SD\n      M\n      SD\n      M\n      SD\n    \n  \n  \n    \n      do.call - .csv\n    \n    \nbase\n1.34\n0.06\n1.11\n0.07\n0.15\n0.03\n    \ndata.table\n1.33\n0.39\n0.88\n0.04\n0.17\n0.02\n    \nreadr\n1.38\n0.05\n1.07\n0.13\n0.15\n0.02\n    \n      dplyr::bind_rows - .csv\n    \n    \nbase\n1.35\n0.16\n0.99\n0.14\n0.15\n0.01\n    \ndata.table\n1.14\n0.04\n0.84\n0.06\n0.16\n0.01\n    \nreadr\n1.25\n0.14\n0.91\n0.04\n0.14\n0.00\n    \n      do.call - .txt\n    \n    \nbase\n1.18\n0.04\n0.88\n0.04\n0.17\n0.02\n    \ndata.table\n1.17\n0.04\n0.80\n0.05\n0.15\n0.02\n    \nreadr\n1.18\n0.05\n0.80\n0.05\n0.15\n0.02\n    \n      dplyr::bind_rows - .txt\n    \n    \nbase\n1.13\n0.03\n0.84\n0.06\n0.20\n0.02\n    \ndata.table\n1.19\n0.13\n0.77\n0.03\n0.14\n0.01\n    \nreadr\n1.13\n0.04\n0.77\n0.03\n0.14\n0.01\n    Mean\n—\n1.23\n0.10\n0.89\n0.06\n0.15\n0.02\n  \n  \n  \n\n\n\n\n\n\n\nFigure 1 and Table Table 1 show the detailed timings The grand mean average time taken by all methods is ~2.12 seconds, but there are some differences.\n\nIt doesn’t really matter what function we use to merge data sets: both do.call and dplyr::bind_rows perform roughly similarly.\nWhat makes the biggest difference is what function we use to vectorise the importing operation across file names to import them. purrr::map is the fastest. Incredibly, is takes less than 0.3 seconds in all conditions. It is also the least sensitive to the format of the files and the function we use to import them.\nThe next vectorising function in terms of temporal efficiency is lapply, which takes ~1.5 seconds. It performs slightly better when working with .txt files, in that when working with .csv files its performance depends on what method we use to import them: data.table::fread is much faster than its base and readr competitors. This post by Daniele Cook sheds some light into the advantage of data.table over other importing functions, also covering the vroom package, which this post doesn’t cover.\nUsing for loops looks like the least efficient method for iterating across data sets when importing data. It also shows a similar profile than lapply: data.table::fread performs a bit better than the rest."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#conclusion",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#conclusion",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Conclusion",
    "text": "Conclusion\nUnder the scenario under which I have simulated the data, it seems that using purrr::map in combination with do.call or dplyr::bind_rows to merge data sets is the most efficient method in terms of time. When using said combination, it doesn’t matter what function we use to import files, but data.table::fread seems like the best choice, as it is also the most flexible (take a look at the documentation of data.table to see all the features it offers).\nIf I have time, I may add another two dimensions: number of rows in the files and number of files, although I dare say similar results are to be expected. If anything, I would say that differences may become greater as file size and number of files increase. Also, it would be interesting to test if pre-allocating the elements of the vector in the for loop speeds up the process (see here what I mean). We shall see.\nHope this was useful, if not interesting!"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#code",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#code",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#session-info",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#session-info",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.8.0          ggsci_2.9         forcats_0.5.2     stringr_1.5.0    \n [5] readr_2.1.3       tidyr_1.2.1       tibble_3.1.8      ggplot2_3.4.0    \n [9] tidyverse_1.3.2   data.table_1.14.6 purrr_1.0.0       dplyr_1.0.10     \n[13] quarto_1.2       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9          lubridate_1.9.0     ps_1.7.2           \n [4] assertthat_0.2.1    digest_0.6.31       utf8_1.2.2         \n [7] R6_2.5.1            cellranger_1.1.0    backports_1.4.1    \n[10] reprex_2.0.2        evaluate_0.19       httr_1.4.4         \n[13] pillar_1.8.1        rlang_1.0.6         googlesheets4_1.0.1\n[16] readxl_1.4.1        rstudioapi_0.14     rmarkdown_2.19     \n[19] labeling_0.4.2      googledrive_2.0.0   munsell_0.5.0      \n[22] broom_1.0.2         compiler_4.2.2      modelr_0.1.10      \n[25] xfun_0.36           pkgconfig_2.0.3     htmltools_0.5.4    \n[28] tidyselect_1.2.0    fansi_1.0.3         crayon_1.5.2       \n[31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n[34] later_1.3.0         commonmark_1.8.1    grid_4.2.2         \n[37] jsonlite_1.8.4      gtable_0.3.1        lifecycle_1.0.3    \n[40] DBI_1.1.3           magrittr_2.0.3      scales_1.2.1       \n[43] cli_3.6.0           stringi_1.7.8       farver_2.1.1       \n[46] renv_0.16.0         fs_1.5.2            xml2_1.3.3         \n[49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.5.1        \n[52] tools_4.2.2         glue_1.6.2          hms_1.1.2          \n[55] processx_3.8.0      fastmap_1.1.0       yaml_2.3.6         \n[58] timechange_0.1.1    colorspace_2.0-3    gargle_1.2.1       \n[61] rvest_1.0.3         knitr_1.41          haven_2.5.1        \n[64] sass_0.4.4"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html",
    "href": "blog/animated-distributions/animated-distributions.html",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "",
    "text": "In the last couple of years I went down the rabbit hole of Bayesian statistics. Although I’ve made considerable progress, as compared to where I started, I still (and will) struggle understand some basic concepts beyond some shallow, abstract idea about what they mean. One of the first challenges I encountered in my first steps was becoming aware of how many probability distributions there are, and the fact that one should pick one should carefully chose which one to use when trying to estimate some parameter.\nBen Lambert’s excellent book A Student’s Guide to Bayesian Statistics offers a chart showing many of the most popular distributions, and how they relate to each other (pp. 145, see Figure 1 below for a similar chart). They then describe each distributions very plain terms, which is to be grateful for (many introductory books get lost in the details when describing likelihood distributions). However, when the moment arrives to pick a distribution in practice, I still struggle to see the risks and benefits of using each distribution. Will it adequately cover the sampling space of my parameter? Can I parameterise it so it allocates most of the probability around the regions I consider more likely? Am I actually able to interpret the values of the parameters of the distribution and map them to my research question?\nMy first step to answer these questions is to explore what a given distribution looks like under different parameters. This might give us a hint on the range of values it covers and the “shape” of the likelihood function. As I mentioned in previous posts, I struggle to grasp statistical/mathematical concepts in absence of a good visualisation. I generated some animations in Julia using the Distributions.jl package, which provides a substantial amount of implemented likelihood functions. The only reason I chose Julia is that I’m trying to learn it step by step and I found this silly project a nice opportunity to do so. You can see the code at the end of this article or on the accompanying GitHub repository. I posted this one Twitter (see below) and got a good response, so I decided to extend a bit the contents. Enjoy! :)"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#normal-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#normal-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Normal distribution",
    "text": "Normal distribution\nWikipedia\nWell, we all know this one. This is a continuous distribution that covers the whole range of real numbers. Why is it used so often? Because most data we find in real life are frequently quite plausible under a normal distribution. Critically, the normal distributions makes very few assumptions about the data we are trying to model. Why is it so? As Richard McElreath explains in Statistical Rethinking1 (section 4.1), there are many ways a data generating mechanism might spit out normal data, even when each of the individual observations are not drawn from a normal distribution. This is because when the data are the result of adding (or multiplying), the resulting distribution tends to converge to a normal one. As McElreath points out, many phenomena we observe in nature is the result of adding multiple small factors together (e.g., someone´s height is the result of adding multiple genetic and environmental factors together). Basically, whenever you are not sure about what distribution suits your data better, the normal distribution might be a good first approximation.\n1 You can also take a look a his excellent lecture on his YouTube channel, which cover much of the contents in the book!\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\big(\\frac{x-\\mu}{\\sigma}\\big)^2}\n\\]\nWhere \\(f(x)\\) us the probability density function, is the standard deviation, and is the mean.\n\n\n\n\n\n\nFigure 2: The normal distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#gamma-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#gamma-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nWikipedia\n\\[\nf(x) = \\frac{\\lambda(\\lambda x)^{\\alpha - 1}  \\;e^{-\\lambda x}}{\\Gamma(\\alpha)}\n\\]\nThis distribution only covers positive values, and has two parameters. It is in important one, as many other distribution, like the exponential, are specific cases of this one. In applied contexts, this distribution is used to model waiting times, and the incidence of some diseases, among others. In general, any continuous variable whose mode is expected to be closer rather than farther from zero can potentially be modelled by a Gamma distribution.\n\n\n\n\n\n\nFigure 3: The Gamma distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#inverse-gamma-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#inverse-gamma-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Inverse-gamma distribution",
    "text": "Inverse-gamma distribution\nWikipedia\nThis distribution is a reciprocal transformation of the Gamma distribution, and has similar properties. In can’t find many uses for this distribution if not as a conjugate prior for the variance of a normal distribution.\n\\[\nf(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1}  \\;exp\\Bigg(-\\frac{\\beta}{x}\\Bigg)\n\\]\n\n\n\n\n\n\nFigure 4: The inverse-gamma distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#exponential-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#exponential-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nWikipedia\nThis distribution is continuous and cover only positive values. It places most of the likelihood around zero, which makes it very convenient for modelling small positive quantities such as small distances or time intervals or standard deviations. Its shape depends on only one parameter, \\(\\lambda\\), which makes it simpler to parameterise (as compared to others like the Beta distribution). It has an inconvenient, though: it is a bit difficult to interpret what the value of this parameter means in the context of our research question (i.e., theory). While one can interpret the mean and standard deviation of the normal distribution as where the most likely value of the distribution lies and its associated uncertainty, respectively, the \\(\\lambda\\) parameter is not that trial to interpret: it doesn’t relate to the most likely values of the distribution linearly, and it cannot be interpreted a a rate or any other occurrence metric. What works for me is to take a look at the resulting distribution and see whether it captures my expectations about the data I’m about to observe.\n\\[\nf(x) = \\lambda e^{-\\lambda x}\n\\]\n\n\n\n\n\n\nFigure 5: The exponential distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#student-t-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#student-t-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Student-t distribution",
    "text": "Student-t distribution\nWikipedia\n\\[\nf(x) = \\frac{\\Gamma((\\upsilon+1)/2)}{\\sqrt{\\upsilon \\pi}  \\;\\Gamma (\\upsilon /2)}\n\\]\n\n\n\n\n\n\nFigure 6: The Student’s t distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#beta-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#beta-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Beta distribution",
    "text": "Beta distribution\nWikipedia\n\\[\nf(x) = \\frac{x^{\\alpha-1} \\;(1-x) \\;x^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\n\n\n\n\n\n\nFigure 7: The Beta distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#cauchy-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#cauchy-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Cauchy distribution",
    "text": "Cauchy distribution\nWikipedia\n\\[\nf(x) = \\frac{1}{\\pi \\gamma \\Bigg[1 + \\big( \\frac{x-x_0}{\\gamma} \\big)^2 \\Bigg]}\n\\]\n\n\n\n\n\n\nFigure 8: The Cauchy distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#frechet-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#frechet-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Frechet distribution",
    "text": "Frechet distribution\nWikipedia\n\\[\nf(x) = \\alpha \\; x^{-1-\\alpha} e^{-x^{-\\alpha}}\n\\]\n\n\n\n\n\n\nFigure 9: The Frechet distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#pareto-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#pareto-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Pareto distribution",
    "text": "Pareto distribution\nWikipedia\n\\[\nf(x) = \\frac{\\alpha  \\;x_{m}^{\\alpha}}{x^{\\alpha+1}}  \\;\\text{for}  \\; x &gt; x_m\n\\]\n\n\n\n\n\n\nFigure 10: The pareto distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#weibull-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#weibull-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Weibull distribution",
    "text": "Weibull distribution\nWikipedia\n\\[\nf(x) = \\lambda \\alpha(\\lambda x)^{\\alpha-1} \\; e^{-(\\lambda x)^{\\alpha}}\n\\]\n\n\n\n\n\n\nFigure 11: The Weibull distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#tri-weight-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#tri-weight-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Tri-weight distribution",
    "text": "Tri-weight distribution\nWikipedia\n\n\n\n\n\n\nFigure 12: The tri-weight distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#rayleigh-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#rayleigh-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Rayleigh distribution",
    "text": "Rayleigh distribution\nWikipedia\nSimilar to Gamma or Poisson, the Rayleigh likelihood function covers all positive values and relies on only one parameter, \\(\\sigma\\), to determine its shape and location. Lower values of \\(\\sigma\\) make the distribution lie closer to zero and be less disperse. Despite its seemingly less interesting appearance, this distributions is extensively used for modelling vibrational data such as water displacement, assessing the quality of railroads, or analysing MRI data2.\n2 See https://www.sciencedirect.com/topics/engineering/rayleigh-distribution\\[\nf(x) = \\frac{x}{\\sigma^2} \\; e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\n\n\n\n\n\nFigure 13: The Rayleigh distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#wigner-semi-circle-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#wigner-semi-circle-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Wigner semi-circle distribution",
    "text": "Wigner semi-circle distribution\nWikipedia\nThis distribution is, well, a semicircle when \\(R = 1\\), were \\(R\\) is the radius of the semi-circle. All values of the sampling space outside the [-R, R] interval have zero probability. I can’t say much more than that. I have no idea why this distribution even exists, and I have struggled to find documentation about it in the reasonably long period of time I searched for it. I suspect this distribution might be useful for geo-spatial modelling, where one wants to actually model the shape of an object. Otherwise, I’m lost.\n\\[\nf(x) = \\frac{2}{\\pi R^2} \\; \\sqrt{R^2 - x^2} \\\\ \\text{where} \\; \\pi \\; \\text{is the actual number } \\pi \\text{, not a parameter}\n\\]\n\n\n\n\n\n\nFigure 14: Wigner’s semicircle distribution.\n\n\n\nInterestingly, this distribution can be considered a particular case3 of the Beta distribution, so that where \\(\\alpha = \\beta = 3/2\\), then \\(X = 2RY – R\\).\n3 See https://handwiki.org/wiki/Wigner_semicircle_distribution"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#triangular-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#triangular-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Triangular distribution",
    "text": "Triangular distribution\nWikipedia\nAs a curiosity, there is such thing as a triangular distribution. It does look triangular, as can be seen in the Wikipedia article linked above. This distribution is not implemented yet in Distributions.jl. As with the semi-circle distribution, I struggle to see any context where this distribution might be useful beyond geo-spatial modelling.\n\\[\nf(x) = \\begin{cases}\n    0  & \\text{for}  \\; x &lt; \\alpha, \\\\\n    \\frac{2(x-a)}{(b-a)(c-a)} & \\text{for} \\; a \\leq x &lt; c, \\\\\n    \\frac{2}{b-a} &  \\text{for} \\; x = c, \\\\\n    \\frac{2(b-x)}{(b-a)(b-c)} & \\text{for} \\; c &lt; x \\leq b, \\\\\n    0 & \\text{for} \\; b &lt; x\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#some-final-remarks",
    "href": "blog/animated-distributions/animated-distributions.html#some-final-remarks",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Some final remarks",
    "text": "Some final remarks\nSome well-known distributions are missing (e.g., Dirichlet, Wishart, Beta-binomial). This is because they have not yet been implemented in Distributions.jl, and I sadly lack the knowledge to contribute on this. Still feel free to explore the Wikipedia or other resources to get an idea of how these distributions look like in what contexts they are useful!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a cognitive scientist interested in the application of Bayesian modelling in the investigation of early language acquisition. As part of my PhD in Biomedicine at at the Center for Brain and Cognition at Universitat Pompeu Fabra (Barcelona, Spain), I study how bilingual toddlers develop their vocabulary, using both experimental and observational techniques. I particularly enjoy building code-based workflows, ensuring that my projects can be run by others and my future self, from data collection to visualisation and reporting. I mainly use Twitter (@gongcastro) for sharing research outputs, programming tips or data visualisations. You can also follow me on GitHub for some portfolio projects like:\n\n{multilex}: R package for downloading and processing vocabulary data from 10-40 month-old children living in the Metropolitan Area of Barcelona (Spain) [website]\n{comidistar}: R package containing the scores given to products in the blind taste tests of El Comidista\npsicotuiterbot: A Twitter bot programmed in R using {rtweet} that automatically retweets mentions of the hasthtags #psicotuiter or #psicotwitter in Spanish"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "upfthesis: a Quarto template for thesis dissertations at UPF\n\n\n\n\n\n\nr\n\n\nquarto\n\n\nthesis\n\n\nbook\n\n\npdf\n\n\n\nI wrote my thesis dissertation in Quarto, using a custom template. Here I illustrate how it works and some tips.\n\n\n\n\n\nJan 10, 2024\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nUsing decktape to convert Quarto slides from RevealJS to PDF\n\n\n\n\n\n\nr\n\n\nquarto\n\n\nrevealjs\n\n\npresentation\n\n\npdf\n\n\n\nUsing decktape to export my Quarto slides to PDF saved my life right before a submission deadline. I might save yours too.\n\n\n\n\n\nJun 8, 2023\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nGetting the most out of logistic regression\n\n\n\n\n\n\nr\n\n\ntidyverse\n\n\nregression\n\n\nstatistics\n\n\nlogistic\n\n\n\nLogistic regression models provide information way beyond a p-value. Using the {palmerpenguins} dataset, I review the relationship between the logistic and the logit functions, and how to interpret the coefficients of a logistic regression model, capitalising on marginal effects.\n\n\n\n\n\nJan 22, 2023\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nrenv (o cómo usar paquetes de R sin ataques de pánico)\n\n\n\n\n\n\nr\n\n\npackage\n\n\nreproducibility\n\n\nrenv\n\n\n\n{renv} es un paquete de R que permite instalar paquetes de R gestionar sus versiones para proyectos de forma independiente. Aquí resumo para qué se utiliza y cómo funciona.\n\n\n\n\n\nFeb 27, 2022\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\n@psicotuiterbot: Un bot de Twitter para Psicotuiter\n\n\n\n\n\n\nr\n\n\nrtweet\n\n\ntwitter\n\n\nraspberry pi\n\n\nbot\n\n\ngithub\n\n\n\nHe creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON.\n\n\n\n\n\nDec 29, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nCreando un paquete de R: una guía informal\n\n\n\n\n\n\nr\n\n\npackage\n\n\ntwitter\n\n\ntutorial\n\n\n\nEn el canal de Twitch de Psicometries hicimos un directo en el que explicamos cómo se ha creado el paquete de R {psicotuiteR}, indicando cada paso lo mejor que hemos podido para que puedas replicarlo, contribuir al mismo paquete o incluso crear tu propio paquete en el futuro.\n\n\n\n\n\nNov 14, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nExploring probability distributions through animations in Julia\n\n\n\n\n\n\njulia\n\n\ndistribution\n\n\nanimation\n\n\nstatistics\n\n\nprobability\n\n\n\nVisualising what different probability distributions look like under different parameters can be helpful when picking a likelihood function for you Bayesian analysis. I present some animations generated with Julia using Distributions.jl\n\n\n\n\n\nOct 4, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising polynomial regression\n\n\n\n\n\n\nr\n\n\nstatistic\n\n\nregression\n\n\nanimation\n\n\nggplot\n\n\n\nThe outputs of polynomial regression can be difficult to interpret. I generated some animated plots to see how model predictions change across different combinations of coefficients for 1st, 2nd, and 3rd degree polynomials.\n\n\n\n\n\nJan 21, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nHow similar is the word “mask” across languages?\n\n\n\n\n\n\nr\n\n\nlinguistics\n\n\nphonology\n\n\nggplot2\n\n\ntranslation\n\n\n\nUsing the Levenshtein distance to quantify the orthographic and phonlogical similarity between translation equivalents of the word mask across multiple languages.\n\n\n\n\n\nNov 20, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nImporting data from multiple files simultaneously in R\n\n\n\n\n\n\nr\n\n\ntidyverse\n\n\nbase\n\n\npurrr\n\n\nimport\n\n\n\nA comparison between base R and Tidyverse methods for importing data from multiple files\n\n\n\n\n\nJul 5, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nA primer on mixed-effects Models: theory and practice\n\n\n\n\n\n\nr\n\n\nstatistics\n\n\nmultilevel\n\n\nmixed models\n\n\nanimations\n\n\n\nSlides from a tutorial on mixed-effects models I presented to my research group.\n\n\n\n\n\nMar 31, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html",
    "title": "Creando un paquete de R: una guía informal",
    "section": "",
    "text": "En este tutorial en colaboración con Alicia Franco-Martínez 1, explicaremos cómo se ha creado el paquete de R {psicotuiteR}. En el canal de Twitch de Alicia, Psicometries (al que también debéis suscribiros), hicimos un directo en el que intentamos mostrar por encima lo que trataré en este tutorial. Trataremos de explicar cada paso lo mejor que podamos para que puedas replicarlo, contribuir al mismo paquete o incluso crear tu propio paquete en el futuro."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-el-paquete-psicotuiter",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-el-paquete-psicotuiter",
    "title": "Creando un paquete de R: una guía informal",
    "section": "¿Qué es el paquete {psicotuiteR}?",
    "text": "¿Qué es el paquete {psicotuiteR}?\n\n\nEl paquete psicotuiteR es un paquete muy simple que hemos creado para que la gente de la comunidad de #psicotuiter, en Twitter, pudiera experimentar con él añadiendo funciones o jugando con los datos que incluye. Podéis ver más información sobre el paquete en su página web. La comunidad es psicotuiter es un grupo de castellanoparlantes que hablan, entre otras cosas, sobre psicología y salud mental en Twitter."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-un-paquete-de-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-es-un-paquete-de-r",
    "title": "Creando un paquete de R: una guía informal",
    "section": "¿Qué es un paquete de R?",
    "text": "¿Qué es un paquete de R?\nEn el manual R packages de Hadley Wickham y Jenny Bryan, se describe un paquete de R así:\n\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others.\n\nA mí me gusta definirlo como un grupo de funciones documentadas que se agrupan siguiendo el formato que el alto consejo jedi2 ha dictado.\n2 La gente que gestiona CRAN, vaya. Tienen cierta fama de boomers.Por cierto, R packages es el mejor recurso que existe en este momento para aprender a hacer paquetes de R. No hay nada en este tutorial que no esté incluido ahí–incluso mejor explicado–a excepción de algún que otro comentario autodespectivo al pie de página."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#por-qué-hacer-un-paquete-de-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#por-qué-hacer-un-paquete-de-r",
    "title": "Creando un paquete de R: una guía informal",
    "section": "¿Por qué hacer un paquete de R?",
    "text": "¿Por qué hacer un paquete de R?\nCuando programamos, es común que necesitemos ejecutar la misma línea de código varias veces. Cuando esto ocurre, en lugar de escribir y ejecutar la misma línea de código una y otra vez en la consola de R, podemos escribir un script. Un script de R es un archivo con la extensión .R3 que contiene las diferentes líneas de código que queremos ejecutar.\n3 Por convención se suele usar la “R” mayúscula, aunque la mayoría de los sistemas operativos son indiferentes a que escribamos las extensiones en mayúsculas o minúsculas. Hagas lo que hagas, trata de ser consistente.A veces el mismo script contiene líneas de código muy parecidas que ejecutamos para aplicar, por ejemplo, la misma función sobre objetos diferentes. Por ejemplo:\nlm(x ~ y, data = df1)\nlm(x ~ y, data = df2)\nEn el bloque de código de arriba, df1 y df2 son objetos de tipo data.frame con variables similares pero diferentes observaciones. Idealmente, no deberíamos repetir la misma línea de código más de una vez. Digo idealmente porque la alternativa es escribir una función. No siempre merece la pena hacer una función, por mucho que la gente más purista insista. Si estás leyendo esto doy por hecho que te interesa hacer tu código más conciso y replicable.\nEn todo caso es generalmente recomendable definir una serie de funciones antes de trabajar sobre tus datos. Una función es un conjunto de comandos que se ejecutan en orden cuando damos la orden4. Estos comandos se agrupan bajo el nombre que asignemos a la función. Así podemos condensar nuestro código en funciones para que sea más conciso.\n4 Definición mediocre pero obligada. Lo siento.En un último nivel de abstracción, nivel cerebro galáctico, está agrupar nuestras funciones en un paquete. Hacer esto tiene unos cuantos beneficios:\n\nNo tendremos que definir las funciones cada vez que abramos un script: bastará con cargar el paquete y sus funciones estarán disponibles para usarlas.\nSerá más fácil compartir nuestro código con otras personas: es común que nuestras funciones requieran, tener instalados ciertos paquetes externos. Por ejemplo, si mi función usa la función mutate del paquete dplyr, quien quiera usar mi función deberá tener instalado dplyr (a veces es inluso necesario tener instalada la misma versión del paquete). Este es uno de las principales amenazas a la reproducibilidad de nuestro código. Un paquete de R, sin embargo, se asegura que, durante la instalación, se instalen las dependencias necesarias en el ordenador de la otra persona,\nPodremos documentar las funciones fácilmente (se acabaron los comentarios escuestos e indescifrables en nuestros scripts) y hacer esta documentación accesible a quien use nuestro paquete al ejecutar ?mifuncion (puedes ejecutar ?mean para ver un ejemplo de cómo se verá nuestra documentación).\n\nHay más motivos por los que puede ser buena idea crear un paquete de R, como por ejemplo trabajar con tu propio código de forma más cómoda y reproducible5, para compartir y documentar bases de datos (a veces nuestro paquete no incluirá ninguna función, sino únicamente unos datos y su documentación) o para aprender R más a fondo y sus entresijos. Yo he aprendido más intentando hacer paquetes de R que en cualquer tutorial.\n5 Tu yo futuro te lo agradecerá y si algún día se puede viajar en el tiempo lo harás para darte un beso en la frente por ello."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-necesito-para-hacer-un-paquete-de-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qué-necesito-para-hacer-un-paquete-de-r",
    "title": "Creando un paquete de R: una guía informal",
    "section": "¿Qué necesito para hacer un paquete de R?",
    "text": "¿Qué necesito para hacer un paquete de R?\nPara seguir este tutorial, y en general para crear un paquete de R necesitaremos instalar en nuestro ordenador6:\n6 En el momento de escribir este tutorial yo estoy usando R 4.0.4, RStudio 1.4.1103, rmarkdown 2.11.1, devtools 2.4.2 y usethis 2.0.1. Echa un vistazo a esta guía para ver cómo instalar una versión específica de un paquete de R.\nR\nRStudio\nLos siguientes paquetes de R: devtools, usethis y rmarkdown.\n\nPuedes instalar estos paquetes así:\ninstall.packages(\"devtools\", \"usethis\", \"rmarkdown\")\nCuando tengamos todo instalado, reiniciaremos nuestra sesión de RStudio y después cargaremos devtools y usethis (usaremos rmarkdown más adelante):\nlibrary(devtools)\nlibrary(usethis)"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#inicializando-el-paquete",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#inicializando-el-paquete",
    "title": "Creando un paquete de R: una guía informal",
    "section": "Inicializando el paquete",
    "text": "Inicializando el paquete\nSalvo contadas excepciones7, las personas de bien utilizaremos RStudio para programar en R, en lugar de usar únicamente la consola como hacen les psicópatas. Trabajar en un proyecto de RStudio nos facilitará mucho la vida al hacer un paquete de R y deberías hacerlo casi siempre que programas en R (como mínimo, te ahorrará mucho tiempo buscando documentos dentro de carpetas)8.\n7 Por ejemplo, si tienes el suficiente tiempo libre como para configurar una sesión de R medianamente funcional en Visual Studio Code.8 Nunca está demás ver este tutorial de Danielle Navarro sobre cómo organizar un repositorioPara crear un paquete tenemos dos opciones: hacerlo a través de RStudio (ver tutorial) o usando usethis en nuestra consola. A mi me gusta la segunda opción:\ncreate_package(path = \"psicotuiteR\") # abre nueva ventana\nEste comando creará una nueva carpeta. Lo hará dentro de tu repositorio de inicio, el cual puedes consultar ejecutando getwd() en tu consola (asumiendo que nos has cambiado el directorio por tu cuenta previamente). Esta carpeta contendrá varios archivos y una carpeta:\n.gitignore\n.Rbuildignore\nDESCRIPTION\nNAMESPACE\npsicotuiteR.Rproj\nR\n\n\n\n\n\n\nImportant\n\n\n\nLos nombres de la mayoría de archivos y carpetas en este directorio son importantes. Trata de no cambiarlos si no es imprescindible.\n\n\nUno de los archivos en la carpeta tiene el mismo nombre que el paquete y la extensión .Rproj. Cada vez que queramos trabajar en nuestro paquete es recomendable abrir la sesión de RStudio haciendo doble click sobre el archivo .Rproj. Veremos qué son el resto de archivos más adelante.\nPues bien: técnicamente, ¡ya tenemos un paquete de R! Si ejecutamos el código de abajo, el paquete se instalará como si fuera uno más en nuestro directorio de paquetes de R.\ndevtools::install()\nEncontrarás la carpeta de instalación junto a la de los demás paquetes que hays instalado en tu ordenador. Puedes consultar dónde se instalan tus paquetes de R ejecutando .libPaths() en tu consola. La primera ruta que aparezca será donde encontrarás tu paquete instalado (en mi caso, encontraré la carpeta ~/Documents/R/win-library/4.0/psicotuiteR).\nLa función que hemos ejecutado, install() del paquete devtools simula lo que otra persona haría al ejecutar la función install.packages() si nuestro paquete estuviera disponible en CRAN. Ahora mismo, podríamos cargar nuestro paquete con library(psicotuiteR) y trabajar con él. Por supuesto, nuestro paquete aún está vacío. En las próximas secciones veremos cómo añadir funciones, entre otras cosas, a nuestro paquete.\nAntes de hacer eso, ahí va un truco: cuando añadimos o hacemos cambios en el paquete, necesitaremos actualizar nuestra sesión y cargar el paquete de nuevo y volver a ejecutar las funciones para comprobar que todo funciona correctamente. En lugar de ejecutar install() cada vez que queremos ver si nuestro paquete funciona, podemos ejecutar load_all() (también del paquete devtools) sin siquera reiniciar la sesión y así cargar de nuevo el paquete actualizado como si alguien hubiera cargado el paquete usando library(). Los contenidos del paquete que se cargarán usando load_all() son los que hay en el nuestro repositorio (desde el que estamos desarrollando el paquete), y no desde el directorio en el que se instala el paquete si ejecutamos install(). ¡Esto es mucho más rápido y eficiente!\nOtro truco: puesto que vamos a utilizar las funciones de los paquetes devtools y usethis a menudo–y por lo tanto vamos a necesitar cargar estos paquetes mediante library(devtools) y library(usethis) cada vez que iniciemos una sesión de R– podría ser recomendable añadir esos dos comandos a nuestro archivo .Rprofile (ver esta guía para más información sobre .Rprofile. Las líneas de código que contenga ese archivo serán ejecutadas en cada inicio de sesión que hagamos en nuestro proyecto de RStudio. Podemos hacer esto usando las siguientes funciones de usethis:\nlibrary(usethis)\nuse_usethis() \nuse_devtools()\nEsto creará un archivo llamado .Rprofile en nuestro directorio y añadirá, entre otras cosas muy útiles, los comandos library(usethis) y library(devtools)."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#description",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#description",
    "title": "Creando un paquete de R: una guía informal",
    "section": "DESCRIPTION",
    "text": "DESCRIPTION\nEl primer archivo que vamos a explicar se llama DESCRIPTION. La mayor parte de la información general (o meta-información) de nuestro paquete se encuentra en este archivo: autoría, ajustes generales, dependencias, etc. Puedes consultar el DESCRIPTION de psicotuiteR para hacerte una idea de cómo se ve cuando está editado. ¡Presta especial atención a cómo hemos indicado la autoría!\nDESCRIPTION es uno de los pocos archivos que tendremos que editar tanto a mano como mediante otras funciones del paquete usethis. Tendremos que cambiar el título, autoría y descripción del paquete a mano (además de algún otro campo), mientras que, por ejemplo, los campos Imports, Depends y Suggests serán rellenados más adelante mediante la función use_package(), de usethis. Cuando hablemos de dependencias, más adelante, comentaremos un par de cosas saobre estos tres campos."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#funciones-en-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#funciones-en-r",
    "title": "Creando un paquete de R: una guía informal",
    "section": "Funciones en R",
    "text": "Funciones en R\nLas funciones de R son el cuerpo principal de un paquete de R. Contienen el código que se ejecutará en nuestras funciones y su correspondiente documentación. Las funciones de por sí no tienen gran misterio. Las puedes hacer más simples o más complejas. Generalmente, por motivos de legibilidad suele ser mejor mantener nuestras funciones lo más simples que podamos. Es mejor escribir muchas funciones que hacen tareas pequeñas que pocas funciones que lo hacen todo. No tienes por qué hacer disponibles todas las funciones que escribas: puedes mantener algunas funciones para uso interno dentro de otras funciones que sí están disponibles al público (ahora veremos cómo). Hay muchos tutoriales para aprender a hacer funciones en R. Por ejemplo este. Merece la pena ganar algo de confianza en poder hacer funciones de R: empodera mucho y te hace entender muchos de los errores que te encontrarás a lo largo de tu vida programando en R. Ahí va un ejemplo de función muy simple:\nprint_name &lt;- function(\n    author = \"Gon\"\n){\n    print(author)\n}\nEsas líneas de código definen la función print_name(). Esta función incluye un argumento llamado author, que, por defecto, toma el valor \"Gon\". Si definimos la función y ejecutamos print_name() en nuestra consola de R, nos devolverá el valor `“Gon”.\nGuardaremos esta función en un archivo con la extensión .R dentro de la carpeta R/ en nuestro repositorio principal (el nombre que pongamos a este archivo da igual, pero trata de que sea informativo de su contenido). Yo llamaré a este archivo print_name.R. Ahora nuestro directorio se ve así:\n.gitignore\n.Rbuildignore\nDESCRIPTION\nNAMESPACE\npsicotuiteR.Rproj\nR\n    |-print_name.R"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#documentación",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#documentación",
    "title": "Creando un paquete de R: una guía informal",
    "section": "Documentación",
    "text": "Documentación\nAhora vamos a documentar esa función. R utiliza un tipo de lenguaje parecido a LaTeX para escribir la documentación de un paquete. Este lenguaje se llama R documentation. Técnicamente, podríamos escribir toda la documentación de cada función de nuestro paquete en este lenguaje y guardar cada archivo en la carpeta man/ con la extensión .Rd. En esta carpeta es donde R espera encontrar la documentación. Gracias a Dios (y a la buena voluntad de la comunidad de R), podemos incluir los contenidos de esos archivos como si fueran comentarios en nuestros scripts de R, encima de cada función. La función document() de devtools se encargará de generar todos los .Rd necesarios en la carpeta man/ por nosotrxs. Volveremos a esto más adelante. Por ahora, observa un ejemplo de función documentada:\n#' Print author\n#' @export print_name\n#' @usage print_author()\n#' @import dplyr\n#' @importFrom tidyr drop_na\n#' @description Print the name of the author of the package we are developing\n#' @param author Name of the package author\n#' @author Gonzalo Garcia-Castro\nprint_name &lt;- function(\n    author = \"Gon\"\n){\n    print(author)\n}\nEn la parte de abajo encontramos las mismas líneas de código que hemos visto antes. En la parte de arriba encontramos la documentación de la función. Estas líneas de código están precedidas por el símbolo #'. La apóstrofe indica que no es un comentario cualquiera, sino parte de la documentación de la función. En la mayoría de estas líneas indicamos mediante el símbolo @ qué campo estamos describiendo (autoría, descripción, un argumento, etc.).\nPor ejemplo, la línea de abajo indica que estamos describiendo uno de los argumentos de la función (por alguna razón que desconozco, se ha decidido refererirse a los argumentos como “@param” y no como @arg“). La primera línea se asume que es el título de la función y por eso no hace falta indicar @title antes de \"Print author\".\n#' @param author Name of the package author\nUna vez hayamos rellenado la documentación de nuestra función, ejecutaremos document() y se generará automáticamente un archivo llamado print_name.Rd en la carpeta man/. Podemos comprobar que la documentación se ha guardado correctamente ejecutando ?print_name. Se debería abrir la ventana Help en uno de los paneles de RStudio. Algunos consejos cuando rellenes la documentación de tus funciones:\n\nExplica las cosas con claridad, pero no tengas miedo de extenderte o repetir las cosas. Más documentación siempre es mejor que menos documentación, siempre que se expliquen las cosas con claridad y se mantenga cierto sistema en la estructura de la documentación.\nEcha un vistazo a la documentación de las funciones princpiales de tus paquetes favoritos. Es la mejor forma de saber cómo documentar un función y qué campos son los más importantes.\n\n\nViñetas y artículos\nUna forma más elaborada de documentar un paquete es crear viñetas. Una viñeta (o vignette, en inglés) es un documento en el que se explica con detalle cómo se trabaja con el paquete, como si fuera un tutorial. Las viñetas son particularmente útiles para quienes usan el paquete por primera vez, y deberían ilustrar, como mínimo, algún ejemplo sobre cómo se usan las funciones. Un buen ejemplo de viñeta es esta, del paquete {dplyr}, en la que indican cómo usar la función group_by(). Para crear una viñeta ejecutaremos el siguiente comando:\nuse_vignette(\n  name = \"print-name\" # así se llamará el archivo,\n  title = \"Imprimiendo un nombre\" # así se titulará la viñeta\n)\nEste comando creará una carpeta llamada vignettes/ y creará un archivo con la extensión .Rmd (Rmarkdown). Los archivos Rmarkdown son una mezcla entre un archivo Markdown (que tienen la extensión .md) y un script de R. Markdown es un lenguaje de edición de textos bastante sencillo (desde luego más sencillo que LaTeX). Rmarkdown permite intercalar bloques de código de R en medio del texto. Cuando renderizamos el documento, esos bloques de código se ejecutan y el resultado se incluye como texto o imagen. Este tipo de documentos son muy útiles para crear informes y actualizar los datos que incluyen con sólo un click. Si quieres aprender a user Rmarkdown (100% recomendado), el manual R Markdown: The Definitive Guide de Yihui Xie es el mejor recurso. Una vez escribamos nuestra viñeta podremos incluirla en nuestra documentación ejecutando:\nbuild_vignettes()\ndocument()\nComo dato curioso, este mismo tutorial que estás leyendo es una viñeta incluida en el paquete psicotuiteR. Muy meta todo, ¿verdad?"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#dependencias-y-otras-pesadillas",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#dependencias-y-otras-pesadillas",
    "title": "Creando un paquete de R: una guía informal",
    "section": "Dependencias y otras pesadillas",
    "text": "Dependencias y otras pesadillas\nCon frecuencia nuestras funciones de R dependerán, a su vez, de funciones de otros paquetes. Considera la siguiente función:\n#' Una función que añade una variable\n#' @param x Una serie de caracteres\n#' @returns El número incluido en x\nextrae_numeros &lt;- function(x){\n  y &lt;- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nComo indica la documentación, esta función devuelve los números incluidos en x, una serie de caracteres que introducimos como argumento, en formato numérico. Para ello utiliza la función str_extract(). Esta función pertenece al paquete {stringr}. Tal y como está escrita esta función, cuando instalemos el paquete y la ejecutemos no dará un error: R nos indicará que la función str_extract no existe. Podría ocurrírsenos usar algo como stringr::str_extract o library(stringr) dentro de la función. Pero esto no es buena idea porque muchas veces no funcionará. Necesitamos incluir la función str_extract y stringr como dependencias de nuestro paquete, para que cuando alguien instale el paquete esa función se encuentre disponible para el paquete cuando ejecutemos nuestra función extrae_numeros(). Para indicar una dependencia, debemos hacerlo en dos pasos:\n\nEjecutar use_package(\"stringr\") para incluir stringr en la lista de paquetes que deben instalarse junto al nuestro. Al hacerlo, verás que stringr ha sido incluido en el campo Imports del archivo DESCRIPTION.\nIncluir la función str_extract del paquete stringr en la documentación de la función de la siguiente forma:\n\n#' @importFrom stringr str_extract\nAhora nuestra función debería verse así:\n#' Una función que añade una variable\n#' @param x Una serie de caracteres\n#' @importFrom stringr str_extract\n#' @returns El número incluido en x\nextrae_numeros &lt;- function(x){\n  y &lt;- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nLa función str_extract estará disponible y nuestra función podrá ejecutarse sin problemas. Otra opción sería importar el paquete stringr al completo, con todas sus funciones. En lugar de indicar (importFrom?) con el nombre del paquete y la función que queremos importar, indicaríamos @import y solamente nombre del paquete: #' @import stringr. Sin embargo, casi siempre es mejor indicar las dependencias una a una, aunque sea repetitivo: así será más fácil detectar de dónde viene cada función que usamos. Si vamos a usar varias funciones del mismo paquete, podemos indicarlas una debajo de otra:\n#' @importFrom stringr str_extract\n#' @importFrom stringr str_detect\n#' @importFrom stringr str_replace\nSólo en el caso de que utilicemos muchísimas funciones del mismo paquete en nuestra función tendrá cierto sentido importar el paquete al completo. Las dependencias de nuestro paquete sólo estará disponibles cuando ejecutemos document() y se actualice la documentación. Esto es porque document() no sólo construye la documentación del paquete (los archivos .Rd en man/), sino que también modifica el archivo NAMESPACE para enumerar las dependencias. Echa un vistazo a NAMESPACE de psicotuiteR. Este archivo ha sido generado al ejecutar document(). Una de nuestras funciones incluye #' @import janitor clean_names y por tanto esta función ha sido incluida en el NAMESPACE.\nHemos aprendido a indicar dependencias de otros paquetes en nuestro código através de la documentación: funciones que nuestras propias funciones necesitan. Para hacer nuestras funciones disponibles en el paquete, necesitaremos incluirlas también en el NAMESPACE, pero no como dependencias, sino como exportaciones: en lugar de incluirlas en la documentación usando @importFrom, lo haremos usando #' @export. Volviendo al ejemplo anterior: #' @export extrae_numeros. Nuestra función se ver ahora así:\n#' Una función que añade una variable\n#' @export extrae_numeros\n#' @param x Una serie de caracteres\n#' @importFrom stringr str_extract\n#' @returns El número incluido en x\nextrae_numeros &lt;- function(x){\n  y &lt;- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nLa segunda línea indica que está función debe estar disponible en nuestro paquete con el nombre extrae_numeros. Aunque es posible exportar la función con un nombre diferente al que le hemos asignado en el script (R nos dará un aviso, pero no un error), es importante que así sea.\nPodrías preguntarte: ¿qué sentido tiene crear funciones si no van a estar disponibles para quien use el paquete? Pues porque a veces es recomendable hacer funciones pequeñas para uso interno que, aunque su funcion no es de gran interés para el público, ayuden a otras funciones más complejas que sí tienen sentido que use el público. Sea como sea, técnicamente sí se puede acceder a este tipo de funciones usando el operador :::.\nSi quieres hacer la prueba, ve a la consola de R y compara las sugerencias que salen cuando escribes usethis:: y usethis:::. Todas esas funciones que salen en el segundo caso no están disponibles cuando cargamos el paquete usando library() porque las personas que han creado el paquete usethis han considerado que no las necesitamos, aunque sí las necesitan las funciones que sí usamos.\nEn resumen, cuando ejecutemos document(), se incluirán en el NAMESPACE las funciones que hayamos indicado en la documentación mediante #' @export. Nunca modificaremos NAMESPACE archivo a mano, sino que cambiaremos sus contenidos modificando la documentación de nuestras funciones en el script de R y luego ejecutaremos document(). Con toda probabilidad, la mayoría de los problemas que vas a encontrar al desarrollar un paquete de R (también los más frustrantes) se deban a sus dependencias. Con el tiempo aprenderás a detectar estos problemas y entender por qué ocurren. Te damos un abrazo por adelantado: been there, done that.\nHay dos dependencias un poco especiales: si usamos pipes, “pipas”–o como quiera Dios que se traduzca al español–en nuestras funciones, en lugar de indicar # @importFrom dplyr %&gt;% en cada función, podremos ejecutar use_pipe() y esta función se encargará de incluir esta dependencia en la documentación por nosotros. Lo hará en un script llamado con el nombre de nuestro paquete, en la carpeta R/, que creará automáticamente. Si alguna de nuestras funciones devuelve un objeto en forma de data.frame y queremos usar la función tibble, del paquete tibble para que quede mejor, también podemos usar la función use_tibble(), que hará lo mismo que use_pipe(), aladiendo tibble a la lista de dependencias.\nPor último, es recomendable mantener las dependencias de paquetes al mínimo. Cada vez que incluimos una dependencia corremos el riesgo de que alguno de los paquetes de los que dependemos cambie de versión y nuestro paquete deje de funcionar porque una de las funciones de ese paquete que utilizamos ha cambiado. CRAN impone un límite de dependencias en 20."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#datos-internos-datos-externos-datos-brutos",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#datos-internos-datos-externos-datos-brutos",
    "title": "Creando un paquete de R: una guía informal",
    "section": "Datos internos, datos externos, datos brutos",
    "text": "Datos internos, datos externos, datos brutos\nMuchos paquetes incluyen objetos del tipo data.frame. Por ejemplo, el paquete {dplyr} contiene el objeto starwars. Este objeto es un data.frame con información sobre muchos personajes del universo de Star Wars. Podemos acceder a este objeto así dplyr::starwars.\n\n\n# A tibble: 6 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n2 C-3PO           167    75 &lt;NA&gt;    gold    yellow    112   none  mascu… Tatooi…\n3 R2-D2            96    32 &lt;NA&gt;    white,… red        33   none  mascu… Naboo  \n4 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n5 Leia Organa     150    49 brown   light   brown      19   fema… femin… Aldera…\n6 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n# … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n#   starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\nEste objeto está documentado: si ejecutamos ?dplyr::starwars se abrirá el panel de documentación de RStudio. Este tipo de objetos son muy útiles para mostrar ejemplos de uso de las funciones de un paquete. Algunos paquetes incluso han sido diseñados con el único propósito de compartir una base de datos documentada, como por ejemplo el paquete {palmerpenguins}, que apenas contiene el objeto penguins: una base de datos sobre la anatomía demlos pingüinos de la Isla de Palmer. En esta sección veremos cómo incluir una base de datos como ésta en nuestro paqeute de R y cómo documentarla.\n\nDatos internos y externos\nPodemos incluir un data.frame por defecto en nuestro paquete de dos formas: como un objeto interno o como un objeto externo. Hacerlo de la primera forma es el equivalente a crear una función sin exportarla al NAMESPACE: nos sirve para utilizarla dentro de las funciones internas del paquete, pero no será inmediatamente visible para quien cargue el paquete usando library (aunque igualmente podrá hacerlo usando el operador :::). Cuando incluimos el data.frame como objeto externo, sí se podrá acceder a él cuando cargemos el paquete (o mediante el operador ::).\nPara guardar un data.frame de cualquiera de las dos formas, primero debemos tenerlo definido entre las variables de nuestra sesión de R. Por ejemplo, supón que hemos definido el data.frame clima (disponible en el paquete psicotuiteR):\n\n\n# A tibble: 6 × 37\n     id lugar  genero   psico1 psico2 psico3 tw1   tw2   tw_pers tw_di…¹ tw_an…²\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1     1 Europa Mujer    Posgr…      3      4 Mas … Vari…       4       3       0\n2     2 Europa Hombre   Posgr…      1      5 Mas … Vari…       4       2       3\n3     3 Europa No bina… Posgr…      4      3 Meno… Vari…       4       2       1\n4     4 Europa Hombre   Posgr…      4      3 Mas … Vari…       5       5       0\n5     5 Europa Hombre   Posgr…      5      1 Mas … Vari…       4       2       3\n6     6 Europa Hombre   Profe…      5      3 Mas … Vari…       1       4       0\n# … with 26 more variables: tw_tuits &lt;int&gt;, tw_hilos_div &lt;int&gt;,\n#   tw_hilos_ot &lt;int&gt;, tw_rt &lt;int&gt;, tw_like &lt;int&gt;, tw_resp &lt;int&gt;,\n#   exp_tw1 &lt;int&gt;, exp_tw2 &lt;int&gt;, exp_tw3 &lt;int&gt;, exp_tw4 &lt;int&gt;, comp1 &lt;chr&gt;,\n#   comp2_verg &lt;int&gt;, comp2_quediran &lt;int&gt;, comp2_calidad &lt;int&gt;,\n#   comp2_error &lt;int&gt;, comp2_conf &lt;int&gt;, comp2_nunca &lt;int&gt;, comp3_divulg &lt;int&gt;,\n#   comp3_apren &lt;int&gt;, comp3_deb_cien &lt;int&gt;, comp3_deb_poli &lt;int&gt;,\n#   comp3_pers &lt;int&gt;, comp3_nunca &lt;int&gt;, psico_tw1 &lt;chr&gt;, psico_tw2 &lt;int&gt;, …\n\n\nUtilizaremos la función use_data() del paquete usethis para incluirlo en nuestro paquete. Si queremos exportarlo al NAMESPACE especificaremos internal = FALSE en los argumentos de la función (opción por defecto). Si no queremos exportarlo especificaremos internal = TRUE:\nuse_data(clima) # objeto externo\nuse_data(clima, internal = TRUE) # objeto interno\nEsta función creará (en caso de que no exita ya) una carpeta en nuestro directorio llamada data/ y guardará el data.frame como un archivo .rds (como un objeto de R), en nuestro caso lo guardará con el nombre clima.rds. En el caso de que el objeto ya exista, debemos incluir overwrite = TRUE en los argumentos para que nos permita sobreescribirlo.\nuse_data() también creará un archivo llamado clima.R en la carpeta R. Como la extensión indica, este archivo es un script de R como el que usamos para las funciones. Debemos documentar nuestros datos en este archivo usando Roxigen, tal y como hacemos para las funciones. Echa un vistazo al archivo clima.R del paquete psicotuiteR. Verás que sólo contiene documentación, excepto por la línea final, que contiene \"clima\". No necesita nada más que el nombre del objeto bajo el cual exportaremos el data.frame. De lo demás se encargará, como siempre la función document().\n\n\nDatos brutos\nHay una forma más de incluir datos, aun más reproducible que la anterior: incluir los datos brutos, como un archivo .csv, .txt, .tsv, .xlsx, etc. También podemos incluir en nuestro paquete el script de R con el código de que hemos usando para procesar los datos contenidos en el archivo y para llegar a la forma final del objeto que guaradamos mediante use_data(). La función use_raw_data() del paquete usethis se encarga de esto.\nuse_raw_data()\nEsta función creará dos carpetas (en caso de que no existan ya): inst/ y data-raw/. La carpeta inst/ (abreviatura de installation) de un paquete de R incluye archivos externos (por ejemplo, .txt, scripts de Python, Stan, C++…) que queremos que estén disponibles cuando alguien cargue el paquete, pero no son archivos que normalmente se incluyan en un paquete. Es recomendable guardar el archivo con nuestros datos brutos en la carpeta inst/. En la carpeta data-raw/ se creará un archivo con la extensión .R. En este archivo (que se abrirá automáticamente cuando ejecutemos use_data_raw()) escribiremos el código que procesa los datos y los deja como queremos que se guarden en el paquete. La última línea del script (incluida por defecto) guarda el objeto resultante como datos externos mediante la función use_data(). Así, cada vez que queramos actualizar el objeto, sólo tendremos que ejecutar este código.\n\n\nArchivos externos e inst/\nComo hemos mencionado, cualquier archivo que queramos incluir en nuestro paquete y no tenga la extensión .R o .rds, debería estar dentro de la carpeta inst/. Cuando alguien instale el paquete, los archivos de esta carpeta se moverán al directorio principal del paquete. Puedes hacer la prueba ejecutando la función install() de devtools. Cuando lo hayas hecho ve a la carpeta del paquete (recuerda que los paquetes se instalan en la primera ruta que indique .libPaths()). Cualquier archivo que hayas dejado en la carpeta inst/ aparecerá en el directorio principal ahora. A veces querremos acceder a estos archivos dentro de nuestras funciones. Esto puede ser un poco complicado. La práctica más recomendable es hacerlo usando la función:\nsystem.file(\"clima.csv\", package = \"psicotuiteR\")`\nEn la línea de código anterior, estaremos accediendo al archivo clima.csv, que hemos incluido en la carpeta inst/, pero que al instalar el paquete se encontrará en ~/Documents/R/win-library/4.0 (al menos en mi ordenador).\n\n\n\n\n\n\nImportant\n\n\n\nSi la función no encuentra ese archivo devolverá \"\", en lugar de un error. Esto puede llevar a confusión. Si quieres que la función devuelva un error si no encuentra el archivo, añade mustWork = TRUE a los argumentos:\nsystem.file(\"clima.csv\", package = \"psicotuiteR\", mustWork = TRUE)`"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#manteniendo-y-compartiendo-el-paquete",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#manteniendo-y-compartiendo-el-paquete",
    "title": "Creando un paquete de R: una guía informal",
    "section": "Manteniendo y compartiendo el paquete",
    "text": "Manteniendo y compartiendo el paquete\n\nGit y GitHub\nUn paquete de R puede volverse complejo en poco tiempo: scripts con muchas funciones, funciones que dependen de otras funciones, funciones dependen de funciones de otros paquetes… Es fácil liarla. Git es una buena herramienta para controlar cómo va cambiando el paquete. Te permite llevar la cuenta de cómo ha cambiado cada archivo dentro del paquete, poder volver a una versión específica del mismo archivo, o tener diferentes versiones del mismo paquete funcionando a la vez de forma independiente. Este último punto es especialmente útil si queremos “jugar” con una versión de prueba del paquete mientras otras personas se pueden descargar una versión estable del mismo. Todas estas utilidades se conocen como control de versiones, una versión sofisticada y menos cortoplacista de crear un millón de archivos de similar contenido y nombres incrementalmente más creativos con el fin de no perder contenido.\nPor otro lado, GitHub es una red social que permite almacenar y compartir repositorios mediante control de versiones mediante Git9. Varias personas pueden acceder al repositorio cuando está alojado en GitHub (un paquete de R, en nuestro caso) y sugerir cambios como si estuvieran trabajando sobre dicho paquete en un sólo ordenador. Por desgracia, el uso de git o GitHub está fuera del alcance de este tutorial por varios motivos10. El mejor manual que conozco (y también el más accesible) para aprender a usar Git y GitHub (especialmente para quien ya trabaja en R) es Happy Git and GitHub for the useR, de Jenny Bryan. Aprender Git no siempre es fácil pero siempre merece la pena11.\n9 No es la única plataforma disponible para hacer esto: Gitlab y Bitbucket, entre otras, hacen lo mismo, aunque son menos populares. Además, por si esto calma alguna conciencia inquita, no son propiedad de Microsoft, al contrario de GitHub.10 El primero de todos siendo que no tengo la confianza suficiente como para hacerlo (yo mismo la lío con Git cada día). El segundo es que aunque stuviera esa confianza, me daría infinita pereza hacerlo. Hacedme caso y echad un vistazo el libro que recomiendo.11 En mi honesta, humilde, ignorable opinión.12 Más que un paquete de R, devtools es una collección de funciones de otros paquetes que han sido agrupadas por su utilidad a la hora de desarrollar paquetes (“devtools” es la abreviatura de developer tools). La función install_github pertenece, originalmente, al paquete {remotes}Git y GitHub cumplen una función muy especial para quienes hacemos un paquete en R: la función install_github, del paquete {devtools}12, permite instalar un paquete sin necesidad de que esté publicado en CRAN. Hablaremos en otro tutorial sobre CRAN, pero por ahora nos interesa saber que podemos compartir cualquier paquete a través de GitHub usando install_github, pero para poder instalarlo usando install.packages, como normalmente hacemos, ese paquete necesita estar publicado en CRAN. Publicar en CRAN requiere un proceso de revisión que en ocasiones es díficil solventar (y a veces innecesario). Para compartir nuestro paquete sin necesidad de pasar por ese calvario, lo haremos a través de GitHub.\nPara hacerlo, primero debemos crear un usuario de GitHub, crear un nuevo repositorio, hacer click en el botón verde que dice Code y finalmente copiar el enlace que aparece.\nEn nuestra sesión de R, ejecutamos las siguientes líneas de código:\nuse_git() # esta línea inicializa Git en el repositorio\nuse_git_remote(name = \"origin\", url = \"https://github.com/gongcastro/psicotuiteR.git\") # sustituye ese link por el que hayas copiado de GitHub\nuse_github_ignore() # crea un archivo llamado .gitignore que indica a Git qué archivos ignorar\ngit_vaccinate() # añade más cosas a .gitignore para evitar subir información sensible a GitHub\n\n\nPoniendo a prueba el paquete con {testthat} y test()\nLos cambios que introducimos en nuestras funciones de R pueden provocar que fallos que a veces no detectamos inmediatamente. Algunos fallos no producen un error, sino que hacen que nuestras funciones se comporten de forma diferente a la que esperamos. Por ejemplo, un data.frame que devuelve nuestra función podría contener una variable con una clase character en lugar de logical. Este comportamiento indeseado podría pasar desapercibido cuando probemos las funciones que hemos cambiado. Para detectar estos problemas debemos poner a prueba todo el código cada vez que hacemos cambios. Hacer esto de forma manual cada vez puede ser muy tedioso. El paquete {testthat} se encarga de hacer esto por nosotros.\nSi ejecutamos use_testhat, se creará una carpeta llamada tests en nuestro repositorio principal. Dentro de esta carpeta, hay otra carpeta llamada testthat. Cualquier script de R que guardemos en esa carpeta se ejecutará automáticamente cuando ejecutemos test() en nuestra consola. Estos scripts deberían seguir tener el siguiente contenido:\ntest_that(\"Los datos de clima se cargan correctamente\", {expect_error(data(\"clima\"), NA)})\nEn esta línea de código estamos creando un test mediante la función test_that (del paquete {testhat}). Primero incluimos un mensaje que indique qué estamos “testeando” en específico (en este caso, que podemos cargar el dataset clima sin error). La función expect_error ejecuta el código de dentro, y evalúa si el resultado se corresponde con lo que indiquemos en el segundo argumento (en nuestro caso NA, que significa que no hay fallo). Para ver con más detalle cómo poner a prueba el paquete que has creado y aprender buenas prácticas en este tema puedes ver la documentación del paquete {testhat}."
  },
  {
    "objectID": "blog/logistic-regression/logistic-regression.html",
    "href": "blog/logistic-regression/logistic-regression.html",
    "title": "Getting the most out of logistic regression",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(scales)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(ggdist)\nlibrary(palmerpenguins)\n\ntheme_set(\n    theme_minimal() +\n        theme(\n            axis.line = element_line(colour = \"black\", size = 0.65),\n            panel.grid = element_blank()\n        ))\n\nclrs &lt;- c(\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\")"
  },
  {
    "objectID": "blog/logistic-regression/logistic-regression.html#marginal-effects",
    "href": "blog/logistic-regression/logistic-regression.html#marginal-effects",
    "title": "Getting the most out of logistic regression",
    "section": "Marginal effects",
    "text": "Marginal effects\nDefining marginal effect is tricky. As it happens with many concepts and labels in statistics, the same label may be used to refer to different concepts, and several labels may be used interchangeably to refer to the same concept. Each subfield of science seems to use a somewhat intrinsic lexicon, which sometimes leads to confusion. I will adopt the terminology in the documentation of the {marginaleffects} R package (Arel-Bundock, 2022), in which a marginal effect is defined in the context of regression as:\n\nArel-Bundock, V. (2022). Marginaleffects: Marginal effects, marginal means, predictions, and contrasts. https://CRAN.R-project.org/package=marginaleffects\n\n\n\n\nThe {marginaleffects} R package\n\n\n\n\n[…] the association between a change in a regressor \\(x\\) and a change in the response \\(y\\). Put differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor \\(x\\).\n\nAccording to this definition, calculating the marginal effect of our predictor of interest flipper_length_mm means extracting its slope for a specific value of the predictor. For linear regression models, this is trivial: since the relationship between the predictor and the response variable is assumed linear, the slope is considered constant across the whole range of the values of the predictor, and therefore its marginal effect is identical for all of them.\nWe can prove this by looking at the estimates of our (linear) model in the logit scale. Let’s say that we are interested in finding out the slope of flipper_length_mm for its average, 190.1027397. A slope is just a difference. And a difference is a derivative. And the linear regression function, \\(y = \\beta_0 + \\beta_1 x\\), is a function that can be derived (see Equation 10).\n\\[\n\\begin{aligned}\ny &= \\beta_0 + \\beta_1 \\times \\text{Flipper length} \\\\\ny' &= \\beta_1 & \\text{First derivative}\n\\end{aligned}\n\\tag{10}\\]\nThe derivative of the linear regression equation is a constant. This constant corresponds to the regression coefficient of flipper_length_mm, which means that the difference in probability of being a male penguin is going to be same for two penguins whose flippers are 180 mm and 185 mm, respectively, and for two penguins whose flippers are 190 and 195 mm, respectively.\nTake a look at Figure 6. The difference in chances of being male between each pair of penguins, in the logit scale, is the same: 0.64, which corresponds to five times the flipper_length_mm regression coefficient because in both cases the difference in flipper length is not 1 mm but 5 mm (\\(5 \\times 0.1271 = 0.635\\)). But, again, any value in the logit scale is difficult to interpret by itself, so we are interested in translating this to the scale of probabilities.\n\n\nCode\n# predictions of model with unstandardised age\npoint_preds &lt;- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %&gt;% \n    mutate(sex_pred = predict(fit, ., type = \"link\"))\n\nggplot(my_data, aes(flipper_length_mm, sex_pred_logit)) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred_logit),\n              size = 1,\n              colour = clrs[1]) +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\",\n             label = paste0(round(point_preds[2,2]-point_preds[1,2], 3), \" increment\"),\n             x = point_preds[2,1],\n             y = mean(c(point_preds[1,2], point_preds[2,2])),\n             hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[3, 1], \n             y = point_preds[3, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\", \n             label = paste0(percent(point_preds[4, 2]-point_preds[3, 2]), \" increment\"),\n             x = point_preds[4, 1], \n             y = mean(c(point_preds[4, 2], point_preds[3, 2])),\n             hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[3, 1], \n             y = point_preds[3, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[1, 1], \n             y = point_preds[1, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male) [logit scale]\",\n         title = \"&lt;span style = 'color:#003f5c;'&gt;Slope (logit scale)&lt;/span&gt;\") +\n    guides(linetype = \"none\") +\n    \n    ggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    geom_point(colour = NA) +\n    geom_hline(yintercept = coef(fit)[\"flipper_length_mm\"],\n               size = 1, colour = \"#ffa600\") +\n    annotate(geom = \"label\", \n             label = paste0(\"Constant = \", round(coef(fit)[\"flipper_length_mm\"], 2)),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = round(coef(fit)[\"flipper_length_mm\"], 2),\n             vjust = -1.25) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length (logit scale)\",\n         title = \"&lt;span style = 'color:#ffa600;'&gt;Derivative (logit scale)&lt;/span&gt;\") +\n    scale_y_continuous(limits = c(0.05, 0.2)) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\n\n\n\n\n\nFigure 6: The derivative of a linear regression equation is a constant.\n\n\n\n\n\nWe have seen that regression coefficients do not behave identically for different values of their predictors when transformed into probabilities. The exact point of flipper_length_mm at which we calculate its marginal effect matters. This is because the derivative of the logistic function, which describes the behaviour of the probability scale we just moved to, is no longer a constant. See Equation 11.\n\\[\n\\begin{aligned}\ny &= \\frac{1}{1 + e^{(-\\beta x)}} & \\text{Logistic function} \\\\\ny' &= \\frac{\\beta_1 · e^{\\beta_0 + \\beta_1 · \\ \\text{Flipper length}} }{(1 + e^{-(\\beta_0 + \\beta_1 · \\ \\text{Flipper length})})^2} & \\text{Derivative}\n\\end{aligned}\n\\tag{11}\\]\nThe derivative of the logistic function still takes into account the value of the predictor (\\(\\text{Flipper length}\\)), which means that the value of the derivative changes depending on such value. Let’s try to visualise this. First, we are going to implement the derivative of the logistic function as an R function that computes it for the fit model:\n\nlogistic_derivative &lt;- function(object, x, value) {\n    slope &lt;- coef(object)[x]\n    intercept &lt;- coef(object)[\"(Intercept)\"]\n    numerator &lt;- slope * exp(-(intercept + (slope * value)))\n    denominator &lt;- (1 + exp(-(intercept + (slope * value))))^2\n    y &lt;- numerator/denominator\n    names(y) &lt;- NULL\n    return(y)\n}\n\nFigure 7 shows how the derivative changes for each value of the predictor. As you can see, the difference in probability of being male is largest at around 190 mm, while such difference decreases as flipper_length_mm shifts away from 190 mm.\n\n\nCode\n# model predictions (on the probability scale by default)\nmy_data &lt;- mutate(\n    my_data,\n    derivative = logistic_derivative(\n        fit, \n        \"flipper_length_mm\",\n        flipper_length_mm\n    )\n)\n\npoint_preds &lt;- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %&gt;% \n    mutate(sex_pred = plogis(predict(fit, .)),\n           derivate = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\", \n             label = paste0(percent(point_preds[2,2]-point_preds[1,2]), \" increment\"),\n             x = point_preds[2,1],\n             y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[1, 1], \n             y = point_preds[1, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\",\n             label = paste0(percent(point_preds[4,2]-point_preds[3,2]), \" increment\"),\n             x = point_preds[4,1],\n             y = mean(c(point_preds[4,2], point_preds[3,2])), \n             hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[3, 1], \n             y = point_preds[3, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male)\",\n         title = \"&lt;span style = 'color:#003f5c;'&gt;Logistic curve&lt;/span&gt;,\n        &lt;span style = 'color:#ff6361;'&gt;intercept&lt;/span&gt;\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    \n    ggplot(my_data, aes(flipper_length_mm, \n                        logistic_derivative(fit, \"flipper_length_mm\", value = flipper_length_mm))) +\n    geom_point(colour = NA) +\n    geom_line(size = 1, colour = \"#ffa600\") +\n    geom_segment(aes(x = flipper_length_mm[which.max(derivative)],\n                     xend = flipper_length_mm[which.max(derivative)],\n                     y = min(derivative), yend = max(derivative)),\n                 colour = \"grey\", linetype = \"dotted\") +\n    annotate(geom = \"label\", label = paste0(\"Max = \", percent(max(my_data$derivative))),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = max(my_data$derivative)*0.5) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length  (probability scale)\",\n         title = \"&lt;span style = 'color:#ffa600;'&gt;Derivative&lt;/span&gt;\") +\n    scale_y_continuous(labels = percent) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\n\n\n\n\n\nFigure 7: The derivative of a non-linear regression equation is not constant.\n\n\n\n\n\nThe maximum change in the probability of being male is 3%, which occurs at around 190 mm flipper length. There is a smarter way of calculating the maximum slope of flipper_length_mm. This value will always occur at the mid-point of the logistic curve, and it turns out that to find the derivative of the logistic function at the mid-point (i.e., for \\(x = x_0\\)), we only need to find \\(\\beta /4\\), where \\(\\beta\\) is the regression coefficient of our predictor of interest! This is called the divide-by-four-rule, and is a simple trick to report the coefficients of a logistic regression model in the scale of probabilities, and for meaningful values of the predictors (the value at which the slope is maximum). This way, we could just divide the regression coefficient of flipper_length_mm by four to get the maximum probability difference of being male between two penguins with flipper lengths \\(x\\) and \\(x + 1\\).\n\n\n\n\n\n\nThe divide-by-four rule\n\n\n\nDividing the coefficients of a logistic regression model (in the logit scale) gets us the maximum slope of the predictor in the probability scale. We mentioned that this has to do with the derivative of the logistic function at the mid-point.\nBut since we dropped that term some equations ago after setting it at zero, it is no longer clear how one would derive the logistic function in such way that the divide-by-four rule holds. Let’s go back to Equation 2, when the mid-point still appeared in our equation. We derive this formula:\n\\[\n\\begin{aligned}\n\\text{Logistic(x)} &= \\frac{1}{1 + e^{(-\\beta \\ · \\ (x-x_0))}} & \\text{Logistic function}\\\\\n\\text{Logistic'(x)} &= \\frac{\\beta · e^{-\\beta (x-x_0)}}{(1 + e^{-\\beta (x-x_0)})^2} & \\text{Derivative}\n\\end{aligned}\n\\]\nAnd since \\(x = x_0\\), \\(x-x_0 = 0\\). We can simplify the derivative now, knowing that \\(e^0 = 1\\).\n\\[\n\\text{Logistic'(x)} = \\frac{\\beta · e^0}{(1 + e^0)^2} = \\frac{\\beta}{(1 + 1)^2} = \\frac{\\beta}{4}\n\\]\nTake a look at the previously cited blog post by TJ Mahr for a derivation that also includes the asymptote term in the logistic function.\n\n\nLet’s put the divide-by-four rule to test. The output of logistic_derivative() that we defined before should, when solved for the mid-point of the logistic curve, return an equivalent value to \\(\\beta_1 / 4\\), where \\(\\beta_1 = 0.1271495\\), and therefore return something close to 0.0317874. We don’t know what value of flipper_length_mm corresponds to the mid-point. In Figure 7 we calculated it my finding the value of flipper_length_mm for which logistic_derivative() returned the maximum value:\n\nderivative_values &lt;- logistic_derivative(fit, \"flipper_length_mm\", my_data$flipper_length_mm)\nmy_data$flipper_length_mm[which.max(derivative_values)]\n\n[1] 190\n\n\nFrom our data, we find that the maximum slope of flipper_length_mm occurs at 190. But finding the mid-point this way requires us to have already calculated the derivative of the logistic function. There is an alternative way to compute this mid-point from the estimated coefficients of the regression model. An additional benefit of computing the mid-point this way is that we are doing so by relying on model-projections, and therefore in a way that does not entirely rely on the range of values of the predictor for which we have computed the derivative of the logistic function. This method consists of Equation 12.\n\\[\nx_0 = -\\beta_0 / \\beta_1\n\\tag{12}\\]\nWhere \\(\\beta\\) is the intercept of the regression model, and \\(\\beta_1\\) is the regression coefficient of the predictor we are calculating the mid-point for. We can implement this formula in R as:\n\n# get point in x at the inflection point (where y = 0.5)\nget_mid &lt;- function(x) {\n    coefs &lt;- coef(x)[-1]\n    mid &lt;- coef(x)[1]/-coefs\n    return(mid)\n}\n\nget_mid(fit)\n\n(Intercept) \n   190.0882 \n\n\nUsing this function, we find that the mid-point is located at flipper length 190.0882 mm, pretty close to what we had estimated from our data. This value can sometimes be extremely interesting!\n\n\n\n\n\n\nA personal experience\n\n\n\nIn my PhD, I investigated how the age at which toddlers and children learn particular words is affected by participant-level (e.g., age, amount of linguistic exposure) and word-level characteristics (e.g., word length, lexical frequency). I used logistic regression to model the probability of a given toddler having learnt a word, adjusting for my predictors of interest, the most important of them being the age of the child. Older children are more likely to have learnt a given word than younger children.\nMy model returned, among others, a coefficient for age in the logit scale, but I wasn’t particularly interested in it, even after having transformed it to the probability scale. I was, however, more interested in finding the value of age at which most children were learning each word, which corresponded to the mid-point of the logistic curve for the age predictor!\n\n\nNow that we have calculated the mid-point of our logistic function, we can finally compare the divide-by-four rule against the actual value of the derivative of the logistic function. If we solve Equation 11 for \\(x_0 = 190.0882\\) using logistic_derivative(fit, \"flipper_length_mm\", 190.0882), we get that the mid-point is located at 0.0317874. If we use the divide-by-four rule, we get 0.0317874. Exactly the same!\nSo far, we have seen that the divide-by-four rule is a simple way to obtain the slope at the mid-point, which is a meaningful value: it tells us the upper limit of the distribution of the regression slope of the predictor. However, there might be other values of the predictor for which we might be interested in finding the slope of the coefficient. We might even want to compute the average of all slopes! These, an others, are different perspectives to adopt when thinking, computing, and reporting marginal effects.\nI cannot say anything that has not already been explained better by Andrew Heiss in his blog post: Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are (Heiss, 2022). I will only mention that to compute the marginal effects of your model, regardless of your strategy towards reporting marginal effects (maximum slope, average marginal effects, marginal effects at specific points), or the characteristics of your model (Gaussian vs. binomial, logit vs. probit, Bayesian vs. frequentist, etc.), the marginaleffects R package will be useful. Take a look at its documentation and functions, and play with them.\n\nHeiss, A. (2022, May 20). Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are  andrew heiss. Andrew heiss. https://www.andrewheiss.com/blog/2022/05/20/marginalia/"
  },
  {
    "objectID": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "href": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "title": "A primer on mixed-effects Models: theory and practice",
    "section": "",
    "text": "When I started my PhD I had to get familiar with Linear Mixed Models very well very quickly. Then I was asked to present what I learnt and prepare this informal tutorial for my colleagues, which I presented on March 10th, 2020 (yes, early pandemic :confounded:). This post shares the result.\nThis is not supposed to be taken as a formal guide to linear mixed-effects models, but rather as a semi-coherent compilation of notes and self-suggestions that I considered worth sharing with my lab mates during my early stages of in the PhD. Here are the slides:\nWhat I should be taken more seriously are (1) the references I suggest in the first slides (which I consider some of the best resources available to learn linear mixed-effects models), and (2) the memes, which I personally curated and even created to sweeten up the dreadful incoherence of the content of some of the slides (sorry about that).\nBefore the presentation I tweeted one of the animations I generated for it.\nThis tweet got some attention (for my usual numbers) and many kind folks have asked for the R code or the GIF file of the specific animation included in the tweet, so here they are (you’ll also find them in the GitHub repository) in a perhaps more comfortable format, ready to be cloned or downloaded):"
  },
  {
    "objectID": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "href": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "title": "A primer on mixed-effects Models: theory and practice",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Madrid\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] xaringanExtra_0.7.0 quarto_1.2         \n\nloaded via a namespace (and not attached):\n [1] digest_0.6.31   later_1.3.0     fastmap_1.1.0   xfun_0.36      \n [5] magrittr_2.0.3  glue_1.6.2      stringr_1.5.0   knitr_1.41     \n [9] htmltools_0.5.4 rmarkdown_2.19  lifecycle_1.0.3 ps_1.7.2       \n[13] cli_3.6.0       processx_3.8.0  vctrs_0.5.1     renv_0.16.0    \n[17] compiler_4.3.2  rstudioapi_0.14 tools_4.3.2     evaluate_0.19  \n[21] Rcpp_1.0.9      yaml_2.3.6      rlang_1.1.2     jsonlite_1.8.4 \n[25] stringi_1.7.8"
  },
  {
    "objectID": "blog/renv-package/renv-package.html",
    "href": "blog/renv-package/renv-package.html",
    "title": "renv (o cómo usar paquetes de R sin ataques de pánico)",
    "section": "",
    "text": "A veces necesitamos instalar versiones diferentes del mismo paquete de R en proyectos diferentes. El paquete {renv} nos permite almacenar los paquetes de R de cada proyecto de forma independiente, evitando posibles conflictos entre proyectos. De paso, incrementará la reproducibilidad computacional de nuestro código.\n\n\nEl problema\nPonte en la siguiente situación: tienes entre manos un proyecto de R que necesita varios paquetes. Cada uno de estos paquetes depende, a su vez, de terceros paquetes. De hecho, dos paquetes pueden depender del mismo paquete, o incluso de versiones diferentes del mismo paquete. Si hay mala suerte, una de las versiones no será lo suficientemente reciente como para funcionar correctamente con ambos paquetes. Resultado: uno de los dos paquetes no funcionará.\n\n\n\nUsuarie de R promedio después de tirar dos horas a la basura intentando instalar los paquetes que necesita para trabajar en un proyecto de R que no tocaba desde hacía cuatro meses.\n\n\nEste problema se extiende al caso de que necesitemos versiones diferentes del mismo paquete en proyectos de R diferentes en los que estamos trabajando de forma simultánea en el mismo ordenador. Para hacerlos funcionar necesitaríamos instalar de nuevo la versión correspondiente del mismo paquete cada vez que cambiemos de proyecto.\n\n\nInstalando paquetes de R\nEn resumidas cuentas, cada vez que instalamos o actualizamos un paquete de R, lo hacemos para todos nuestros proyectos de R de forma global. Esto se debe a que por defecto R busca todos los paquete de R en la misma carpeta. Para ver dónde instala R tus paquetes puedes ejecutar el siguiente comando:\n.libPaths()\nEste comando te mostrará el directorio o directorios donde R instala sus paquetes por defecto. Si hay más de un directorio significa que, en caso de que no sea posible encontrar un paquete en el primer directorio, R lo buscará en el segundo, tercero, etc., hasta que te devuelva un error indicando que no has instalado ese paquete.\nSi accedes al primer directorio que muestra .libPaths() verás que cada paquete tiene una carpeta. Cada carpeta incluye el código de R, los datos y la documentación asociada a cada paquete (entre otras cosas). Cada vez que instalamos o actualizamos un paquete, se crea o reemplaza su carpeta correspondiente en nuestro directorio, es decir, en nuestra “biblioteca global” de paquetes de R.\n\n\n\nAsí es como tienes tu carpeta de paquetes de R. Que lo sé yo. Que te he visto. Vergüenza me daría a mí.\n\n\nHemos visto que esto no es ideal. ¿No será mejor tener una carpeta diferente para cada proyecto en la que instalamos sus paquetes de forma independente, sin afectar a los paquetes de otros proyectos? Sí. De hecho este procedimiento es estándar en otros lenguajes de programación como JavaScript1 o Julia 2. Existe un paquete de R que nos permite hacer esto: renv. Veamos cómo funciona.\n1 Echa un vistazo a este post de Nikola Đuza: Ride Down Into JavaScript Dependency Hell2 Echa un vistazo a este post de Bogumił Kamiński: My practices for managing project dependencies in Julia\n\nUsando renv\nPrimero hay que instalar renv. Como está incluido en el CRAN, podemos hacerlo usando install.packages():\ninstall.packages(\"renv\")\nAhora abrimos una sesión de R en la carpeta de nuestro proyecto. Digamos que nuestra carpeta tiene la siguiente estructura:\ndata\n |-some-data.csv\ndocs\n |-index.Rmd\n |-index.html\nR\n |-main.R\n |-functions.R\n.Rprofile\nAsí es la estructura de la mayoría de carpetas de mis proyectos de R. Tiene su razón de ser, pero eso es material para otro post. Lo importante es cómo cambiará esta estructura en unos momentos. La documentación de renv incluye los pasos para usar renv, pero explicaré los principales. Primero inicializaremos renv en nuestra consola de R:\nrenv::init()\nEsto creará una carpeta (renv) y un archivo (renv.lock) nuevos en nuestro directorio:\n.Rprofile\ndata\n    |-some-data.csv\ndocs\n    |-index.Rmd\n    |-index.html\nR\n    |-main.R\n    |-functions.R\nrenv\n    |-.gitignore\n    |-activate.R\n    |-library\n        |-...\n    |-local\n        |-...\n    |-settings.dcf\nrenv.lock\nNo necesitaremos modificar ni consultar nunca ningunos de los archivos creados, pero vamos a curiosear un poco. Al usar init(), renv ha echado un vistazo a los scripts de R de la carpeta (achivos con la extensión .R, como main.R y functions.R), y ha detectado los paquetes que necesita nuestro código para ejecutarse (puedes consultar las dependencias de tu proyecto usando renv::dependencies()). Por ejemplo, si main.R incluye library(dplyr) o dplyr::mutate(), detectará el paquete dplyr como una dependencia.\n\n\n\nrenv detectando tus dependencias.\n\n\nA continuación, renv ha instalado todas los paquetes necesarios en renv/library/. Si comparas esa carpeta con el directorio mostrado en .libPaths() (como hicimos hace un momento), verás que ambas carpetas son muy parecidas. Eso es porque ahora R buscará los paquetes que necesites en esa carpeta, y no en la “biblioteca global” de paquetes de R. Esa es la magia de renv: podrás instalar y actualizar paquetes de R de forma independiente para cada uno de tus proyectos. Para instalar nuevos paquetes deberás hacerlo usando la función renv::install(). Por ejemplo:\nrenv::install(\"tidyr\")\nEsta función es el equivalente a install.packages() en renv. esta función asumirá que el paquete que quieres se encuentra en CRAN y será allí donde lo buscará. Si el paquete que quieres instalar se encuentra alojado en otro sitio (o quieres instalar una versión experimental del mismo, en un repositorio de GitHub, por ejemplo), puedes indicar el repositorio de la siguiente forma:\nrenv::install(\"crsh/papaja\")\nSi echas un vistazo a renv.lock verás que incluye una lista de todas las dependencias de tu proyecto, en un formato un poco raro, con muchos paréntesis, y la extensión .lock. No necesitas entender este archivo, sólo que sigue un formato parecido al que usan otros leguajes de programación para hacer lo mismo. Es el equivalente al archivo package-lock.json de un proyecto de JavaScript o al archivo Manifest.toml de un proyecto de Julia. Si te fijas, verás que simplemente incluye información mínima para cada paquete: nombre, versión, origen y un código que lo identifica. Por ejemplo, el renv.lock de nuestro proyecto incluye lo siguiente:\n{\n  \"R\": {\n    \"Version\": \"4.0.4\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"dplyr\": {\n      \"Package\": \"dplyr\",\n      \"Version\": \"1.0.8\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"ef47665e64228a17609d6df877bf86f2\"\n    },\n    \"papaja\": {\n      \"Package\": \"papaja\",\n      \"Version\": \"0.1.0.9997\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"crsh\",\n      \"RemoteRepo\": \"papaja\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a231c3628ccf24359cc17f11a5bbc743e3fed920\",\n      \"Remotes\": \"tidymodels/broom\",\n      \"Hash\": \"3df0637229690f807616c46d3ff77113\"\n    },\n    \"tidyr\": {\n      \"Package\": \"tidyr\",\n      \"Version\": \"1.2.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"d8b95b7fee945d7da6888cf7eb71a49c\"\n    },\n  }\n}\nUna ventaja enorme de usar renv es que si descargas o copias y pegas esta carpeta en un ordenador diferente (en el que posiblemente tengas una colección de paquetes diferente a la del ordenador donde trabajaste con el proyecto por última vez), renv podrá consultar este archivo para instalar por tí los paquetes necesarios en sus versiones correspondientes. Esto se puede hacer usando el comando:\nrenv::restore()\nImportante: cuando instales nuevos paquetes usando renv::install(), el archivo renv.lock no se actualizará de forma automática. Para incluir los nuevos paquetes en este archivo, tendremos que usar el siguiente comando:\nrenv::snapshot()\nComo podrás imaginar, poder instalar los paquetes que necesita un proyecto en su versión adecuada resuelve uno de los problemas más frecuentes que amenazan la reproducibilidad computacional de nuestros proyectos.\n\n\n\nImaginando un mundo donde todo el mundo se preocupa lo suficiente por la reproducibilidad computacional de sus proyectos.\n\n\n\n\nConclusiones\nTe recomiendo empezar a usar renv en algún proyecto “de juguete” con el que puedas experimentar, e ir poco a poco incorporando esta rutina en tus proyectos por el bien de tu salud mental y de la de les demás. :smile:\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{garcia-castro2022,\n  author = {Garcia-Castro, Gonzalo},\n  title = {Renv (o Cómo Usar Paquetes de {R} Sin Ataques de Pánico)},\n  date = {2022-02-27},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGarcia-Castro, G. (2022, February 27). renv (o cómo usar paquetes de\nR sin ataques de pánico)."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html",
    "href": "blog/upfthesis/upfthesis.html",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "",
    "text": "So I submitted my thesis (🎉). As anticipated, the submission process was frustrating—this is a canonical event. Though, it was less of a pain thanks Quarto, which took care of the formatting. I spent a considerable amount of time preparing this template, which was a risky move, given the tight schedule I was on1, but it paid off in the end.\nIn this post, I describe how the upfthesis Quarto template works, and perhaps more importantly, how it can be adjusted to align with the dissertation format required by other universities. First, take a look at how it looks once rendered [link here, in case it does not show up]:\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#the-format",
    "href": "blog/upfthesis/upfthesis.html#the-format",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "The format",
    "text": "The format\nUPF currently requires the following sections, according to the guidelines listed by Guies BibTIC:\n\nCover: provided by the university, to be requested as a ticket (through CAU)2\nTitle page\nDedication (optional)\nAcknowledgements (optional)\nAbstract (English and Catalan)\nPreface\nTable of contents\nList of figures (optional)\nList of templates (optional)\nBody of the thesis\nBibliography\nGlossary (optional)\n\n2 Mind that they take from two to five working days to send it over.The guide also specifies the accepted formats (A4 or B5), fonts (Times New Roman, Arial, or Garamond), font size, and margins. Whenever possible, the upfthesis extension allows the user to choose between the options. Since my LaTeX skills are limited, I constrained some other options for convenience, but help is welcome making the template more flexible.\nThe main workhorse behind the upfthesis template is the Quarto books format, to which I added some tweaks adjust to the required format taking advantage of the LaTeX template offered by the secretariat. In addition, I used some LaTeX packages to define useful functions (I’ll dig into this later).\nThe default output format of the upfthesis template is a PDF (upfthesis-pdf), which is the end product required by the thesis secretatiat. It is also possible to render the dissertation in Microsoft Word format (upfthesis-docx), Open Document format (upfthesis-odt), and HTML (upfthesis-html). While the upfthesis output in .docx and .odt formats is not submission-ready yet (word in progress), I found it useful for having my supervisor adding comments and for tracking changes across versions. The HTML format might be useful for publishing the dissertation as a website (work in progress)."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#installing",
    "href": "blog/upfthesis/upfthesis.html#installing",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Installing ⬇️",
    "text": "Installing ⬇️\nThis extension requires Quarto to be 1.3.0 or higher. I also recommend an updated version of MikTeX complications during compilation.\nquarto use template gongcastro/upfthesis\nThis will install the extension and create an set up the structure of the project. Take a look at the source code in the GitHub repository . If you are used to working from RStudio, I recommend you create a new RStudio project in the same folder."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#template-structure",
    "href": "blog/upfthesis/upfthesis.html#template-structure",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Template structure 📁",
    "text": "Template structure 📁\nThe downloaded directory contains several sub-directories and files:\n\n_thesis: contains the rendered thesis dissertation in whichever formats you have specified (e.g., thesis.pdf, thesis.docx)\n_quarto.yml: this is a YAML file with the global settings of the thesis. Here you can change the title, authors, department, etc. You can change most settings as indicated in the Book Options section of the Quarto reference. Some of these settings are internally set in the _extension.yml file; feel free to change them if you know what you are doing.\nchapters/ contains the Quarto (.qmd) files with the body of the thesis. Each section embedded in a different file. The chapters included are based on the Theses: Parts and content section of the Guies BibTIC UPF website. Some of these sections might not be required. If so, you may keep them from being included in the rendered output by removing them from the _quarto.yml file (chapters:) section. For example, here I am commenting out the Glossary section, which is not required:\n\nchapters:\n  - index.qmd\n  - chapters/01-introduction.qmd\n  - chapters/02-chapter-1.qmd\n  - chapters/03-chapter-2.qmd\n  - chapters/04-discussion.qmd\n  - chapters/05-bibliography.qmd\n  #- chapters/06-glossary.qmd\n\nNote that the numeric prefix in each .qmd file is only there for convenience: the order in which the files appear in the rendered output is determined by the order in which they appear in chapters:. Also, you may place these files wherever you want in the directory (e.g., in the base directory), as long as you indicate their right paths in chapters:.\n\n\nimg/ contains some images included in the example template. I personally find it more convenient to store all figures in a folder like this). Note that the .qmd files will look for the images (and any other refered resource) in the same folder they are located. In this template repository, the .qmd files are locating in the chapters/ folder, so images are found in the parent directory using double dots ../img/image.png.\n\n\n💡 You may create as many folders as you find convenient to store files that will be used in your dissertation. The img/ folder is one of those folders you may delete or rename. Just remember to change the file paths referring to the affected files accordingly.\n\n\nindex.qmd is an empty Quarto document that must be present in the main directory. Do not move or rename it. Any content inside this document will be rendered as a separate chapter right after the Table of Contents, which is usually not what one wants.This file must also be listed in chapters: in the _quarto.yml file.\nreferences.bib contains the BibTex references of the thesis. I you are using a reference manager (I strongly recommend using Zotero) you can export you library and replace this file with you desired references. You can also introduce you references manually in BibTex format (e.g., by copy-pasting them from Google Scholar). If you have more tha one .bib document or your .bib document is named differently, you can do the appropriate changes in the _quarto.yml file. For instance, this is how you can indicate more than one .bib file:\n\nbibliography:\n  - references.bib\n  - other-references.bib\n\n.gitignore: if you are a Git users, you might find this file convenient to avoid committing unwanted documents to your Git history.\napa.csl contains the code necessary to format the citations and bibliography in APA style. You may replace this file by whichever other style you find convenient by replacing apa.csl with the corresponding file from the citation-style-language repository. Remember to change the path in _quarto.yml if necessary."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#using-the-template",
    "href": "blog/upfthesis/upfthesis.html#using-the-template",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Using the template 🚀",
    "text": "Using the template 🚀\nTo render your dissertation to a PDF, you can use the Quarto command line in your console (Command Prompt/Power Shell in Windows, Terminal in MacOS, and command line in Linux):\nquarto render\nThis will render the thesis in PDF and Word formats by default. You can control in which format the theris is rendered this way:\nquarto render --to upfthesis-pdf\nIf you don’t want to render the thesis is either document ever, you can change the default behaviour in the _quarto.yml file, by changing:\nformat:\n  upfthesis-pdf: default\n  upfthesis-docx: default\nTo:\nformat:\n  upfthesis-pdf: default"
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#tricks-and-comments",
    "href": "blog/upfthesis/upfthesis.html#tricks-and-comments",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Tricks and comments 💡",
    "text": "Tricks and comments 💡\n\nHorizontal pages: this template defines the \\blandscape and \\elandscape commands (from the lscape LaTeX package) to flip pages to landscape mode. This wway some figures or tables may take more space and be easier to read. This is especially convenient for B5 format.\nLogo in colour: To use the logo in colour, just replace the logo.png file in the main directory with the official logo in colour.\nLow-level sections: for some reason, I have not been able to add section headers lower than 5 (e.g., 6, 7). I recommend reducing the number of sub-sections. This is issue is currently open (#7) and help is appreciated."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#related-extensions",
    "href": "blog/upfthesis/upfthesis.html#related-extensions",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Related extensions",
    "text": "Related extensions\nSome folks have alreafy provided more generic templates for thesis dissertations, or have developed an extension for their specifc university. Take a look at them for inspiration for customising your own, or for learning nice tricks. Here are some of them, together with some blog posts in covering the topic.\n\nquarto-thesis by nmfs-opensci\nEnough Markdown to Write a Thesis by Richard J Telford\nWriting a dissertation in Quarto by rich Posert\nSome Quarto PDF formatting tips, with particular reference to thesis writing\nMonash Quarto Template by Rob J Hyndman"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Industry CV (PDF)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\nOctober 2018 - Present\nPhD, Biomedicine; Universitat Pompeu Fabra (Barcelona, Spain); Supervisor: Núria Sebastian-Galles.\nOctober 2017 - July 2018\n\nMSc, Neurosciences; University of Barcelona (Barcelona, Spain). Dissertation: Phonemic Contrast Perception: A Segmentation Study on Monolingual and Bilingual Infants; Supervisors: Núria Sebastian-Galles and Chiara Santolin.\n\nSeptember 2013 - July 2017\n\nBSc, Psychology; University of Oviedo (Oviedo, Spain). Dissertation: Effects of Environmental Enrichment on Attention, Spatial Reference Memory, and Cytochrome C Oxidase Activity; Supervisors: Azucena Begega and Marcelino Cuesta."
  },
  {
    "objectID": "cv.html#contributions",
    "href": "cv.html#contributions",
    "title": "CV",
    "section": "Contributions",
    "text": "Contributions\n\nArticles\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., Saffran, J. (2020). Experience with research paradigms relates to infants’ direction of preference. Infancy, 00, 1–8.  URL  OSF  PsyArxiv   DOI  GitHub\nSampedro-Piquero, P., Álvarez-Suárez, P., Moreno-Fernández, R., García-Castro, G., Cuesta, M., Begega, A. (2018). Environmental enrichment results in both brain connectivity efficiency and selective improvement in different behavioral tasks. Neuroscience, 388, 374-383.  URL   Google Scholar"
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "CV",
    "section": "Conference presentations",
    "text": "Conference presentations\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2022). A longitudinal exploration of bilingual toddler’s processing of cognates. Presented at the International Congress on Infant Studies.  URL\nGarcia-Castro, G., Franco-Martínez, A., Rodríguez-Prada, C., Castillejo, I., Sebastian-Galles, N. (2022). Estimando curvas de adquisición léxica en la infancia mediante un modelo de bayesiano de TRI. Presented at the XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.  URL"
  },
  {
    "objectID": "cv.html#proceedings",
    "href": "cv.html#proceedings",
    "title": "CV",
    "section": "Proceedings",
    "text": "Proceedings\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2022). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Proceedings of the 46th annual Boston University Conference on Language Development.  URL   DOI\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presented at the Boston University Conference in Language Development, held online (Boston, United States).\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., Plunkett, K. (2021). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Presentation at the Architectures and Mechanisms for Language Processing conference, held online (Paris, France).  URL   DOI\n\nPosters\nGarcia-Castro, G., Siow, S., Lepadatu, I., Guillen, N., Avila-Varela, D. S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The emergence of inhibitory links in the developing lexicon: insights from bilingual participants. Poster presented at the Workshop in Infant Language Development (San Sebastián/Donostia, Spain).  URL\nZacharaki, K. E., Garcia-Castro, G., Sebastian-Galles, N. (June, 2022). Selective attention to the mouth of signing faces. Poster presented at the Workshop in Infant Language Development (San Sebastián/Donostia, Spain).  URL\nZacharaki, K. E., Garcia-Castro, G., Sebastian-Galles, N. (June, 2022). Selective attention to the mouth of signing faces. Poster presented at the Workshop in Infant Language Development (San Sebastián/Donostia, Spain).  URL\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D. S., Sebastian-Galles, N., Plunkett, K. (August, 2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presentation at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The impact of cognateness on bilingual lexical access: a longitudinal priming study. Poster presented at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (June, 2021). The role of cognateness in nonnative spoken word recognition. Poster presented at the XI International Symposium of Psycholinguistics (Madrid, Spain, held online).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (July, 2020). The role of lexical similarity on bilingual parallel activation: A priming study in toddlers. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nGarcia-Castro, G., Avila-Varela, D. S., and Sebastian-Galles, N. (July, 2020). Does phonological overlap across translation equivalents predict earlier age of acquisition?. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., and Plunkett, K. (August, 2019). The impact of phonology (cognateness) on the bilingual lexicon: Parallel cross-language phonological priming. Poster presented at the Lancaster Conference on Infant and Early Child Development (Lancaster, United Kingdom).\nGarcia-Castro, G., Marimon, M., Santolin, C., and Sebastian-Galles, N. (June, 2019). Encoding new word forms when contrastive phonemes are interchanged: A preliminary study on 8-month-old infants. Poster presented at the Workshop in Infant Language Development (Potsdam, Germany). https://doi.org/10.17605/OSF.IO/GYKUH\nGarcía-Castro, G., Álvarez-Suárez, P., García-Abad, N., Cuesta, M., and Begega, A. (June, 2017). Pro-cognitive effects of environmental enrichment on attention and spatial memory: hippocampal metabolic activity in a Wistar rat model. Poster presented at the 4th International Congress on Health and Aging Research (Murcia, Spain).\nGarcía-Abad, N., Santirso, M., García-Castro, G., and Álvarez-Suárez, P. (June, 2017). Efectos del omega-3 en las redes implicadas en la memoria de trabajo espacial en ratas Wistar. Poster presented at the 3rd National Congress of Psychology (Oviedo, Spain)\n\n\nInvited talks\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., and Plunkett, K. (March, 2022). The role of cognateness during word recognition in a non-native language. Talk given at the Psycholinguistics Coffee (University of Edinburgh, held online).  URL\nGarcia-Castro, G., Siow, S., Lepadatu, I., Guillen, N., Avila-Varela, D., Sebastian-Galles, N. y Plunkett, K. (January 2022). Parallel activation and the developing bilingual lexicon: a longitudinal study on word recognition. Talk given at the LACRE research group (Cardiff University, held online).\n\n\nScientific dissemination\nZacharaki, K., García-Castro, G. (2019, October). “Descubriendo la mente de los bebés” Talk presented at 13a Festa de la Ciència, Barcelona, Spain. [Link] [Video]\nGarcía-Castro, G., Avila-Varela (2019, October). “Estudiando la mente de los bebés: Desde el lenguaje hasta la lógica” Talk presented at Centre Cívic, Barcelona, Spain. [Link] [Video])\n\n\nRepositories\nGarcía-Castro, G., Santolin, C., Marimon, M., and Sebastian-Galles, N. (2019, October 22th). Segmentation: Catalan and Spanish natural speech segmentation at 8 months of age. https://doi.org/10.17605/OSF.IO/42GUP)\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 5). Flip: Experience with research paradigms relates to infants’ direction of preference. https://doi.org/10.17605/OSF.IO/G95UB"
  },
  {
    "objectID": "cv.html#others",
    "href": "cv.html#others",
    "title": "CV",
    "section": "Others",
    "text": "Others\n\nAnimal research\nResearch Staff Certificate for animal research, issued on 15/05/2018 by the Direcció General de Polítiques Ambientals i Medi Natural.\n\n\nSkills\n\nData processing: I particularly enjoy wrangling my way through messy data using the tidyverse family of packages. with some time, I can also google my way through Python.\nData analysis: Linear mixed models using R, both frequentist (lme4) and Bayesian (brms); reproducible reports using RMarkdown. I can also perform acoustic analysis on stimuli using Praat, and one of its R interfaces (PraatR).\nData visualisation using ggplot2. I can make animations using gganimate, and I’m currently learning to make maps, and Shiny Apps.\nDesigning experiments and questionnaires: I can program lab-based experiments using Matlab (PsychToolbox-3) and Python (Psychopy), online experiments using the PsychoPy/PsychoJS/Pavlovia workflow, and design reproducible online questionnaires using formR.\nGit/GitHub/Bitbucket\nJASP/SPSS"
  },
  {
    "objectID": "cv.html#languages",
    "href": "cv.html#languages",
    "title": "CV",
    "section": "Languages",
    "text": "Languages\n\nSpanish (native speaker)\nEnglish: C1\nFrench: C1"
  },
  {
    "objectID": "cv.html#short-courses",
    "href": "cv.html#short-courses",
    "title": "CV",
    "section": "Short courses",
    "text": "Short courses\nAugust 2018\n\nData Science: Multiple imputation in practice, Utrecht Summer School, Utrecht University. Introduction to Multiple Imputation as a powerful tool for dealing with missing data in experimental studies.\n\nAugust 2018\n\nData Science: Statistical Programming with R, Utrecht Summer School, Utrecht University.\n\nAugust 2016\n\nIntroduction to Neuroscience: From Molecule to Behavior, Radboud Summer School, Radboud University Nijmegen. Hands-on training at research labs of the Donders Institute for Brain and Cognition.\n\nAugust 2016\n\nHow to Improve the Quality and Translatability of Preclinical Animal Studies, Radboud Summer School, Radboud University Nijmegen. Course organized by SYRCLE institute, Radboud University Nijmegen and Radboud University Medical Centre. Theory and hands-on experience on Systematic Reviews and Meta-Analysis in preclinical animal studies. Overview of the main software available to conduct a Meta-Analysis and basic formation on academic reading and writing."
  }
]