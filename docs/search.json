[
  {
    "objectID": "blog/logistic-regression/logistic-regression.html",
    "href": "blog/logistic-regression/logistic-regression.html",
    "title": "Getting the most out of logistic regression",
    "section": "",
    "text": "Codelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(scales)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(ggdist)\nlibrary(palmerpenguins)\n\ntheme_set(\n    theme_minimal() +\n        theme(\n            axis.line = element_line(colour = \"black\", size = 0.65),\n            panel.grid = element_blank()\n        ))\n\nclrs <- c(\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\")"
  },
  {
    "objectID": "blog/logistic-regression/logistic-regression.html#marginal-effects",
    "href": "blog/logistic-regression/logistic-regression.html#marginal-effects",
    "title": "Getting the most out of logistic regression",
    "section": "Marginal effects",
    "text": "Marginal effects\nDefining marginal effect is tricky. As it happens with many concepts and labels in statistics, the same label may be used to refer to different concepts, and several labels may be used interchangeably to refer to the same concept. Each subfield of science seems to use a somewhat intrinsic lexicon, which sometimes leads to confusion. I will adopt the terminology in the documentation of the {marginaleffects} R package (Arel-Bundock, 2022), in which a marginal effect is defined in the context of regression as:\n\nArel-Bundock, V. (2022). Marginaleffects: Marginal effects, marginal means, predictions, and contrasts. https://CRAN.R-project.org/package=marginaleffects\n\n[…] the association between a change in a regressor \\(x\\) and a change in the response \\(y\\). Put differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor \\(x\\).\n\nAccording to this definition, calculating the marginal effect of our predictor of interest flipper_length_mm means extracting its slope for a specific value of the predictor. For linear regression models, this is trivial: since the relationship between the predictor and the response variable is assumed linear, the slope is considered constant across the whole range of the values of the predictor, and therefore its marginal effect is identical for all of them.\nWe can prove this by looking at the estimates of our (linear) model in the logit scale. Let’s say that we are interested in finding out the slope of flipper_length_mm for its average, 190.1027397. A slope is just a difference. And a difference is a derivative. And the linear regression function, \\(y = \\beta_0 + \\beta_1 x\\), is a function that can be derived (see Equation 10).\n\\[\n\\begin{aligned}\ny &= \\beta_0 + \\beta_1 \\times \\text{Flipper length} \\\\\ny' &= \\beta_1 & \\text{First derivative}\n\\end{aligned}\n\\tag{10}\\]\nThe derivative of the linear regression equation is a constant. This constant corresponds to the regression coefficient of flipper_length_mm, which means that the difference in probability of being a male penguin is going to be same for two penguins whose flippers are 180 mm and 185 mm, respectively, and for two penguins whose flippers are 190 and 195 mm, respectively.\nTake a look at Figure 6. The difference in chances of being male between each pair of penguins, in the logit scale, is the same: 0.64, which corresponds to five times the flipper_length_mm regression coefficient because in both cases the difference in flipper length is not 1 mm but 5 mm (\\(5 \\times 0.1271 = 0.635\\)). But, again, any value in the logit scale is difficult to interpret by itself, so we are interested in translating this to the scale of probabilities.\n\nCode# predictions of model with unstandardised age\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = predict(fit, ., type = \"link\"))\n\nggplot(my_data, aes(flipper_length_mm, sex_pred_logit)) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred_logit),\n              size = 1,\n              colour = clrs[1]) +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(round(point_preds[2,2]-point_preds[1,2], 3), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(round(point_preds[4,2]-point_preds[3,2], 3), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male) [logit scale]\",\n         title = \"<span style = 'color:#003f5c;'>Slope (logit scale)</span>\") +\n    guides(linetype = \"none\") +\n    \n    \n    ggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    geom_point(colour = NA) +\n    geom_hline(yintercept = coef(fit)[\"flipper_length_mm\"],\n               size = 1, colour = \"#ffa600\") +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length [logit scale])\",\n         title = \"<span style = 'color:#ffa600;'>Derivative (logit scale)</span>\") +\n    scale_y_continuous(limits = c(0.05, 0.2)) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\nFigure 6: The derivative of a linear regression equation is a constant.\n\n\n\n\nWe have seen that regression coefficients do not behave identically for different values of their predictors when transformed into probabilities. The exact point of flipper_length_mm at which we calculate its marginal effect matters. This is because the derivative of the logistic function, which describes the behaviour of the probability scale we just moved to, is no longer a constant. See Equation 11.\n\\[\n\\begin{aligned}\ny &= \\frac{1}{1 + e^{(-\\beta x)}} & \\text{Logistic function} \\\\\ny' &= \\frac{\\beta_1 · e^{\\beta_0 + \\beta_1 · \\ \\text{Flipper length}} }{(1 + e^{-(\\beta_0 + \\beta_1 · \\ \\text{Flipper length})})^2} & \\text{Derivative}\n\\end{aligned}\n\\tag{11}\\]\nThe derivative of the logistic function still takes into account the value of the predictor (\\(\\text{Flipper length}\\)), which means that the value of the derivative changes depending on such value. Let’s try to visualise this. First, we are going to implement the derivative of the logistic function as an R function that computes it for the fit model:\n\nlogistic_derivative <- function(object, x, value) {\n    slope <- coef(object)[x]\n    intercept <- coef(object)[\"(Intercept)\"]\n    numerator <- slope * exp(-(intercept + (slope * value)))\n    denominator <- (1 + exp(-(intercept + (slope * value))))^2\n    y <- numerator/denominator\n    names(y) <- NULL\n    return(y)\n}\n\nFigure 7 shows how the derivative changes for each value of the predictor. As you can see, the difference in probability of being male is largest at around 190 mm, while such difference decreases as flipper_length_mm shifts away from 190 mm.\n\nCode# model predictions (on the probability scale by default)\nmy_data <- mutate(my_data,\n                  derivative = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = plogis(predict(fit, .)),\n           derivate = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[2,2]-point_preds[1,2]), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[4,2]-point_preds[3,2]), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male)\",\n         title = \"<span style = 'color:#003f5c;'>Logistic curve</span>,\n        <span style = 'color:#ff6361;'>intercept</span>\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    \n    ggplot(my_data, aes(flipper_length_mm, \n                        logistic_derivative(fit, \"flipper_length_mm\", value = flipper_length_mm))) +\n    geom_point(colour = NA) +\n    geom_line(size = 1, colour = \"#ffa600\") +\n    geom_segment(aes(x = flipper_length_mm[which.max(derivative)],\n                     xend = flipper_length_mm[which.max(derivative)],\n                     y = min(derivative), yend = max(derivative)),\n                 colour = \"grey\", linetype = \"dotted\") +\n    annotate(geom = \"label\", label = paste0(\"Max = \", percent(max(my_data$derivative))),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = max(my_data$derivative)*0.5) +\n    \n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length [logit scale])\",\n         title = \"<span style = 'color:#ffa600;'>Derivative</span>\") +\n    scale_y_continuous(labels = percent) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\nFigure 7: The derivative of a non-linear regression equation is not constant.\n\n\n\n\nThe maximum change in the probability of being male is 3%, which occurs at around 190 mm flipper length. There is a smarter way of calculating the maximum slope of flipper_length_mm. This value will always occur at the mid-point of the logistic curve, and it turns out that to find the derivative of the logistic function at the mid-point (i.e., for \\(x = x_0\\), go back to ?@eq-logistic for a reminder), we only need to find \\(\\beta /4\\), where \\(\\beta\\) is the regression coefficient of our predictor of interest! This is called the divide-by-four-rule, and is a simple trick to report the coefficients of a logistic regression model in the scale of probabilities, and for meaningful values of the predictors (the value at which the slope is maximum). This way, we could just divide the regression coefficient of flipper_length_mm by four to get the maximum probability difference of being male between two penguins with flipper lengths \\(x\\) and \\(x + 1\\).\n\n\n\n\n\n\nThe divide-by-four rule\n\n\n\nWe have seen that dividing the coefficients of a logistic regression model (in the logit scale) gets us the maximum slope of the predictor in the probability scale. We mentioned that this has to do with the derivative of the logistic function at the mid-point. But since we dropped that term some equations ago after setting it at zero, it is no longer clear how one would derive the logistic function in such way that the divide-by-four-rule holds. Let’s go back to Equation 2, when the mid-point still appeared in our equation. We derive this formula:\n\\[\n\\begin{aligned}\n\\text{Logistic(x)} &= \\frac{1}{1 + e^{(-\\beta \\ · \\ (x-x_0))}} & \\text{Logistic function}\\\\\n\\text{Logistic'(x)} &= \\frac{\\beta · e^{-\\beta (x-x_0)}}{(1 + e^{-\\beta (x-x_0)})^2} & \\text{Derivative}\n\\end{aligned}\n\\]\nAnd since \\(x = x_0\\), \\(x-x_0 = 0\\). We can simplify the derivative now, knowing that \\(e^0 = 1\\).\n\\[\n\\text{Logistic'(x)} = \\frac{\\beta · e^0}{(1 + e^0)^2} = \\frac{\\beta}{(1 + 1)^2} = \\frac{\\beta}{4}\n\\]\nTake a look at the previously cited blog post by TJ Mahr for a derivation that also includes the asymptote term in the logistic function.\n\n\nLet’s put the divide-by-four rule to test. The output of logistic_derivative() that we defined before should, when solved for the mid-point of the logistic curve, return an equivalent value to \\(\\beta_1 / 4\\), where \\(\\beta_1 = 0.1271495\\), and therefore return something close to 0.0317874. We don’t know what value of flipper_length_mm corresponds to the mid-point. In Figure 7 we calculated it my finding the value of flipper_length_mm for which logistic_derivative() returned the maximum value:\n\nderivative_values <- logistic_derivative(fit, \"flipper_length_mm\", my_data$flipper_length_mm)\nmy_data$flipper_length_mm[which.max(derivative_values)]\n\n[1] 190\n\n\nFrom our data, we find that the maximum slope of flipper_length_mm occurs at 190. But finding the mid-point this way requires us to have already calculated the derivative of the logistic function. There is an alternative way to compute this mid-point from the estimated coefficients of the regression model. An additional benefit of computing the mid-point this way is that we are doing so by relying on model-projections, and therefore in a way that does not entirely rely on the range of values of the predictor for which we have computed the derivative of the logistic function. This method consists of Equation 12.\n\\[\nx_0 = -\\beta_0 / \\beta_1\n\\tag{12}\\]\nWhere \\(\\beta\\) is the intercept of the regression model, and \\(\\beta_1\\) is the regression coefficient of the predictor we are calculating the mid-point for. We can implement this formula in R as:\n\n# get point in x at the inflection point (where y = 0.5)\nget_mid <- function(x) {\n    coefs <- coef(x)[-1]\n    mid <- coef(x)[1]/-coefs\n    return(mid)\n}\n\nget_mid(fit)\n\n(Intercept) \n   190.0882 \n\n\nUsing this function, we find that the mid-point is located at flipper length 190.0882 mm, pretty close to what we had estimated from our data. This value can sometimes be extremely interesting!\n\n\n\n\n\n\nA personal experience\n\n\n\nIn my PhD, I investigated how the age at which toddlers and children learn particular words is affected by participant-level (e.g., age, amount of linguistic exposure) and word-level characteristics (e.g., word length, lexical frequency). I used logistic regression to model the probability of a given toddler having learnt a word, adjusting for my predictors of interest, the most important of them being the age of the child. Older children are more likely to have learnt a given word than younger children.\nMy model returned, among others, a coefficient for age in the logit scale, but I wasn’t particularly interested in it, even after having transformed it to the probability scale. I was, however, more interested in finding the value of age at which most children were learning each word, which corresponded to the mid-point of the logistic curve for the age predictor!\n\n\nNow that we have calculated the mid-point of our logistic function, we can finally compare the divide-by-four rule against the actual value of the derivative of the logistic function. If we solve Equation 11 for \\(x_0 = 190.0882\\) using logistic_derivative(fit, \"flipper_length_mm\", 190.0882), we get that the mid-point is located at 0.0317874. If we use the divide-by-four rule, we get 0.0317874. Exactly the same!\nSo far, we have seen that the divide-by-four rule is a simple way to obtain the slope at the mid-point, which is a meaningful value: it tells us the upper limit of the distribution of the regression slope of the predictor. However, there might be other values of the predictor for which we might be interested in finding the slope of the coefficient. We might even want to compute the average of all slopes! These, an others, are different perspectives to adopt when thinking, computing, and reporting marginal effects.\nI cannot say anything that has not already been explained better by Andrew Heiss in his blog post: Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are. I will only mention that to compute the marginal effects of your model, regardless of your strategy towards reporting marginal effects (maximum slope, average marginal effects, marginal effects at specific points), or the characteristics of your model (Gaussian vs. binomial, logit vs. probit, Bayesian vs. frequentist, etc.), the marginaleffects R package will be useful. Take a look at its documentation and functions, and play with them."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Getting the most out of logistic regression\n\n\n\n\n\n\n\nr\n\n\ntidyverse\n\n\nregression\n\n\nstatistics\n\n\nlogistic\n\n\n\n\nLogistic regression models provide information way beyond a p-value. Using the {palmerpenguins} dataset, I review the relationship between the logistic and the logit functions, and how it relates to the outputs of a binomial regression model with an emphasis on marginal effects.\n\n\n\n\n\n\nJan 22, 2023\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nrenv (o cómo usar paquetes de R sin ataques de pánico)\n\n\n\n\n\n\n\nr\n\n\npackage\n\n\nreproducibility\n\n\nrenv\n\n\n\n\n{renv} es un paquete de R que permite instalar paquetes de R gestionar sus versiones para proyectos de forma independiente. Aquí resumo para qué se utiliza y cómo funciona.\n\n\n\n\n\n\nFeb 27, 2022\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\n@psicotuiterbot: Un bot de Twitter para Psicotuiter\n\n\n\n\n\n\n\nr\n\n\nrtweet\n\n\ntwitter\n\n\nraspberry pi\n\n\nbot\n\n\ngithub\n\n\n\n\nHe creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON.\n\n\n\n\n\n\nDec 29, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nCreando un paquete de R: una guía informal\n\n\n\n\n\n\n\nr\n\n\npackage\n\n\ntwitter\n\n\ntutorial\n\n\n\n\nEn el canal de Twitch de Psicometries hicimos un directo en el que explicamos cómo se ha creado el paquete de R {psicotuiteR}, indicando cada paso lo mejor que hemos podido para que puedas replicarlo, contribuir al mismo paquete o incluso crear tu propio paquete en el futuro.\n\n\n\n\n\n\nNov 14, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nExploring probability distributions through animations in Julia\n\n\n\n\n\n\n\njulia\n\n\ndistribution\n\n\nanimation\n\n\nstatistics\n\n\nprobability\n\n\n\n\nVisualising what different probability distributions look like under different parameters can be helpful when picking a likelihood function for you Bayesian analysis. I present some animations generated with Julia using Distributions.jl\n\n\n\n\n\n\nOct 4, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nVisualising polynomial regression\n\n\n\n\n\n\n\nr\n\n\nstatistic\n\n\nregression\n\n\nanimation\n\n\nggplot\n\n\n\n\nThe outputs of polynomial regression can be difficult to interpret. I generated some animated plots to see how model predictions change across different combinations of coefficients for 1st, 2nd, and 3rd degree polynomials.\n\n\n\n\n\n\nJan 21, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nHow similar is the word “mask” across languages?\n\n\n\n\n\n\n\nr\n\n\nlinguistics\n\n\nphonology\n\n\nggplot2\n\n\ntranslation\n\n\n\n\nUsing the Levenshtein distance to quantify the orthographic and phonlogical similarity between translation equivalents of the word mask across multiple languages.\n\n\n\n\n\n\nNov 20, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nImporting data from multiple files simultaneously in R\n\n\n\n\n\n\n\nr\n\n\ntidyverse\n\n\nbase\n\n\npurrr\n\n\nimport\n\n\n\n\nA comparison between base R and Tidyverse methods for importing data from multiple files\n\n\n\n\n\n\nJul 5, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n  \n\n\n\n\nA primer on mixed-effects Models: theory and practice\n\n\n\n\n\n\n\nr\n\n\nstatistics\n\n\nmultilevel\n\n\nmixed models\n\n\nanimations\n\n\n\n\nSlides from a tutorial on mixed-effects models I presented to my research group.\n\n\n\n\n\n\nMar 31, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Gonzalo García-Castro",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Click here for PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\nOctober 2018 - Present\nPhD, Biomedicine; Pompeu Fabra University (Barcelona, Spain); Supervisor: Núria Sebastian-Galles.\nOctober 2017 - July 2018\n\nMSc, Neurosciences; University of Barcelona (Barcelona, Spain). Dissertation: Phonemic Contrast Perception: A Segmentation Study on Monolingual and Bilingual Infants; Supervisors: Núria Sebastian-Galles and Chiara Santolin.\n\nSeptember 2013 - July 2017\n\nBSc, Psychology; University of Oviedo (Oviedo, Spain). Dissertation: Effects of Environmental Enrichment on Attention, Spatial Reference Memory, and Cytochrome C Oxidase Activity; Supervisors: Azucena Begega and Marcelino Cuesta."
  },
  {
    "objectID": "cv.html#contributions",
    "href": "cv.html#contributions",
    "title": "CV",
    "section": "Contributions",
    "text": "Contributions\n\nArticles\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., Saffran, J. (2020). Experience with research paradigms relates to infants’ direction of preference. Infancy, 00, 1–8.  URL  OSF  PsyArxiv   DOI  GitHub\nSampedro-Piquero, P., Álvarez-Suárez, P., Moreno-Fernández, R., García-Castro, G., Cuesta, M., Begega, A. (2018). Environmental enrichment results in both brain connectivity efficiency and selective improvement in different behavioral tasks. Neuroscience, 388, 374-383.  URL   Google Scholar"
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "CV",
    "section": "Conference presentations",
    "text": "Conference presentations\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2022). A longitudinal exploration of bilingual toddler’s processing of cognates. Presented at the International Congress on Infant Studies.  URL\nGarcia-Castro, G., Franco-Martínez, A., Rodríguez-Prada, C., Castillejo, I., Sebastian-Galles, N. (2022). Estimando curvas de adquisición léxica en la infancia mediante un modelo de bayesiano de TRI. Presented at the XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.  URL"
  },
  {
    "objectID": "cv.html#proceedings",
    "href": "cv.html#proceedings",
    "title": "CV",
    "section": "Proceedings",
    "text": "Proceedings\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2022). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Proceedings of the 46th annual Boston University Conference on Language Development.  URL   DOI\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presented at the Boston University Conference in Language Development, held online (Boston, United States).\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., Plunkett, K. (2021). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Presentation at the Architectures and Mechanisms for Language Processing conference, held online (Paris, France).  URL   DOI\n\nPosters\nGarcia-Castro, G., Siow, S., Lepadatu, I., Guillen, N., Avila-Varela, D. S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The emergence of inhibitory links in the developing lexicon: insights from bilingual participants. Poster presented at the Workshop in Infant Language Development (San Sebastián/Donostia, Spain).  URL\nZacharaki, K. E., Garcia-Castro, G., Sebastian-Galles, N. (June, 2022). Selective attention to the mouth of signing faces. Poster presented at the Workshop in Infant Language Development (San Sebastián/Donostia, Spain).  URL\nZacharaki, K. E., Garcia-Castro, G., Sebastian-Galles, N. (June, 2022). Selective attention to the mouth of signing faces. Poster presented at the Workshop in Infant Language Development (San Sebastián/Donostia, Spain).  URL\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D. S., Sebastian-Galles, N., Plunkett, K. (August, 2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presentation at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The impact of cognateness on bilingual lexical access: a longitudinal priming study. Poster presented at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (June, 2021). The role of cognateness in nonnative spoken word recognition. Poster presented at the XI International Symposium of Psycholinguistics (Madrid, Spain, held online).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (July, 2020). The role of lexical similarity on bilingual parallel activation: A priming study in toddlers. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nGarcia-Castro, G., Avila-Varela, D. S., and Sebastian-Galles, N. (July, 2020). Does phonological overlap across translation equivalents predict earlier age of acquisition?. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., and Plunkett, K. (August, 2019). The impact of phonology (cognateness) on the bilingual lexicon: Parallel cross-language phonological priming. Poster presented at the Lancaster Conference on Infant and Early Child Development (Lancaster, United Kingdom).\nGarcia-Castro, G., Marimon, M., Santolin, C., and Sebastian-Galles, N. (June, 2019). Encoding new word forms when contrastive phonemes are interchanged: A preliminary study on 8-month-old infants. Poster presented at the Workshop in Infant Language Development (Potsdam, Germany). https://doi.org/10.17605/OSF.IO/GYKUH\nGarcía-Castro, G., Álvarez-Suárez, P., García-Abad, N., Cuesta, M., and Begega, A. (June, 2017). Pro-cognitive effects of environmental enrichment on attention and spatial memory: hippocampal metabolic activity in a Wistar rat model. Poster presented at the 4th International Congress on Health and Aging Research (Murcia, Spain).\nGarcía-Abad, N., Santirso, M., García-Castro, G., and Álvarez-Suárez, P. (June, 2017). Efectos del omega-3 en las redes implicadas en la memoria de trabajo espacial en ratas Wistar. Poster presented at the 3rd National Congress of Psychology (Oviedo, Spain)\n\n\nInvited talks\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., and Plunkett, K. (March, 2022). The role of cognateness during word recognition in a non-native language. Talk given at the Psycholinguistics Coffee (University of Edinburgh, held online).  URL\nGarcia-Castro, G., Siow, S., Lepadatu, I., Guillen, N., Avila-Varela, D., Sebastian-Galles, N. y Plunkett, K. (January 2022). Parallel activation and the developing bilingual lexicon: a longitudinal study on word recognition. Talk given at the LACRE research group (Cardiff University, held online).\n\n\nScientific dissemination\nZacharaki, K., García-Castro, G. (2019, October). “Descubriendo la mente de los bebés” Talk presented at 13a Festa de la Ciència, Barcelona, Spain. [Link] [Video]\nGarcía-Castro, G., Avila-Varela (2019, October). “Estudiando la mente de los bebés: Desde el lenguaje hasta la lógica” Talk presented at Centre Cívic, Barcelona, Spain. [Link] [Video])\n\n\nRepositories\nGarcía-Castro, G., Santolin, C., Marimon, M., and Sebastian-Galles, N. (2019, October 22th). Segmentation: Catalan and Spanish natural speech segmentation at 8 months of age. https://doi.org/10.17605/OSF.IO/42GUP)\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 5). Flip: Experience with research paradigms relates to infants’ direction of preference. https://doi.org/10.17605/OSF.IO/G95UB"
  },
  {
    "objectID": "cv.html#others",
    "href": "cv.html#others",
    "title": "CV",
    "section": "Others",
    "text": "Others\n\nAnimal research\nResearch Staff Certificate for animal research, issued on 15/05/2018 by the Direcció General de Polítiques Ambientals i Medi Natural.\n\n\nSkills\n\nData processing: I particularly enjoy wrangling my way through messy data using the tidyverse family of packages. with some time, I can also google my way through Python.\nData analysis: Linear mixed models using R, both frequentist (lme4) and Bayesian (brms); reproducible reports using RMarkdown. I can also perform acoustic analysis on stimuli using Praat, and one of its R interfaces (PraatR).\nData visualisation using ggplot2. I can make animations using gganimate, and I’m currently learning to make maps, and Shiny Apps.\nDesigning experiments and questionnaires: I can program lab-based experiments using Matlab (PsychToolbox-3) and Python (Psychopy), online experiments using the PsychoPy/PsychoJS/Pavlovia workflow, and design reproducible online questionnaires using formR.\nGit/GitHub/Bitbucket\nJASP/SPSS"
  },
  {
    "objectID": "cv.html#languages",
    "href": "cv.html#languages",
    "title": "CV",
    "section": "Languages",
    "text": "Languages\n\nSpanish (native speaker)\nEnglish: C1\nFrench: C1"
  },
  {
    "objectID": "cv.html#short-courses",
    "href": "cv.html#short-courses",
    "title": "CV",
    "section": "Short courses",
    "text": "Short courses\nAugust 2018\n\nData Science: Multiple imputation in practice, Utrecht Summer School, Utrecht University. Introduction to Multiple Imputation as a powerful tool for dealing with missing data in experimental studies.\n\nAugust 2018\n\nData Science: Statistical Programming with R, Utrecht Summer School, Utrecht University.\n\nAugust 2016\n\nIntroduction to Neuroscience: From Molecule to Behavior, Radboud Summer School, Radboud University Nijmegen. Hands-on training at research labs of the Donders Institute for Brain and Cognition.\n\nAugust 2016\n\nHow to Improve the Quality and Translatability of Preclinical Animal Studies, Radboud Summer School, Radboud University Nijmegen. Course organized by SYRCLE institute, Radboud University Nijmegen and Radboud University Medical Centre. Theory and hands-on experience on Systematic Reviews and Meta-Analysis in preclinical animal studies. Overview of the main software available to conduct a Meta-Analysis and basic formation on academic reading and writing."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#tldr",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#tldr",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "TLDR",
    "text": "TLDR\n\nWe need to import several CSV or TXT files and merge them into one data frame in R. Regardless of what function we use to import the files, vectorising the operation using purrr::map in combination with do.call or dplyr::bind_rows is the most time-efficient method (~25 ms importing 50 files with 10,000 rows each), compared to for loops (~220ms) or using lapply (~123 ms). data.table::fread is the fastest function for importing data. Importing TXT files is slightly faster than importing CSV files."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Why this post",
    "text": "Why this post\nTo analyse data in any programming environment, one must first import some data. Sometimes, the data we want to analyse are distributed across several files in the same folder. I work with eye-tracking data from toddlers. This means that I work with multiple files that have many rows. At 120 Hz sampling frequency, we take ~8.33 samples per second. A session for one participants can take up to 10 minutes. So these files are somewhat big. These data also tend to be messy, requiring a lot of preprocessing. This means that I need to import the same large files many times during the same R session when wrangling my way through the data, which takes a few seconds. After some iterations, it can be annoying. I have decided to invest all my lost time into analysing what method for importing and merging large files is the fastest in R so that the universe and I are even again.\nBelow I provide several options for importing data from the different files, using base R and tidyverse, among other tools. I will compare how long it takes to import and merge data using each method under different circumstances. You can find the whole code here in case you want to take a look, reproduce it or play with it1.1 Ironically, this code is super inefficient and messy. It takes ages to run, and has been written by copy-pasting multiple times. I didn’t feel like doing anything more elegant. Also, I don’t know how. Help yourself."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "How can I import large files and merge them?",
    "text": "How can I import large files and merge them?\nSo we have some files in a folder. All files have the same number of columns, the same column names, and are in the same format. I assume that data are tabular (i.e., in the shape of a rectangle defined by rows and columns). I also assume that data are stored as Comma-Separated Values (.csv) or Tab-separated Text (.txt or .tsv), as these formats are the most reproducible.\nWe to import all files and bind their rows together to form a unique long data frame. There are multiple combinations of functions we can use. Each function comes with a different package and does the job in different ways. Next, I will show some suggestions, but first let’s create some data. We are creating 50 datasets with 10 columns and 10,000 rows in .txt format. The variables included are numeric and made of 0s and 1s. There is also a column that identifies the data set. These files are created in a temporary directory using the temp.dir function for reproducibility. After closing you R session, this directory and all of its contents will disappear.\n\n\n\nBase R: for loops\nfor loops are one of the fundamental skills in many programming languages. The idea behind for loops is quite intuitive: take a vector or list of length n, and apply a series of functions to each element in order. First, to element 1 and then to element 2, and so on, until we get to element n. Then, the loop ends. We will first make a vector with the paths of our files, and then apply the read.delim function to each element of the vector (i.e., to each path). Every time we import a file, we store the resulting data frame as an element of a list. After the loop finishes, we merge the rows of all element of the list using a combination of the functions do.call and rbind.\n\n\n\nBase R: lapply\n\nWe will use the functions read.delim and read.csv in combination with the function lapply. The former are well known. The later is part of a family of functions (together with sapply, mapply, and some others I can’t remember) that take two arguments: a list and a function, which will be applied over each element of the list in parallel (i.e., in a vectorised way).\n\n\n\nTidyverse\nThe tidyverse is a family of packages that suggests a workflow when working in R. The use of pipes (%>%) is one of its signature moves, which allow you to chain several operations applied on the same object within the same block of code. In contrast, base R makes you choose between applying several functions to the same object in different blocks of code, or applying those functions in a nested way, so that the first functions you read are those applied the last to your object (e.g., do.call(rbind, as.list(data.frame(x = \"this is annoying\", y = 1:100)))). We will use a combination of the dplyr and purrr packages to import the files listed in a vector, using read.delim and bind_rows.\n\n\n\ndata.table\nThe function rbindlist function from the package data.table also allows to merge the datasets contained in a list. In combination with fread (from the same package), it can be very fast."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "What method is the fastest?",
    "text": "What method is the fastest?\nI will compare how long each combination of importing, vectorising, and merging functions needs to import 50 data sets with 10 columns and 10,000 rows each. Additionally, I will compare the performance of each method when working with CSV (.csv) and TSV (.txt) files. For each method, I will repeat the process 100 times, measuring how long it takes from the moment we list the extant files in the folder to the moment we finish merging the data sets. Here are the results:\n\n\n\n\nFigure 1: Mean time (and standard deviation) for each combination of methods and file formats across 100 replications\n\n\n\n\nFor more detail:\n\n\n\n\n\n\n\nTable 1:  Execution times \n  \n\nTime taken to import and merge\n    \n\n50 datasets with 10 columns and 10,000 rows each\n    \n\n\n\n\n      package\n      \n        for loop\n      \n      \n        lapply\n      \n      \n        purrr::map\n      \n    \n\nM\n      SD\n      M\n      SD\n      M\n      SD\n    \n\n\n\ndo.call - .csv\n    \n\n\nbase\n1.34\n0.06\n1.11\n0.07\n0.15\n0.03\n\n\n\ndata.table\n1.33\n0.39\n0.88\n0.04\n0.17\n0.02\n\n\n\nreadr\n1.38\n0.05\n1.07\n0.13\n0.15\n0.02\n\n\ndplyr::bind_rows - .csv\n    \n\n\nbase\n1.35\n0.16\n0.99\n0.14\n0.15\n0.01\n\n\n\ndata.table\n1.14\n0.04\n0.84\n0.06\n0.16\n0.01\n\n\n\nreadr\n1.25\n0.14\n0.91\n0.04\n0.14\n0.00\n\n\ndo.call - .txt\n    \n\n\nbase\n1.18\n0.04\n0.88\n0.04\n0.17\n0.02\n\n\n\ndata.table\n1.17\n0.04\n0.80\n0.05\n0.15\n0.02\n\n\n\nreadr\n1.18\n0.05\n0.80\n0.05\n0.15\n0.02\n\n\ndplyr::bind_rows - .txt\n    \n\n\nbase\n1.13\n0.03\n0.84\n0.06\n0.20\n0.02\n\n\n\ndata.table\n1.19\n0.13\n0.77\n0.03\n0.14\n0.01\n\n\n\nreadr\n1.13\n0.04\n0.77\n0.03\n0.14\n0.01\n\n\nMean\n—\n1.23\n0.10\n0.89\n0.06\n0.15\n0.02\n\n\n\n\n\n\n\nFigure 1 and Table Table 1 show the detailed timings The grand mean average time taken by all methods is ~2.12 seconds, but there are some differences.\n\nIt doesn’t really matter what function we use to merge data sets: both do.call and dplyr::bind_rows perform roughly similarly.\nWhat makes the biggest difference is what function we use to vectorise the importing operation across file names to import them. purrr::map is the fastest. Incredibly, is takes less than 0.3 seconds in all conditions. It is also the least sensitive to the format of the files and the function we use to import them.\nThe next vectorising function in terms of temporal efficiency is lapply, which takes ~1.5 seconds. It performs slightly better when working with .txt files, in that when working with .csv files its performance depends on what method we use to import them: data.table::fread is much faster than its base and readr competitors. This post by Daniele Cook sheds some light into the advantage of data.table over other importing functions, also covering the vroom package, which this post doesn’t cover.\nUsing for loops looks like the least efficient method for iterating across data sets when importing data. It also shows a similar profile than lapply: data.table::fread performs a bit better than the rest."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#conclusion",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#conclusion",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Conclusion",
    "text": "Conclusion\nUnder the scenario under which I have simulated the data, it seems that using purrr::map in combination with do.call or dplyr::bind_rows to merge data sets is the most efficient method in terms of time. When using said combination, it doesn’t matter what function we use to import files, but data.table::fread seems like the best choice, as it is also the most flexible (take a look at the documentation of data.table to see all the features it offers).\nIf I have time, I may add another two dimensions: number of rows in the files and number of files, although I dare say similar results are to be expected. If anything, I would say that differences may become greater as file size and number of files increase. Also, it would be interesting to test if pre-allocating the elements of the vector in the for loop speeds up the process (see here what I mean). We shall see.\nHope this was useful, if not interesting!"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#code",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#code",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#session-info",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#session-info",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.8.0          ggsci_2.9         forcats_0.5.2     stringr_1.5.0    \n [5] readr_2.1.3       tidyr_1.2.1       tibble_3.1.8      ggplot2_3.4.0    \n [9] tidyverse_1.3.2   data.table_1.14.6 purrr_1.0.0       dplyr_1.0.10     \n[13] quarto_1.2       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9          lubridate_1.9.0     ps_1.7.2           \n [4] assertthat_0.2.1    digest_0.6.31       utf8_1.2.2         \n [7] R6_2.5.1            cellranger_1.1.0    backports_1.4.1    \n[10] reprex_2.0.2        evaluate_0.19       httr_1.4.4         \n[13] pillar_1.8.1        rlang_1.0.6         googlesheets4_1.0.1\n[16] readxl_1.4.1        rstudioapi_0.14     rmarkdown_2.19     \n[19] labeling_0.4.2      googledrive_2.0.0   munsell_0.5.0      \n[22] broom_1.0.2         compiler_4.2.2      modelr_0.1.10      \n[25] xfun_0.36           pkgconfig_2.0.3     htmltools_0.5.4    \n[28] tidyselect_1.2.0    fansi_1.0.3         crayon_1.5.2       \n[31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n[34] later_1.3.0         commonmark_1.8.1    grid_4.2.2         \n[37] jsonlite_1.8.4      gtable_0.3.1        lifecycle_1.0.3    \n[40] DBI_1.1.3           magrittr_2.0.3      scales_1.2.1       \n[43] cli_3.6.0           stringi_1.7.8       farver_2.1.1       \n[46] renv_0.16.0         fs_1.5.2            xml2_1.3.3         \n[49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.5.1        \n[52] tools_4.2.2         glue_1.6.2          hms_1.1.2          \n[55] processx_3.8.0      fastmap_1.1.0       yaml_2.3.6         \n[58] timechange_0.1.1    colorspace_2.0-3    gargle_1.2.1       \n[61] rvest_1.0.3         knitr_1.41          haven_2.5.1        \n[64] sass_0.4.4"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "title": "How similar is the word “mask” across languages?",
    "section": "",
    "text": "The ubiquity of masks has given psycholinguists a frequent-ish stimulus to use in experiments. This word is more form-similar across languages than one may think. I gathered a big-ish dataset with translation equivalents of the word mask across ~110 languages. I tweeted about this today, and wanted to dedicate some more lines to nuance.\nHere’s the data:\nTo compute the similarity of each pair of translation equivalents, I followed Floccia et al.’s (2018) procedure. For each pair of translation equivalents, I computed their Levenshtein distance as the number of insertions, deletions and replacements a string character has to go through to become identical to the other, and then divided this value by the number of characters of the longest of the two strings, so that all values range between 0 and 1. To compute the Levenshtein distance, I used the stringdist() function of the stringdist R package."
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Orthographic distance",
    "text": "Orthographic distance\nI first computed the orthographic distance between each pair of translation equivalents. Since some word forms make use of different alphabets, I first romanised all word forms. By romanised, I mean that I searched for the transcription of each word form in the Roman alphabet, and used it as input to compute the Levenshtein distance for each pair of translation equivalents. Here’s how orthographically similar (the romanisations of) the translations of mask are (N = 110 pairs):"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Phonological distance",
    "text": "Phonological distance\nThe phonological similarity/distance may be more informative. This time I searched for or generated with the help of a native speaker a phonological IPA transcription of each word-form. I then used this transcription as input to compute the phonological similarity of each pair of translation equivalents. A pitfall in this process is the fact that phonemes are almost never identical across languages, so even the common phoneme /m/ could vary slightly on its pronunciation in two languages. If this difference is encoded in the IPA transcription (as different characters), the Levenshtein distance will be inflated. For this reason, I simplified some IPA transcriptions to preserve this similarity. I also removed tones. This is terribly wrong from a linguistics perspective, but it’s the only way I see to be able to play with some reliable data. Also I’m no linguist, so you have no power here.\n\nHere’s the same analysis performed on phonological transcriptions of a subset of those languages (N = 75 pairs, those I could find a reliable IPA transcription for or could find help from a native speaker):"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "title": "How similar is the word “mask” across languages?",
    "section": "Onsets",
    "text": "Onsets\nMost of the times, the phonological overlap comes from onset graphemes/phonemes. This is how many word-forms start with each onset:"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "title": "How similar is the word “mask” across languages?",
    "section": "Some disclaimers:",
    "text": "Some disclaimers:\nI tried ensuring that words referred to surgical masks (instead of other types of masks) with help from native speakers. Wrong translations may still have slipped in (or be just wrong). I wish I had time to double-check all of them (I did this for fun).\nThis analysis is probably affected by selection bias. I suspect many dissimilar translations are missing due to not being included in the translation apps I used (e.g. Google Translate). Feel free to contribute missing entries or make corrections!"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "title": "How similar is the word “mask” across languages?",
    "section": "Code and data",
    "text": "Code and data"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "title": "How similar is the word “mask” across languages?",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] htmltools_0.5.4  kableExtra_1.3.4 knitr_1.41       gt_0.8.0        \n [5] readxl_1.4.1     forcats_0.5.2    stringr_1.5.0    dplyr_1.0.10    \n [9] purrr_1.0.0      readr_2.1.3      tidyr_1.2.1      tibble_3.1.8    \n[13] ggplot2_3.4.0    tidyverse_1.3.2  quarto_1.2      \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.4          sass_0.4.4          jsonlite_1.8.4     \n [4] viridisLite_0.4.1   modelr_0.1.10       assertthat_0.2.1   \n [7] renv_0.16.0         googlesheets4_1.0.1 cellranger_1.1.0   \n[10] yaml_2.3.6          pillar_1.8.1        backports_1.4.1    \n[13] glue_1.6.2          digest_0.6.31       rvest_1.0.3        \n[16] colorspace_2.0-3    pkgconfig_2.0.3     broom_1.0.2        \n[19] haven_2.5.1         scales_1.2.1        webshot_0.5.4      \n[22] processx_3.8.0      svglite_2.1.0       stringdist_0.9.10  \n[25] later_1.3.0         tzdb_0.3.0          timechange_0.1.1   \n[28] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[31] withr_2.5.0         cli_3.6.0           magrittr_2.0.3     \n[34] crayon_1.5.2        evaluate_0.19       ps_1.7.2           \n[37] fs_1.5.2            fansi_1.0.3         xml2_1.3.3         \n[40] tools_4.2.2         hms_1.1.2           gargle_1.2.1       \n[43] lifecycle_1.0.3     munsell_0.5.0       reprex_2.0.2       \n[46] compiler_4.2.2      systemfonts_1.0.4   rlang_1.0.6        \n[49] grid_4.2.2          rstudioapi_0.14     rmarkdown_2.19     \n[52] gtable_0.3.1        DBI_1.1.3           R6_2.5.1           \n[55] lubridate_1.9.0     fastmap_1.1.0       utf8_1.2.2         \n[58] commonmark_1.8.1    stringi_1.7.8       parallel_4.2.2     \n[61] Rcpp_1.0.9          vctrs_0.5.1         dbplyr_2.2.1       \n[64] tidyselect_1.2.0    xfun_0.36"
  },
  {
    "objectID": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "href": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "title": "A primer on mixed-effects Models: theory and practice",
    "section": "",
    "text": "When I started my PhD I had to get familiar with Linear Mixed Models very well very quickly. Then I was asked to present what I learnt and prepare this informal tutorial for my colleagues, which I presented on March 10th, 2020 (yes, early pandemic :confounded:). This post shares the result.\nThis is not supposed to be taken as a formal guide to linear mixed-effects models, but rather as a semi-coherent compilation of notes and self-suggestions that I considered worth sharing with my lab mates during my early stages of in the PhD. Here are the slides:\nWhat I should be taken more seriously are (1) the references I suggest in the first slides (which I consider some of the best resources available to learn linear mixed-effects models), and (2) the memes, which I personally curated and even created to sweeten up the dreadful incoherence of the content of some of the slides (sorry about that).\nBefore the presentation I tweeted one of the animations I generated for it.\nThis tweet got some attention (for my usual numbers) and many kind folks have asked for the R code or the GIF file of the specific animation included in the tweet, so here they are (you’ll also find them in the GitHub repository) in a perhaps more comfortable format, ready to be cloned or downloaded):"
  },
  {
    "objectID": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "href": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "title": "A primer on mixed-effects Models: theory and practice",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] xaringanExtra_0.7.0 quarto_1.2         \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9      ps_1.7.2        digest_0.6.31   later_1.3.0    \n [5] lifecycle_1.0.3 jsonlite_1.8.4  magrittr_2.0.3  evaluate_0.19  \n [9] stringi_1.7.8   rlang_1.0.6     cli_3.6.0       renv_0.16.0    \n[13] rstudioapi_0.14 vctrs_0.5.1     rmarkdown_2.19  tools_4.2.2    \n[17] stringr_1.5.0   glue_1.6.2      compiler_4.2.2  xfun_0.36      \n[21] yaml_2.3.6      fastmap_1.1.0   processx_3.8.0  htmltools_0.5.4\n[25] knitr_1.41"
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html",
    "href": "blog/psicotuiterbot/psicotuiterbot.html",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "",
    "text": "Follow @psicotuiterbot"
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Escribiendo el código",
    "text": "Escribiendo el código\nUn bot de Twitter no es más que un código que se ejecuta automáticamente de forma periódica (ej., cada 15 minutos) y realiza una acción en Twitter a través de una cuenta (hacer un RT o responder a un tweet). Para poder realizar esta acción a través del código, es necesario tener acceso a la API de Twitter. API es un acrónimo para Application Programming Interface y como dice su nombre, es una plataforma desde la que podemos interactuar con una aplicación (en este caso Twitter) a través de programación con una serie de comandos que el equipo de Twitter ha diseñado la API ha definido.\nNunca había hecho un bot de Twitter, pero sabía de la existencia de bastantes tutoriales para hacerlo. La mayoría de los bots de Twitter (y por tanto de los tutoriales) están escritos en Python y JavaScript, pero yo me encuentro algo más cómodo con R. Además gran parte de Psicotuiter (especialmente quienes están relacionades con la metodología) también está más familiarizado con R. Mi intención era hacer el bot lo más trasparente y accesible para la comunidad, así que me decanté por R. En un pricipio seguí el tutorial de Matt Dray, en el que utiliza el paquete de R {rtweet} para interactuar con la API de Twitter. Hay más tutoriales que usan rtweet para crear un bot con R. Pero a diferencia de otros, este tutorial explicaba cómo usar GitHub actions para ejecutar el código de forma periódica. Ahora explico esto último. Mientras tanto, vamos al código de R.\nPrimero creé un repositorio de GitHub donde alojar el código (GitHub es como un Google Drive especializado en código donde además podemos hacer control de versiones de los archivos que subimos). Puedes echar un vistazo al código de R en la carpeta R/. El código principalmente recoge los últimos tweets que se han escrito en las últimos 6 horas mencionando #psicotuiter o #psicotwitter y los retuitea. No me meteré en detalle a explicar cómo funciona el código, pero aquí va un pequeño resumen del programa. Si tienes curiosidad te recomiendo explorar el repositorio de GitHub, que contiene todo lo necesario para hacer funcionar el bot:\nPrimero cargamos el paquete de R {dplyr}, que utilizamos en bastantes ocasiones en el programa, ajustamos un pequeño detalle relacionado con el comportamiento de la API de Twitter, e instalamos los paquetes necesarios (en caso de que hayan cambiado) usando el pqeute de R {renv} (si no lo conoces y te interesa la reproducibilidad computacional tienes que echarle un vistazo).\n# bot R code\nlibrary(dplyr)\noptions(httr_oob_default = TRUE)\n# restore packages\nrenv::restore()\nA continuación extraemos las credenciales que necesitamos para acceder a la API de Twitter (las paso a como variables de entorno para evitar hacerlas públicas, ya que eso dería acceso a cualquiera a la cuenta de Twitter del bot).\n# authenticate Twitter API\nmy_token <- rtweet::create_token(\n    app = \"psicotuiterbot\",  # the name of the Twitter app\n    consumer_key = Sys.getenv(\"TWITTER_CONSUMER_API_KEY\"),\n    consumer_secret = Sys.getenv(\"TWITTER_CONSUMER_API_KEY_SECRET\"),\n    access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n    access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"), \n    set_renv = FALSE\n)\nLuego extraemos los tweets usando rtweet, filtramos los que sean relevantes y no contengan posible contenido ofensivo (y algún otro filtro más).\nlibrary(dplyr)\n# define hashtags\nhashtags_vct <- c(\"#psicotuiter\", \"#psicotwitter\", \"#Psicotuiter\", \"#Psicotwitter\", \"#PsicoTuiter\", \"#PsicoTwitter\")\nhashtags <- paste(hashtags_vct, collapse = \" OR \")\nhate_words <- unlist(strsplit(Sys.getenv(\"HATE_WORDS\"), \" \")) # words banned from psicotuiterbot (separated by a space)\nblocked_accounts <- unlist(strsplit(Sys.getenv(\"BLOCKED_ACCOUNTS\"), \" \")) # accounts banned from psicotuiterbot (separated by a space)\ntime_interval <- lubridate::now(tzone = \"UCT\")-lubridate::minutes(120)\n# get mentions to #psicotuiter and others\nall_tweets <- rtweet::search_tweets(\n    hashtags, \n    type = \"recent\", \n    token = my_token, \n    include_rts = FALSE, \n    tzone = \"CET\"\n) \nstatus_ids <- all_tweets %>% \n    filter(\n        !(screen_name %in% gsub(\"@\", \"\", blocked_accounts)),\n        created_at >= time_interval, # 15 min\n        !grepl(paste(hate_words, collapse = \"|\"), text), # filter out hate words\n        stringr::str_count(text, \"#\") < 4, # no more than 3 hashtags\n        lang %in% c(\"es\", \"und\") # in Spanish or undefined language\n    ) %>% \n    pull(status_id)\n# get request ID\nrequest_tweets <- rtweet::get_mentions(\n    token = my_token, \n    tzone = \"CET\"\n) \nFinalmente, hacemos RT uno a uno usando rtweet (si ya habíamos hecho RT a uno de ellos simplemente se ignora).\nif (nrow(request_tweets) > 0) {\n    request_ids <- request_tweets %>% \n        filter(\n            created_at >= time_interval, # 15 min\n            grepl(\"@psicotuiterbot\", text),\n            grepl(\"rt|RT|Rt\", text),\n            !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n        ) %>% \n        pull(status_in_reply_to_status_id)\n    \n    # get requested IDS\n    if (length(request_ids) > 0) {\n        requested_ids <- rtweet::lookup_statuses(request_ids, token = my_token) %>% \n            filter(\n                !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n            ) %>% \n            pull(status_id)\n    } else {\n        requested_ids <- NULL\n    }\n} else {\n    requested_ids <- NULL\n}\n# RT all IDs\nif (length(status_ids) > 0){\n    for (i in 1:length(status_ids)){\n        rtweet::post_tweet(\n            retweet_id = unique(status_ids)[i], # vector with IDs\n            token = my_token\n        )\n    }\n    print(paste0(length(status_ids), \" RT(s): \", paste(status_ids, collapse = \", \")))\n} else {\n    print(\"No tweets to RT\")\n}\nRecientemente incluí un pequeño bloque de código para permitir que la gente solicitase un RT para tuits que no mencionaban #psicotuiter, pero podrían ser de interés para la comunidad. Para hacerlo solo hay que responder al tuit en cuestión mencionando a @psicotuiterbot junto con la palabra RT (ej., “RT @psicotuiterbot por favor”).\n# tweet requests\nif (length(requested_ids) > 0){\n    for (i in 1:length(requested_ids)){\n        rtweet::post_tweet(\n            retweet_id = unique(requested_ids)[i], # vector with IDs\n            token = my_token\n        )\n    }\n    print(paste0(length(requested_ids), \" request(s) posted: \", paste(requested_ids, collapse = \", \")))\n} else {\n    print(\"No requests\")\n}"
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#ejecutando-el-código",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#ejecutando-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Ejecutando el código",
    "text": "Ejecutando el código\nLo que hace “bot” a un bot es que no requiere intervención manual para que realice la acción que deseamos. Hay muchas opciones para conseguir esto, pero casi todas tienen un inconveniente: necesitamos que una máquina (ordenador/servidor, móvil, etc.) esté encendido en el momento en el que queremos ejecutar nuestro código. En nuestro caso necesitamos que se ejecute cada 15 minutos, lo que implica que deberíamos tener un dispositivo conectado a la corrriente y funcionando todo el día. Tener mi ordenador personal haciendo esto no es una opción viable. Me encontré con dos alternativas.\n\nPrimer intento (sale mal): GitHub actions\nComo mencioné antes, en el tutorial de Matt Dray se ilustra cómo usar GitHub Actions para ejecutar nuestro código una vez está alojado en un repositorio de GitHub. GitHub Actions es un servicio que ofrece GitHub que permite ejecutar ciertos comandos en determinadas condiciones o cada cierto tiempo usando un servidor que ponen a nuestra disposición (con ciertos límites). Este grupo de comandos se denominan workflows o flujos de trabajo, y si los incluimos en una carpeta de nuestro repositorio llamada .github/workflows/ siguiendo cierto formato en un archivo YAML (.yml), GitHub se encargará de ejecutarlo sin nuestra intervención. Hay buenos tutoriales sobre cómo y cuándo escribir workflows para GutHub Actions. El workflow principal era inicialmente este:\nname: bot\non:\n  push:\n    branches:\n      - main # run every time there is a push to main branch\n      - test\njobs:\n  psicotuiterbot-post:\n    runs-on: macOS-latest\n    env: #  twitter API keys (used to authenticate) defined in the gh actions environment\n      TWITTER_CONSUMER_API_KEY: ${{ secrets.TWITTER_CONSUMER_API_KEY }}\n      TWITTER_CONSUMER_API_KEY_SECRET: ${{ secrets.TWITTER_CONSUMER_API_KEY_SECRET }}\n      TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}\n      TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}\n      \n    steps:\n      - uses: actions/checkout@v2\n      - uses: r-lib/actions/setup-r@v1\n      - uses: r-lib/actions/setup-renv@v1\n        with:\n          cache-version: 1\n      - name: Restore packages using renv\n        shell: Rscript {0}\n        run: |\n          if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::restore()\n      - name: Create and post tweet\n        run: Rscript R/bot.R\nLa gran ventaja de usar este sistema es que no necesitamos usar nuestro ordenador personal, ya que usamos el que que GitHub nos asigna (un servidor no deja de ser un ordenador). Pero tiene varios inconvenientes. El primero es que el proceso de establecer un workflow en GitHub Actions suele requerir varios intentos (en mi caso muchos). Esto suele deberse a problemas de reproducibilidad computacional: el código funciona correctamente en mi ordenador porque en él tengo instalado todo el sofware del que depende. Cuando uso el servidor de GitHub, el sistema operativo suele necesitar que instalemos estas dependencias antes de ejecutar el código. GitHub Actions permite cierta flexibilidad a la hora de seleccionar el software que viene instalado en el sistema operativo que vamos a usar (ej., R, compiladores de C++, dependencias de Linux, etc.). El problema es que muchas veces ni siquiera somos conscientes de cuántas dependencias requiere nuestro código. Con paciencia y muchas búsquedas de Google es posible solventar este problema.\nUn segundo inconveniente que encontré a la hora de implementar el bot en GitHub Actions tiene que ver con los tiempos: instalar todas las dependencias del código en el servidor cada 15 minutos (la configuración se pierde casi totalmente tras cada ejecución) es poco eficiente. Instalar las dependencias puede tardar más de 10 minutos (en el caso de este bot). Esto puede además hacer fallar en ocasiones el flujo de trabajo. GitHub Actions tampoco es lo más consistente del mundo, aunque no deja de ser gratis."
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Segundo intento (sale bien): Raspberry Pi",
    "text": "Segundo intento (sale bien): Raspberry Pi\nTras varios problemas en la ejecución del bot a través de GitHub Actions, decidí cambiar de método. Por razones ajenas al bot, hacía unos meses que tenía muerta de aburrimiento una Raspberry Pi 4 que compré con un amigo para jugar con ella. Este dispositivo es un mini-ordenador relativamente barato (~40€) que salió al mercado como herramienta educativa para enseñar a programar (ej., róbotica para niñes) pero que poco a poco ha ido tomando espacio en lugares de producción. Tiene mil posibilidades por su simplicidad y, en nuestro caso, por su bajo consumo: tener una Raspberry Pi funcionando todo el día apenas tiene impacto sobre el consumo de luz.\n\n\n\nAquí está alojado el @psicotuiterbot\n\n\nPrimero instalé el código en la Raspberry con sus dependencias: básicamente, cloné el repositorio de GitHub en una carpeta dentro de home/Documents/. Para ejecutar el código cada 15 minutos utilicé una función muy útil que incluye Linux (sistema operativo con el que funciona la Raspberry) llamado CRON. Simplemente consiste en un archivo en el que incluimos una serie de comandos que queremos que se ejecuten de forma periódica, junto con una código que indica la periodicidad de la ejcución de este comando. Aquí tienes unos ejemplos. Incluí cuatro comandos (cada uno en su propio archivo con extensión .sh, que denota comandos de Linux):\n# descarga el código de GitHub, por si ha habido cambios\ngit pull origin main\n# ejecuta el código principal del bot\nTZ=\"Spain/Madrid\" Rscript -e 'source(\"R/bot.R\")'\n# guarda los tweets detectados en un archivo y crea un gráfico\nRscript -e 'source(\"R/counts.R\")'\nrm Rplots.pdf\n# sube los nuevos datos a GitHub\ngit add .\ngit commit -m \"Update repository\"\ngit push\nEstos comandos se ejecutan en este orden cada 15 minutos."
  },
  {
    "objectID": "blog/renv-package/renv-package.html",
    "href": "blog/renv-package/renv-package.html",
    "title": "renv (o cómo usar paquetes de R sin ataques de pánico)",
    "section": "",
    "text": "A veces necesitamos instalar versiones diferentes del mismo paquete de R en proyectos diferentes. El paquete {renv} nos permite almacenar los paquetes de R de cada proyecto de forma independiente, evitando posibles conflictos entre proyectos. De paso, incrementará la reproducibilidad computacional de nuestro código.\n\n\nEl problema\nPonte en la siguiente situación: tienes entre manos un proyecto de R que necesita varios paquetes. Cada uno de estos paquetes depende, a su vez, de terceros paquetes. De hecho, dos paquetes pueden depender del mismo paquete, o incluso de versiones diferentes del mismo paquete. Si hay mala suerte, una de las versiones no será lo suficientemente reciente como para funcionar correctamente con ambos paquetes. Resultado: uno de los dos paquetes no funcionará.\n\n\n\nUsuarie de R promedio después de tirar dos horas a la basura intentando instalar los paquetes que necesita para trabajar en un proyecto de R que no tocaba desde hacía cuatro meses.\n\n\nEste problema se extiende al caso de que necesitemos versiones diferentes del mismo paquete en proyectos de R diferentes en los que estamos trabajando de forma simultánea en el mismo ordenador. Para hacerlos funcionar necesitaríamos instalar de nuevo la versión correspondiente del mismo paquete cada vez que cambiemos de proyecto.\n\n\nInstalando paquetes de R\nEn resumidas cuentas, cada vez que instalamos o actualizamos un paquete de R, lo hacemos para todos nuestros proyectos de R de forma global. Esto se debe a que por defecto R busca todos los paquete de R en la misma carpeta. Para ver dónde instala R tus paquetes puedes ejecutar el siguiente comando:\n.libPaths()\nEste comando te mostrará el directorio o directorios donde R instala sus paquetes por defecto. Si hay más de un directorio significa que, en caso de que no sea posible encontrar un paquete en el primer directorio, R lo buscará en el segundo, tercero, etc., hasta que te devuelva un error indicando que no has instalado ese paquete.\nSi accedes al primer directorio que muestra .libPaths() verás que cada paquete tiene una carpeta. Cada carpeta incluye el código de R, los datos y la documentación asociada a cada paquete (entre otras cosas). Cada vez que instalamos o actualizamos un paquete, se crea o reemplaza su carpeta correspondiente en nuestro directorio, es decir, en nuestra “biblioteca global” de paquetes de R.\n\n\n\nAsí es como tienes tu carpeta de paquetes de R. Que lo sé yo. Que te he visto. Vergüenza me daría a mí.\n\n\nHemos visto que esto no es ideal. ¿No será mejor tener una carpeta diferente para cada proyecto en la que instalamos sus paquetes de forma independente, sin afectar a los paquetes de otros proyectos? Sí. De hecho este procedimiento es estándar en otros lenguajes de programación como JavaScript1 o Julia 2. Existe un paquete de R que nos permite hacer esto: renv. Veamos cómo funciona.1 Echa un vistazo a este post de Nikola Đuza: Ride Down Into JavaScript Dependency Hell2 Echa un vistazo a este post de Bogumił Kamiński: My practices for managing project dependencies in Julia\n\n\nUsando renv\nPrimero hay que instalar renv. Como está incluido en el CRAN, podemos hacerlo usando install.packages():\ninstall.packages(\"renv\")\nAhora abrimos una sesión de R en la carpeta de nuestro proyecto. Digamos que nuestra carpeta tiene la siguiente estructura:\ndata\n |-some-data.csv\ndocs\n |-index.Rmd\n |-index.html\nR\n |-main.R\n |-functions.R\n.Rprofile\nAsí es la estructura de la mayoría de carpetas de mis proyectos de R. Tiene su razón de ser, pero eso es material para otro post. Lo importante es cómo cambiará esta estructura en unos momentos. La documentación de renv incluye los pasos para usar renv, pero explicaré los principales. Primero inicializaremos renv en nuestra consola de R:\nrenv::init()\nEsto creará una carpeta (renv) y un archivo (renv.lock) nuevos en nuestro directorio:\n.Rprofile\ndata\n    |-some-data.csv\ndocs\n    |-index.Rmd\n    |-index.html\nR\n    |-main.R\n    |-functions.R\nrenv\n    |-.gitignore\n    |-activate.R\n    |-library\n        |-...\n    |-local\n        |-...\n    |-settings.dcf\nrenv.lock\nNo necesitaremos modificar ni consultar nunca ningunos de los archivos creados, pero vamos a curiosear un poco. Al usar init(), renv ha echado un vistazo a los scripts de R de la carpeta (achivos con la extensión .R, como main.R y functions.R), y ha detectado los paquetes que necesita nuestro código para ejecutarse (puedes consultar las dependencias de tu proyecto usando renv::dependencies()). Por ejemplo, si main.R incluye library(dplyr) o dplyr::mutate(), detectará el paquete dplyr como una dependencia.\n\n\n\nrenv detectando tus dependencias.\n\n\nA continuación, renv ha instalado todas los paquetes necesarios en renv/library/. Si comparas esa carpeta con el directorio mostrado en .libPaths() (como hicimos hace un momento), verás que ambas carpetas son muy parecidas. Eso es porque ahora R buscará los paquetes que necesites en esa carpeta, y no en la “biblioteca global” de paquetes de R. Esa es la magia de renv: podrás instalar y actualizar paquetes de R de forma independiente para cada uno de tus proyectos. Para instalar nuevos paquetes deberás hacerlo usando la función renv::install(). Por ejemplo:\nrenv::install(\"tidyr\")\nEsta función es el equivalente a install.packages() en renv. esta función asumirá que el paquete que quieres se encuentra en CRAN y será allí donde lo buscará. Si el paquete que quieres instalar se encuentra alojado en otro sitio (o quieres instalar una versión experimental del mismo, en un repositorio de GitHub, por ejemplo), puedes indicar el repositorio de la siguiente forma:\nrenv::install(\"crsh/papaja\")\nSi echas un vistazo a renv.lock verás que incluye una lista de todas las dependencias de tu proyecto, en un formato un poco raro, con muchos paréntesis, y la extensión .lock. No necesitas entender este archivo, sólo que sigue un formato parecido al que usan otros leguajes de programación para hacer lo mismo. Es el equivalente al archivo package-lock.json de un proyecto de JavaScript o al archivo Manifest.toml de un proyecto de Julia. Si te fijas, verás que simplemente incluye información mínima para cada paquete: nombre, versión, origen y un código que lo identifica. Por ejemplo, el renv.lock de nuestro proyecto incluye lo siguiente:\n{\n  \"R\": {\n    \"Version\": \"4.0.4\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"dplyr\": {\n      \"Package\": \"dplyr\",\n      \"Version\": \"1.0.8\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"ef47665e64228a17609d6df877bf86f2\"\n    },\n    \"papaja\": {\n      \"Package\": \"papaja\",\n      \"Version\": \"0.1.0.9997\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"crsh\",\n      \"RemoteRepo\": \"papaja\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a231c3628ccf24359cc17f11a5bbc743e3fed920\",\n      \"Remotes\": \"tidymodels/broom\",\n      \"Hash\": \"3df0637229690f807616c46d3ff77113\"\n    },\n    \"tidyr\": {\n      \"Package\": \"tidyr\",\n      \"Version\": \"1.2.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"d8b95b7fee945d7da6888cf7eb71a49c\"\n    },\n  }\n}\nUna ventaja enorme de usar renv es que si descargas o copias y pegas esta carpeta en un ordenador diferente (en el que posiblemente tengas una colección de paquetes diferente a la del ordenador donde trabajaste con el proyecto por última vez), renv podrá consultar este archivo para instalar por tí los paquetes necesarios en sus versiones correspondientes. Esto se puede hacer usando el comando:\nrenv::restore()\nImportante: cuando instales nuevos paquetes usando renv::install(), el archivo renv.lock no se actualizará de forma automática. Para incluir los nuevos paquetes en este archivo, tendremos que usar el siguiente comando:\nrenv::snapshot()\nComo podrás imaginar, poder instalar los paquetes que necesita un proyecto en su versión adecuada resuelve uno de los problemas más frecuentes que amenazan la reproducibilidad computacional de nuestros proyectos.\n\n\n\nImaginando un mundo donde todo el mundo se preocupa lo suficiente por la reproducibilidad computacional de sus proyectos.\n\n\n\n\nConclusiones\nTe recomiendo empezar a usar renv en algún proyecto “de juguete” con el que puedas experimentar, e ir poco a poco incorporando esta rutina en tus proyectos por el bien de tu salud mental y de la de les demás. :smile:\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{garcia-castro2022,\n  author = {Gonzalo Garcia-Castro},\n  title = {Renv (o Cómo Usar Paquetes de {R} Sin Ataques de Pánico)},\n  date = {2022-02-27},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGonzalo Garcia-Castro. (2022, February 27). renv (o cómo usar\npaquetes de R sin ataques de pánico)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "title": "Visualising polynomial regression",
    "section": "",
    "text": "When modelling data using regression, sometimes the relationship between input variables and output variables is not very well captured by a straight line. A standard linear model is defined by the equation\n\\[y_i = \\beta_{0} + \\beta_{1}x_{i}\\]\nwhere \\(\\beta_{0}\\) is the intercept (the value of the input variable \\(x\\) where the output variable \\(y=0\\)), and where \\(\\beta_{1}\\) is the coefficient of the input variable (how much \\(y\\) increases for every unit increase in \\(x\\)). To illustrate this, let’s imagine we are curious abut what proportion of the students in a classroom are paying attention, and how this proportion changes as minutes pass. We could formalise our model as\n\\(y_i = \\beta_{0} + \\beta_{1} Time_i\\)\nLet’s generate some data to illustrate this example. Let’s say that, at the beginning of the lesson, almost 100% of the students are paying attention, but that after some time stop paying attention. Right before the end of the class, students start paying attention again.\nThe attention paid by the students did not decay linearly, but first dropped and rose up again, following a curvilinear trend. In these cases, we may want to perform some transformation on some input variables to account for this non-linear relationship. One of these transformations are polynomial transformations. In this context, when we talk about applying a polynomial function to a set of values, we usually mean exponentiating it by a positive number larger than 1. The power by which we exponentiate our variable defines the degree of the polynomial we are obtaining. Exponentiating our variable to the power of 2 will give us its second-degree polynomial. Exponentiating it by 3 will give us its third-degree polynomial, and so on. Back to our classroom example, we could add a new term to our regression equation: the second-degree polynomial of the input variable \\(Time\\), or even a third degree polynomial if we wanted to test to what extend our model follows a more complex pattern. Our regression trend will not be linear any more, but curvilinear. Let’s take a look at the anatomy of polynomials from a visual (and very informal perspective). Our model would look like this:\n\\[\ny_i = \\beta_{0} + \\beta_{1} Time_i + \\beta_{2} Time_{i}^2 + \\beta_{3} Time_{i}^3\n\\]\nAdding polynomial terms to our regression offers much flexibility to researchers when modelling this kind of associations between input and output variables. This practice is, for example, common in Cognitive Science when analysing repeated measures data such as eye-tracking data, where we register what participants fixated in a screen during a trial under several conditions. Polynomial regression could be considered as of the main techniques in the more general category of Growth Curve Analyis (GCA) methods. If you are interested in learning GCA, you should take a look at Daniel Mirman’s “Growth Curve Analysis and Visualization Using R” [book].\nPowerful as this technique is, it presents some pitfalls, especially to newbies like me. For instance, interpreting the outputs of a regression model that includes polynomials can tricky. In our example, depending on the values of the coefficients \\(\\beta_{1}\\), \\(\\beta_2\\) and \\(\\beta_3\\)–the first-degree and second-degree polynomials of \\(Time\\)–the shape of the resulting curve will be different. The combination of values that these two coefficient can take is infinite, and so is the number of potential shapes our curve can adopt. Interpreting how the values of these coefficients affect the shape of our model, and more importantly, their interaction with other predictors of interest in the model can be difficult without any kind of visualisation. The aim of this post is to visualise how the regression lines of a regression model changes with the degree of its polynomials. For computational constraints, and to make visualisation easier, I will only cover one, two, and three-degree polynomials. I will generate plots for multiple combinations of the coefficients of these polynomials using the base R function poly() to generate polynomials, the R package ggplot2() to generate plots, and the gganimate R package to animate the plots. I will briefly describe what is going on in each plot, but I hope the figures are themselves more informative than anything I can say about them!"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "title": "Visualising polynomial regression",
    "section": "Intercept",
    "text": "Intercept\nFirst, let’s start with how the value of the intercept (\\(\\beta_0\\)) changes the regression line for polynomials of different degree (1st, 2nd, and 3rd). I set the rest of the coefficients to arbitrary values for simplicity (\\(\\beta_1 = \\beta_2 = \\beta_3 = 1\\)). As you can see, regardless of the order of the polynomials involved in the model, increasing the intercept makes the line be higher in the Y-axis, and decreasing the value of the intercept makes the line be lower in the Y-axis. Simple as that.\n\nThe interpretation of the intercept is similar to how we interpret it in standard linear regression models. It tells us the value of \\(y\\) when all predictors are set to 0 (in our case \\(Time = 0\\)). As we will discuss later, what that means in practice depends on what that zero means for the other coefficients, that is, how we coded them. For now, let’s continue adding more terms to the equation."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Linear term: adding a 1st-order polynomial",
    "text": "Linear term: adding a 1st-order polynomial\nNow let’s see how a linear model (with only a 1st degree polynomial) changes as we vary the value of \\(\\beta_1\\), the coefficient of the linear term \\(Time\\). As you can see, nothing special happens, the line just gets steeper, meaning that for every unit increase in \\(x\\), \\(y\\) increases (or decreases, depending on the sign) in \\(\\beta_1\\) units. When the coefficient equals zero, there is no increase nor decrease in \\(y\\) for any change in \\(x\\).\n\nWhen \\(\\beta_1=0\\), the resulting line is completely horizontal, parallel to the X-axis. This is what a model with just an intercept (\\(y = \\beta_{0}\\)) would look like. We generalise this to say that the linear model we just visualised is exactly the same as adding a 2nd and a 3rd degree polynomial to the model with their correspondent coefficients set to zero (\\(\\beta_2 = 0\\) and \\(\\beta_3 = 0\\), respectively)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Quadratic: adding a 2nd-order polynomial",
    "text": "Quadratic: adding a 2nd-order polynomial\nNow things get a bit more interesting. When we add a second degree polynomial (\\(Time^2\\)), the line is not linear any more. If the coefficient of the 2nd-order polynomial (\\(\\beta_2\\)) is positive, the curve will go down and up in that order. When \\(\\beta_2 < 0\\), the curve goes up and then down. When \\(\\beta_2 = 0\\), the curve turns out the be a line whose slope is defined by \\(\\beta_1\\), just like in the previous example.\n\nImportantly, varying the value of the coefficient of 1st-order polynomials (\\(\\beta_1\\)) also changes the shape of the curve: more positive values of \\(\\beta_1\\) make the curve “fold” at higher values of \\(x\\). As you can see, when \\(\\beta_1 < 0\\) (left panel, in blue), the point at which the curve starts increasing or decreasing occurs more to the left. When \\(\\beta_2 > 0\\), this change occurs more to the right."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Cubic: adding a 3rd-order polynomial",
    "text": "Cubic: adding a 3rd-order polynomial\nFinally, let’s complicate things a bit more by adding a third-order polynomial. Now the curve will “fold” two times. The magnitude of \\(\\beta_3\\) (the coefficient of the 3rd-degree polynomial) determines how distant both folding points are in the y-axis. When \\(\\beta_3\\) is close to zero, both folding points get closer, resembling the shape we’ve seen in a model with just a 2nd-degree polynomial. In fact, when \\(\\beta_3 = 0\\), we get the same plot (compare the panel to the right-upper corner to the plot in the previous section). The sign of \\(\\beta_3\\) also determines whether the curve goes down-up-down or up-down-up: down-up-down if \\(\\beta_3 < 0\\), and up-down-up if \\(\\beta_3 > 0\\).\nThe magnitude of \\(\\beta_2\\) (the coefficient of the 2rd-degree polynomial) determines the location of the mid-point between both folding points. For more positive values of \\(\\beta_2\\) this point is located higher in the y-axis, while for more negative values of \\(\\beta_2\\), this point is located lower in the y-axis. This value is a bit difficult to put in perspective in our practical example. Probably \\(\\beta_1\\) is more informative: \\(\\beta_1\\) changes the value of \\(x\\) at which the curve folds. More negative values of \\(\\beta_1\\) make the curve fold at lower values of \\(x\\), while more positive values of \\(\\beta_1\\) make the curve fold at higher values of \\(x\\)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "title": "Visualising polynomial regression",
    "section": "Conclusion",
    "text": "Conclusion\nThere are way more things to say about polynomial regression, and it’s more than likely that I sacrifice accuracy for simplicity. After all, the aim of generating these animations was helping myself understand the outputs of polynomial models a bit more easily in the future. I hope it helps others too. If you consider something is misleading or inaccurate, please let me know! I’m the first interested in getting it right. Cheers!"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "title": "Visualising polynomial regression",
    "section": "Just the code",
    "text": "Just the code"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "title": "Visualising polynomial regression",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] quarto_1.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9      ps_1.7.2        digest_0.6.31   later_1.3.0    \n [5] lifecycle_1.0.3 jsonlite_1.8.4  magrittr_2.0.3  evaluate_0.19  \n [9] stringi_1.7.8   rlang_1.0.6     cli_3.6.0       renv_0.16.0    \n[13] rstudioapi_0.14 vctrs_0.5.1     rmarkdown_2.19  tools_4.2.2    \n[17] stringr_1.5.0   glue_1.6.2      compiler_4.2.2  xfun_0.36      \n[21] yaml_2.3.6      fastmap_1.1.0   processx_3.8.0  htmltools_0.5.4\n[25] knitr_1.41"
  }
]