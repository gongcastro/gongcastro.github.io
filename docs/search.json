[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD student at the Center for Brain and Cognition, at Universitat Pompeu Fabra (Barcelona, Spain). I study how bilingual toddlers develop their vocabulary, using both experimental and observational techniques. Learning new things about programming languages brings me joy. I mainly use Twitter for sharing research outputs, programming tips or data visualisations. Don’t hesitate to get in touch on Twitter (@gongcastro) or at gonzalo.garciadecastro@upf.edu!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "renv (o cómo usar paquetes de R sin ataques de pánico)\n\n\n\n\n\nrenv es un paquete de R que permite instalar paquetes de R gestionar sus versiones para proyectos de forma independiente. Aqui resumo para qué se utiliza y cómo funciona.\n\n\n\n\n\n\nFeb 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\n@psicotuiterbot: Un bot de Twitter para Psicotuiter\n\n\n\n\n\nHe creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON.\n\n\n\n\n\n\nDec 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow similar is the word “mask” across languages?\n\n\n\n\n\nUsing the Levenshtein distance to quantify the orthographic and phonlogical similarity between translation equivalents of the word mask across multiple languages.\n\n\n\n\n\n\nNov 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nImporting data from multiple files simultaneously in R\n\n\n\n\n\nA comparison between base R and Tidyverse methods for importing data from multiple files\n\n\n\n\n\n\nJul 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nA primer on Mixed-Effects Models: Theory and practice\n\n\n\n\n\nSlides from a tutorial on mixed-effects models I presented to my research group.\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Click here for PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\nOctober 2018 - Present\nPhD, Biomedicine; Pompeu Fabra University (Barcelona, Spain); Supervisor: Núria Sebastian-Galles.\nOctober 2017 - July 2018\n\nMSc, Neurosciences; University of Barcelona (Barcelona, Spain). Dissertation: Phonemic Contrast Perception: A Segmentation Study on Monolingual and Bilingual Infants; Supervisors: Núria Sebastian-Galles and Chiara Santolin.\n\nSeptember 2013 - July 2017\n\nBSc, Psychology; University of Oviedo (Oviedo, Spain). Dissertation: Effects of Environmental Enrichment on Attention, Spatial Reference Memory, and Cytochrome C Oxidase Activity; Supervisors: Azucena Begega and Marcelino Cuesta."
  },
  {
    "objectID": "cv.html#contributions",
    "href": "cv.html#contributions",
    "title": "CV",
    "section": "Contributions",
    "text": "Contributions\n\nArticles\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., Saffran, J. (2020). Experience with research paradigms relates to infants’ direction of preference. Infancy, 00, 1–8. https://doi.org/10.1111/infa.12372 [preprint] [OSF] [code]\nSampedro-Piquero, P., Álvarez-Suárez, P., Moreno-Fernández, R., García-Castro, G., Cuesta, M., Begega, A. (2018). Environmental enrichment results in both brain connectivity efficiency and selective improvement in different behavioral tasks. Neuroscience, 388, 374-383. https://doi.org/10.1016/j.neuroscience.2018.07.036\n\n\nPreprints\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 4). Experience with research paradigms relates to infants’ direction of preference. https://doi.org/10.31234/osf.io/xgvbh"
  },
  {
    "objectID": "cv.html#proceedings",
    "href": "cv.html#proceedings",
    "title": "CV",
    "section": "Proceedings",
    "text": "Proceedings\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (Sempember, 2021). A study of lingusitic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presented at the Boston University Conference in Language Development, held online (Boston, United States).\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., Plunkett, K. (September, 2021). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Presentation at the Architectures and Mechanisms for Language Processing conference, held online (Paris, France). pdf scholar\n\nPosters\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (August, 2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presentation at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).se\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The impact of cognateness on bilingual lexical access: a longitudinal priming study. Poster presented at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (June, 2021). The role of cognateness in nonnative spoken word recognition. Poster presented at the XI International Symposium of Psycholinguistics (Madrid, Spain, held online).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (July, 2020). The role of lexical similarity on bilingual parallel activation: A priming study in toddlers. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nGarcia-Castro, G., Avila-Varela, D., and Sebastian-Galles, N. (July, 2020). Does phonological overlap across translation equivalents predict earlier age of acquisition?. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., and Plunkett, K. (August, 2019). The impact of phonology (cognateness) on the bilingual lexicon: Parallel cross-language phonological priming. Poster presented at the Lancaster Conference on Infant and Early Child Development (Lancaster, United Kingdom).\nGarcia-Castro, G., Marimon, M., Santolin, C., and Sebastian-Galles, N. (June, 2019). Encoding new word forms when contrastive phonemes are interchanged: A preliminary study on 8-month-old infants. Poster presented at the Workshop in Infant Language Development (Potsdam, Germany). https://doi.org/10.17605/OSF.IO/GYKUH\nGarcía-Castro, G., Álvarez-Suárez, P., García-Abad, N., Cuesta, M., and Begega, A. (June, 2017). Pro-cognitive effects of environmental enrichment on attention and spatial memory: hippocampal metabolic activity in a Wistar rat model. Poster presented at the 4th International Congress on Health and Aging Research (Murcia, Spain).\nGarcía-Abad, N., Santirso, M., García-Castro, G., and Álvarez-Suárez, P. (June, 2017). Efectos del omega-3 en las redes implicadas en la memoria de trabajo espacial en ratas Wistar. Poster presented at the 3rd National Congress of Psychology (Oviedo, Spain)\n\n\nScientific dissemination\nZacharaki, K., García-Castro, G. (2019, October). “Descubriendo la mente de los bebés” Talk presented at 13a Festa de la Ciència, Barcelona, Spain. [Link] [Video]\nGarcía-Castro, G., Avila-Varela (2019, October). “Estudiando la mente de los bebés: Desde el lenguaje hasta la lógica” Talk presented at Centre Cívic, Barcelona, Spain. [Link] [Video])\n\n\nRepositories\nGarcía-Castro, G., Santolin, C., Marimon, M., and Sebastian-Galles, N. (2019, October 22th). Segmentation: Catalan and Spanish natural speech segmentation at 8 months of age. https://doi.org/10.17605/OSF.IO/42GUP)\nSantolin, C., García-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 5). Flip: Experience with research paradigms relates to infants’ direction of preference. https://doi.org/10.17605/OSF.IO/G95UB"
  },
  {
    "objectID": "cv.html#others",
    "href": "cv.html#others",
    "title": "CV",
    "section": "Others",
    "text": "Others\n\nAnimal research\nResearch Staff Certificate for animal research, issued on 15/05/2018 by the Direcció General de Polítiques Ambientals i Medi Natural.\n\n\nSkills\n\nData processing: I particularly enjoy wrangling my way through messy data using the tidyverse family of packages. with some time, I can also google my way through Python.\nData analysis: Linear mixed models using R, both frequentist (lme4) and Bayesian (brms); reproducible reports using RMarkdown. I can also perform acoustic analysis on stimuli using Praat, and one of its R interfaces (PraatR).\nData visualisation using ggplot2. I can make animations using gganimate, and I’m currently learning to make maps, and Shiny Apps.\nDesigning experiments and questionnaires: I can program lab-based experiments using Matlab (PsychToolbox-3) and Python (Psychopy), online experiments using the PsychoPy/PsychoJS/Pavlovia workflow, and design reproducible online questionnaires using formR.\nGit/GitHub/Bitbucket\nJASP/SPSS"
  },
  {
    "objectID": "cv.html#languages",
    "href": "cv.html#languages",
    "title": "CV",
    "section": "Languages",
    "text": "Languages\n\nSpanish (native speaker)\nEnglish: C1\nFrench: C1"
  },
  {
    "objectID": "cv.html#short-courses",
    "href": "cv.html#short-courses",
    "title": "CV",
    "section": "Short courses",
    "text": "Short courses\nAugust 2018\n\nData Science: Multiple imputation in practice, Utrecht Summer School, Utrecht University. Introduction to Multiple Imputation as a powerful tool for dealing with missing data in experimental studies.\n\nAugust 2018\n\nData Science: Statistical Programming with R, Utrecht Summer School, Utrecht University.\n\nAugust 2016\n\nIntroduction to Neuroscience: From Molecule to Behavior, Radboud Summer School, Radboud University Nijmegen. Hands-on training at research labs of the Donders Institute for Brain and Cognition.\n\nAugust 2016\n\nHow to Improve the Quality and Translatability of Preclinical Animal Studies, Radboud Summer School, Radboud University Nijmegen. Course organized by SYRCLE institute, Radboud University Nijmegen and Radboud University Medical Centre. Theory and hands-on experience on Systematic Reviews and Meta-Analysis in preclinical animal studies. Overview of the main software available to conduct a Meta-Analysis and basic formation on academic reading and writing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language, reproducibility, and R",
    "section": "",
    "text": "renv (o cómo usar paquetes de R sin ataques de pánico)\n\n\n\n\n\nrenv es un paquete de R que permite instalar paquetes de R gestionar sus versiones para proyectos de forma independiente. Aqui resumo para qué se utiliza y cómo funciona.\n\n\n\n\n\n\nFeb 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\n@psicotuiterbot: Un bot de Twitter para Psicotuiter\n\n\n\n\n\nHe creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON.\n\n\n\n\n\n\nDec 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow similar is the word “mask” across languages?\n\n\n\n\n\nUsing the Levenshtein distance to quantify the orthographic and phonlogical similarity between translation equivalents of the word mask across multiple languages.\n\n\n\n\n\n\nNov 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nImporting data from multiple files simultaneously in R\n\n\n\n\n\nA comparison between base R and Tidyverse methods for importing data from multiple files\n\n\n\n\n\n\nJul 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nA primer on Mixed-Effects Models: Theory and practice\n\n\n\n\n\nSlides from a tutorial on mixed-effects models I presented to my research group.\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#tldr",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#tldr",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "TLDR",
    "text": "TLDR\n\nWe need to import several CSV or TXT files and merge them into one data frame in R. Regardless of what function we use to import the files, vectorising the operation using purrr::map in combination with do.call or dplyr::bind_rows is the most time-efficient method (~25 ms importing 50 files with 10,000 rows each), compared to for loops (~220ms) or using lapply (~123 ms). data.table::fread is the fastest function for importing data. Importing TXT files is slightly faster than importing CSV files."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Why this post",
    "text": "Why this post\nTo analyse data in any programming environment, one must first import some data. Sometimes, the data we want to analyse are distributed across several files in the same folder. I work with eye-tracking data from toddlers. This means that I work with multiple files that have many rows. At 120 Hz sampling frequency, we take ~8.33 samples per second. A session for one participants can take up to 10 minutes. So these files are somewhat big. These data also tend to be messy, requiring a lot of preprocessing. This means that I need to import the same large files many times during the same R session when wrangling my way through the data, which takes a few seconds. After some iterations, it can be annoying. I have decided to invest all my lost time into analysing what method for importing and merging large files is the fastest in R so that the universe and I are even again.\nBelow I provide several options for importing data from the different files, using base R and tidyverse, among other tools. I will compare how long it takes to import and merge data using each method under different circumstances. You can find the whole code here in case you want to take a look, reproduce it or play with it1."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "How can I import large files and merge them?",
    "text": "How can I import large files and merge them?\nSo we have some files in a folder. All files have the same number of columns, the same column names, and are in the same format. I assume that data are tabular (i.e., in the shape of a rectangle defined by rows and columns). I also assume that data are stored as Comma-Separated Values (.csv) or Tab-separated Text (.txt or .tsv), as these formats are the most reproducible.\nWe to import all files and bind their rows together to form a unique long data frame. There are multiple combinations of functions we can use. Each function comes with a different package and does the job in different ways. Next, I will show some suggestions, but first let’s create some data. We are creating 50 datasets with 10 columns and 10,000 rows in .txt format. The variables included are numeric and made of 0s and 1s. There is also a column that identifies the data set. These files are created in a temporary directory using the temp.dir function for reproducibility. After closing you R session, this directory and all of its contents will disappear.\n\n\n\n\nBase R: for loops\nfor loops are one of the fundamental skills in many programming languages. The idea behind for loops is quite intuitive: take a vector or list of length n, and apply a series of functions to each element in order. First, to element 1 and then to element 2, and so on, until we get to element n. Then, the loop ends. We will first make a vector with the paths of our files, and then apply the read.delim function to each element of the vector (i.e., to each path). Every time we import a file, we store the resulting data frame as an element of a list. After the loop finishes, we merge the rows of all element of the list using a combination of the functions do.call and rbind.\n\n\n\n\n\nBase R: lapply\nWe will use the functions read.delim and read.csv in combination with the function lapply. The former are well known. The later is part of a family of functions (together with sapply, mapply, and some others I can’t remember) that take two arguments: a list and a function, which will be applied over each element of the list in parallel (i.e., in a vectorised way).\n\n\n\n\n\nTidyverse\nThe tidyverse is a family of packages that suggests a workflow when working in R. The use of pipes (%>%) is one of its signature moves, which allow you to chain several operations applied on the same object within the same block of code. In contrast, base R makes you choose between applying several functions to the same object in different blocks of code, or applying those functions in a nested way, so that the first functions you read are those applied the last to your object (e.g., do.call(rbind, as.list(data.frame(x = \"this is annoying\", y = 1:100)))). We will use a combination of the dplyr and purrr packages to import the files listed in a vector, using read.delim and bind_rows.\n\n\n\n\n\ndata.table\nThe function rbindlist function from the package data.table also allows to merge the datasets contained in a list. In combination with fread (from the same package), it can be very fast."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "What method is the fastest?",
    "text": "What method is the fastest?\nI will compare how long each combination of importing, vectorising, and merging functions needs to import 50 data sets with 10 columns and 10,000 rows each. Additionally, I will compare the performance of each method when working with CSV (.csv) and TSV (.txt) files. For each method, I will repeat the process 100 times, measuring how long it takes from the moment we list the extant files in the folder to the moment we finish merging the data sets. Here are the results:\n\n\n\n\n\nFigure 1: Mean time (and standard deviation) for each combination of methods and file formats across 100 replications\n\n\n\n\nFor more detail:\n\n\n\n\n\n\nTable 1:  Execution times \n  \n    \n      Time taken to import and merge\n    \n    \n      50 datasets with 10 columns and 10,000 rows each\n    \n  \n  \n    \n      \n      package\n      \n        for loop\n      \n      \n        lapply\n      \n      \n        purrr::map\n      \n    \n    \n      M\n      SD\n      M\n      SD\n      M\n      SD\n    \n  \n  \n    \n      do.call - .csv\n    \n    \nbase\n1.34\n0.06\n1.11\n0.07\n0.15\n0.03\n    \ndata.table\n1.33\n0.39\n0.88\n0.04\n0.17\n0.02\n    \nreadr\n1.38\n0.05\n1.07\n0.13\n0.15\n0.02\n    \n      dplyr::bind_rows - .csv\n    \n    \nbase\n1.35\n0.16\n0.99\n0.14\n0.15\n0.01\n    \ndata.table\n1.14\n0.04\n0.84\n0.06\n0.16\n0.01\n    \nreadr\n1.25\n0.14\n0.91\n0.04\n0.14\n0.00\n    \n      do.call - .txt\n    \n    \nbase\n1.18\n0.04\n0.88\n0.04\n0.17\n0.02\n    \ndata.table\n1.17\n0.04\n0.80\n0.05\n0.15\n0.02\n    \nreadr\n1.18\n0.05\n0.80\n0.05\n0.15\n0.02\n    \n      dplyr::bind_rows - .txt\n    \n    \nbase\n1.13\n0.03\n0.84\n0.06\n0.20\n0.02\n    \ndata.table\n1.19\n0.13\n0.77\n0.03\n0.14\n0.01\n    \nreadr\n1.13\n0.04\n0.77\n0.03\n0.14\n0.01\n    Mean\n—\n1.23\n0.10\n0.89\n0.06\n0.15\n0.02\n  \n  \n  \n\n\n\n\n\nFigure 1 and Table Table 1 show the detailed timings The grand mean average time taken by all methods is ~2.12 seconds, but there are some differences.\n\nIt doesn’t really matter what function we use to merge data sets: both do.call and dplyr::bind_rows perform roughly similarly.\nWhat makes the biggest difference is what function we use to vectorise the importing operation across file names to import them. purrr::map is the fastest. Incredibly, is takes less than 0.3 seconds in all conditions. It is also the least sensitive to the format of the files and the function we use to import them.\nThe next vectorising function in terms of temporal efficiency is lapply, which takes ~1.5 seconds. It performs slightly better when working with .txt files, in that when working with .csv files its performance depends on what method we use to import them: data.table::fread is much faster than its base and readr competitors. This post by Daniele Cook sheds some light into the advantage of data.table over other importing functions, also covering the vroom package, which this post doesn’t cover.\nUsing for loops looks like the least efficient method for iterating across data sets when importing data. It also shows a similar profile than lapply: data.table::fread performs a bit better than the rest."
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#conclusion",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#conclusion",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Conclusion",
    "text": "Conclusion\nUnder the scenario under which I have simulated the data, it seems that using purrr::map in combination with do.call or dplyr::bind_rows to merge data sets is the most efficient method in terms of time. When using said combination, it doesn’t matter what function we use to import files, but data.table::fread seems like the best choice, as it is also the most flexible (take a look at the documentation of data.table to see all the features it offers).\nIf I have time, I may add another two dimensions: number of rows in the files and number of files, although I dare say similar results are to be expected. If anything, I would say that differences may become greater as file size and number of files increase. Also, it would be interesting to test if pre-allocating the elements of the vector in the for loop speeds up the process (see here what I mean). We shall see.\nHope this was useful, if not interesting!"
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#code",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#code",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "posts/importing-data-multiple/importing-data-multiple.html#session-info",
    "href": "posts/importing-data-multiple/importing-data-multiple.html#session-info",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22000)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n[3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=Spanish_Spain.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.8.0          ggsci_2.9         forcats_0.5.2     stringr_1.5.0    \n [5] readr_2.1.3       tidyr_1.2.1       tibble_3.1.8      ggplot2_3.4.0    \n [9] tidyverse_1.3.2   data.table_1.14.6 purrr_1.0.0       dplyr_1.0.10     \n[13] quarto_1.2       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9          lubridate_1.9.0     ps_1.7.2           \n [4] assertthat_0.2.1    digest_0.6.31       utf8_1.2.2         \n [7] R6_2.5.1            cellranger_1.1.0    backports_1.4.1    \n[10] reprex_2.0.2        evaluate_0.19       httr_1.4.4         \n[13] pillar_1.8.1        rlang_1.0.6         googlesheets4_1.0.1\n[16] readxl_1.4.1        rstudioapi_0.14     rmarkdown_2.19     \n[19] labeling_0.4.2      googledrive_2.0.0   munsell_0.5.0      \n[22] broom_1.0.2         compiler_4.2.2      modelr_0.1.10      \n[25] xfun_0.36           pkgconfig_2.0.3     htmltools_0.5.4    \n[28] tidyselect_1.2.0    fansi_1.0.3         crayon_1.5.2       \n[31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n[34] later_1.3.0         commonmark_1.8.1    grid_4.2.2         \n[37] jsonlite_1.8.4      gtable_0.3.1        lifecycle_1.0.3    \n[40] DBI_1.1.3           magrittr_2.0.3      scales_1.2.1       \n[43] cli_3.5.0           stringi_1.7.8       farver_2.1.1       \n[46] renv_0.15.4         fs_1.5.2            xml2_1.3.3         \n[49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.5.1        \n[52] tools_4.2.2         glue_1.6.2          hms_1.1.2          \n[55] processx_3.8.0      fastmap_1.1.0       yaml_2.3.6         \n[58] timechange_0.1.1    colorspace_2.0-3    gargle_1.2.1       \n[61] rvest_1.0.3         knitr_1.41          haven_2.5.1        \n[64] sass_0.4.4"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "title": "How similar is the word “mask” across languages?",
    "section": "",
    "text": "The ubiquity of masks has given psycholinguists a frequent-ish stimulus to use in experiments. This word is more form-similar across languages than one may think. I gathered a big-ish dataset with translation equivalents of the word mask across ~110 languages. I tweeted about this today, and wanted to dedicate some more lines to nuance.\nHere’s the data:\nTo compute the similarity of each pair of translation equivalents, I followed Floccia et al.’s (2018) procedure. For each pair of translation equivalents, I computed their Levenshtein distance as the number of insertions, deletions and replacements a string character has to go through to become identical to the other, and then divided this value by the number of characters of the longest of the two strings, so that all values range between 0 and 1. To compute the Levenshtein distance, I used the stringdist() function of the stringdist R package."
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Orthographic distance",
    "text": "Orthographic distance\nI first computed the orthographic distance between each pair of translation equivalents. Since some word forms make use of different alphabets, I first romanised all word forms. By romanised, I mean that I searched for the transcription of each word form in the Roman alphabet, and used it as input to compute the Levenshtein distance for each pair of translation equivalents. Here’s how orthographically similar (the romanisations of) the translations of mask are (N = 110 pairs):"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "title": "How similar is the word “mask” across languages?",
    "section": "Phonological distance",
    "text": "Phonological distance\nThe phonological similarity/distance may be more informative. This time I searched for or generated with the help of a native speaker a phonological IPA transcription of each word-form. I then used this transcription as input to compute the phonological similarity of each pair of translation equivalents. A pitfall in this process is the fact that phonemes are almost never identical across languages, so even the common phoneme /m/ could vary slightly on its pronunciation in two languages. If this difference is encoded in the IPA transcription (as different characters), the Levenshtein distance will be inflated. For this reason, I simplified some IPA transcriptions to preserve this similarity. I also removed tones. This is terribly wrong from a linguistics perspective, but it’s the only way I see to be able to play with some reliable data. Also I’m no linguist, so you have no power here.\n\nHere’s the same analysis performed on phonological transcriptions of a subset of those languages (N = 75 pairs, those I could find a reliable IPA transcription for or could find help from a native speaker):"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "title": "How similar is the word “mask” across languages?",
    "section": "Onsets",
    "text": "Onsets\nMost of the times, the phonological overlap comes from onset graphemes/phonemes. This is how many word-forms start with each onset:"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "title": "How similar is the word “mask” across languages?",
    "section": "Some disclaimers:",
    "text": "Some disclaimers:\nI tried ensuring that words referred to surgical masks (instead of other types of masks) with help from native speakers. Wrong translations may still have slipped in (or be just wrong). I wish I had time to double-check all of them (I did this for fun).\nThis analysis is probably affected by selection bias. I suspect many dissimilar translations are missing due to not being included in the translation apps I used (e.g. Google Translate). Feel free to contribute missing entries or make corrections!"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "title": "How similar is the word “mask” across languages?",
    "section": "Code and data",
    "text": "Code and data"
  },
  {
    "objectID": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "href": "posts/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "title": "How similar is the word “mask” across languages?",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] htmltools_0.5.4  kableExtra_1.3.4 knitr_1.41       gt_0.8.0        \n [5] readxl_1.4.1     forcats_0.5.2    stringr_1.5.0    dplyr_1.0.10    \n [9] purrr_1.0.0      readr_2.1.3      tidyr_1.2.1      tibble_3.1.8    \n[13] ggplot2_3.4.0    tidyverse_1.3.2  quarto_1.2      \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.4          sass_0.4.4          jsonlite_1.8.4     \n [4] viridisLite_0.4.1   modelr_0.1.10       assertthat_0.2.1   \n [7] renv_0.16.0         googlesheets4_1.0.1 cellranger_1.1.0   \n[10] yaml_2.3.6          pillar_1.8.1        backports_1.4.1    \n[13] glue_1.6.2          digest_0.6.31       rvest_1.0.3        \n[16] colorspace_2.0-3    pkgconfig_2.0.3     broom_1.0.2        \n[19] haven_2.5.1         scales_1.2.1        webshot_0.5.4      \n[22] processx_3.8.0      svglite_2.1.0       stringdist_0.9.10  \n[25] later_1.3.0         tzdb_0.3.0          timechange_0.1.1   \n[28] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[31] withr_2.5.0         cli_3.6.0           magrittr_2.0.3     \n[34] crayon_1.5.2        evaluate_0.19       ps_1.7.2           \n[37] fs_1.5.2            fansi_1.0.3         xml2_1.3.3         \n[40] tools_4.2.2         hms_1.1.2           gargle_1.2.1       \n[43] lifecycle_1.0.3     munsell_0.5.0       reprex_2.0.2       \n[46] compiler_4.2.2      systemfonts_1.0.4   rlang_1.0.6        \n[49] grid_4.2.2          rstudioapi_0.14     rmarkdown_2.19     \n[52] gtable_0.3.1        DBI_1.1.3           R6_2.5.1           \n[55] lubridate_1.9.0     fastmap_1.1.0       utf8_1.2.2         \n[58] commonmark_1.8.1    stringi_1.7.8       parallel_4.2.2     \n[61] Rcpp_1.0.9          vctrs_0.5.1         dbplyr_2.2.1       \n[64] tidyselect_1.2.0    xfun_0.36"
  },
  {
    "objectID": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "href": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "title": "A primer on Mixed-Effects Models: Theory and practice",
    "section": "",
    "text": "When I started my PhD I had to get familiar with Linear Mixed Models very well very quickly. Then I was asked to present what I learnt and prepare this informal tutorial for my colleagues, which I presented on March 10th, 2020 (yes, early pandemic :confounded:). This post shares the result.\nThis is not supposed to be taken as a formal guide to linear mixed-effects models, but rather as a semi-coherent compilation of notes and self-suggestions that I considered worth sharing with my lab mates during my early stages of in the PhD. Here are the slides:\nWhat I should be taken more seriously are (1) the references I suggest in the first slides (which I consider some of the best resources available to learn linear mixed-effects models), and (2) the memes, which I personally curated and even created to sweeten up the dreadful incoherence of the content of some of the slides (sorry about that).\nBefore the presentation I tweeted one of the animations I generated for it.\nThis tweet got some attention (for my usual numbers) and many kind folks have asked for the R code or the GIF file of the specific animation included in the tweet, so here they are (you’ll also find them in the GitHub repository) in a perhaps more comfortable format, ready to be cloned or downloaded):"
  },
  {
    "objectID": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "href": "posts/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "title": "A primer on Mixed-Effects Models: Theory and practice",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] xaringanExtra_0.7.0 quarto_1.2         \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9      ps_1.7.2        digest_0.6.31   later_1.3.0    \n [5] lifecycle_1.0.3 jsonlite_1.8.4  magrittr_2.0.3  evaluate_0.19  \n [9] stringi_1.7.8   rlang_1.0.6     cli_3.6.0       renv_0.16.0    \n[13] rstudioapi_0.14 vctrs_0.5.1     rmarkdown_2.19  tools_4.2.2    \n[17] stringr_1.5.0   glue_1.6.2      compiler_4.2.2  xfun_0.36      \n[21] yaml_2.3.6      fastmap_1.1.0   processx_3.8.0  htmltools_0.5.4\n[25] knitr_1.41"
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html",
    "href": "posts/psicotuiterbot/psicotuiterbot.html",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "",
    "text": "He creado un bot de Twitter que hace RT a cualquier mención a #psicotuiter. El código está escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y está alojado en una Raspberry Pi que hace las veces de servidor ejecutando el código cada 15 minutos usando CRON."
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "href": "posts/psicotuiterbot/psicotuiterbot.html#escribiendo-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Escribiendo el código",
    "text": "Escribiendo el código\nUn bot de Twitter no es más que un código que se ejecuta automáticamente de forma periódica (ej., cada 15 minutos) y realiza una acción en Twitter a través de una cuenta (hacer un RT o responder a un tweet). Para poder realizar esta acción a través del código, es necesario tener acceso a la API de Twitter. API es un acrónimo para Application Programming Interface y como dice su nombre, es una plataforma desde la que podemos interactuar con una aplicación (en este caso Twitter) a través de programación con una serie de comandos que el equipo de Twitter ha diseñado la API ha definido.\nNunca había hecho un bot de Twitter, pero sabía de la existencia de bastantes tutoriales para hacerlo. La mayoría de los bots de Twitter (y por tanto de los tutoriales) están escritos en Python y JavaScript, pero yo me encuentro algo más cómodo con R. Además gran parte de Psicotuiter (especialmente quienes están relacionades con la metodología) también está más familiarizado con R. Mi intención era hacer el bot lo más trasparente y accesible para la comunidad, así que me decanté por R. En un pricipio seguí el tutorial de Matt Dray, en el que utiliza el paquete de R {rtweet} para interactuar con la API de Twitter. Hay más tutoriales que usan rtweet para crear un bot con R. Pero a diferencia de otros, este tutorial explicaba cómo usar GitHub actions para ejecutar el código de forma periódica. Ahora explico esto último. Mientras tanto, vamos al código de R.\nPrimero creé un repositorio de GitHub donde alojar el código (GitHub es como un Google Drive especializado en código donde además podemos hacer control de versiones de los archivos que subimos). Puedes echar un vistazo al código de R en la carpeta R/. El código principalmente recoge los últimos tweets que se han escrito en las últimos 6 horas mencionando #psicotuiter o #psicotwitter y los retuitea. No me meteré en detalle a explicar cómo funciona el código, pero aquí va un pequeño resumen del programa. Si tienes curiosidad te recomiendo explorar el repositorio de GitHub, que contiene todo lo necesario para hacer funcionar el bot:\nPrimero cargamos el paquete de R {dplyr}, que utilizamos en bastantes ocasiones en el programa, ajustamos un pequeño detalle relacionado con el comportamiento de la API de Twitter, e instalamos los paquetes necesarios (en caso de que hayan cambiado) usando el pqeute de R {renv} (si no lo conoces y te interesa la reproducibilidad computacional tienes que echarle un vistazo).\n# bot R code\nlibrary(dplyr)\noptions(httr_oob_default = TRUE)\n# restore packages\nrenv::restore()\nA continuación extraemos las credenciales que necesitamos para acceder a la API de Twitter (las paso a como variables de entorno para evitar hacerlas públicas, ya que eso dería acceso a cualquiera a la cuenta de Twitter del bot).\n# authenticate Twitter API\nmy_token <- rtweet::create_token(\n    app = \"psicotuiterbot\",  # the name of the Twitter app\n    consumer_key = Sys.getenv(\"TWITTER_CONSUMER_API_KEY\"),\n    consumer_secret = Sys.getenv(\"TWITTER_CONSUMER_API_KEY_SECRET\"),\n    access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n    access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"), \n    set_renv = FALSE\n)\nLuego extraemos los tweets usando rtweet, filtramos los que sean relevantes y no contengan posible contenido ofensivo (y algún otro filtro más).\nlibrary(dplyr)\n# define hashtags\nhashtags_vct <- c(\"#psicotuiter\", \"#psicotwitter\", \"#Psicotuiter\", \"#Psicotwitter\", \"#PsicoTuiter\", \"#PsicoTwitter\")\nhashtags <- paste(hashtags_vct, collapse = \" OR \")\nhate_words <- unlist(strsplit(Sys.getenv(\"HATE_WORDS\"), \" \")) # words banned from psicotuiterbot (separated by a space)\nblocked_accounts <- unlist(strsplit(Sys.getenv(\"BLOCKED_ACCOUNTS\"), \" \")) # accounts banned from psicotuiterbot (separated by a space)\ntime_interval <- lubridate::now(tzone = \"UCT\")-lubridate::minutes(120)\n# get mentions to #psicotuiter and others\nall_tweets <- rtweet::search_tweets(\n    hashtags, \n    type = \"recent\", \n    token = my_token, \n    include_rts = FALSE, \n    tzone = \"CET\"\n) \nstatus_ids <- all_tweets %>% \n    filter(\n        !(screen_name %in% gsub(\"@\", \"\", blocked_accounts)),\n        created_at >= time_interval, # 15 min\n        !grepl(paste(hate_words, collapse = \"|\"), text), # filter out hate words\n        stringr::str_count(text, \"#\") < 4, # no more than 3 hashtags\n        lang %in% c(\"es\", \"und\") # in Spanish or undefined language\n    ) %>% \n    pull(status_id)\n# get request ID\nrequest_tweets <- rtweet::get_mentions(\n    token = my_token, \n    tzone = \"CET\"\n) \nFinalmente, hacemos RT uno a uno usando rtweet (si ya habíamos hecho RT a uno de ellos simplemente se ignora).\nif (nrow(request_tweets) > 0) {\n    request_ids <- request_tweets %>% \n        filter(\n            created_at >= time_interval, # 15 min\n            grepl(\"@psicotuiterbot\", text),\n            grepl(\"rt|RT|Rt\", text),\n            !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n        ) %>% \n        pull(status_in_reply_to_status_id)\n    \n    # get requested IDS\n    if (length(request_ids) > 0) {\n        requested_ids <- rtweet::lookup_statuses(request_ids, token = my_token) %>% \n            filter(\n                !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n            ) %>% \n            pull(status_id)\n    } else {\n        requested_ids <- NULL\n    }\n} else {\n    requested_ids <- NULL\n}\n# RT all IDs\nif (length(status_ids) > 0){\n    for (i in 1:length(status_ids)){\n        rtweet::post_tweet(\n            retweet_id = unique(status_ids)[i], # vector with IDs\n            token = my_token\n        )\n    }\n    print(paste0(length(status_ids), \" RT(s): \", paste(status_ids, collapse = \", \")))\n} else {\n    print(\"No tweets to RT\")\n}\nRecientemente incluí un pequeño bloque de código para permitir que la gente solicitase un RT para tuits que no mencionaban #psicotuiter, pero podrían ser de interés para la comunidad. Para hacerlo solo hay que responder al tuit en cuestión mencionando a @psicotuiterbot junto con la palabra RT (ej., “RT @psicotuiterbot por favor”).\n# tweet requests\nif (length(requested_ids) > 0){\n    for (i in 1:length(requested_ids)){\n        rtweet::post_tweet(\n            retweet_id = unique(requested_ids)[i], # vector with IDs\n            token = my_token\n        )\n    }\n    print(paste0(length(requested_ids), \" request(s) posted: \", paste(requested_ids, collapse = \", \")))\n} else {\n    print(\"No requests\")\n}"
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html#ejecutando-el-código",
    "href": "posts/psicotuiterbot/psicotuiterbot.html#ejecutando-el-código",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Ejecutando el código",
    "text": "Ejecutando el código\nLo que hace “bot” a un bot es que no requiere intervención manual para que realice la acción que deseamos. Hay muchas opciones para conseguir esto, pero casi todas tienen un inconveniente: necesitamos que una máquina (ordenador/servidor, móvil, etc.) esté encendido en el momento en el que queremos ejecutar nuestro código. En nuestro caso necesitamos que se ejecute cada 15 minutos, lo que implica que deberíamos tener un dispositivo conectado a la corrriente y funcionando todo el día. Tener mi ordenador personal haciendo esto no es una opción viable. Me encontré con dos alternativas.\n\nPrimer intento (sale mal): GitHub actions\nComo mencioné antes, en el tutorial de Matt Dray se ilustra cómo usar GitHub Actions para ejecutar nuestro código una vez está alojado en un repositorio de GitHub. GitHub Actions es un servicio que ofrece GitHub que permite ejecutar ciertos comandos en determinadas condiciones o cada cierto tiempo usando un servidor que ponen a nuestra disposición (con ciertos límites). Este grupo de comandos se denominan workflows o flujos de trabajo, y si los incluimos en una carpeta de nuestro repositorio llamada .github/workflows/ siguiendo cierto formato en un archivo YAML (.yml), GitHub se encargará de ejecutarlo sin nuestra intervención. Hay buenos tutoriales sobre cómo y cuándo escribir workflows para GutHub Actions. El workflow principal era inicialmente este:\nname: bot\non:\n  push:\n    branches:\n      - main # run every time there is a push to main branch\n      - test\njobs:\n  psicotuiterbot-post:\n    runs-on: macOS-latest\n    env: #  twitter API keys (used to authenticate) defined in the gh actions environment\n      TWITTER_CONSUMER_API_KEY: ${{ secrets.TWITTER_CONSUMER_API_KEY }}\n      TWITTER_CONSUMER_API_KEY_SECRET: ${{ secrets.TWITTER_CONSUMER_API_KEY_SECRET }}\n      TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}\n      TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}\n      \n    steps:\n      - uses: actions/checkout@v2\n      - uses: r-lib/actions/setup-r@v1\n      - uses: r-lib/actions/setup-renv@v1\n        with:\n          cache-version: 1\n      - name: Restore packages using renv\n        shell: Rscript {0}\n        run: |\n          if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::restore()\n      - name: Create and post tweet\n        run: Rscript R/bot.R\nLa gran ventaja de usar este sistema es que no necesitamos usar nuestro ordenador personal, ya que usamos el que que GitHub nos asigna (un servidor no deja de ser un ordenador). Pero tiene varios inconvenientes. El primero es que el proceso de establecer un workflow en GitHub Actions suele requerir varios intentos (en mi caso muchos). Esto suele deberse a problemas de reproducibilidad computacional: el código funciona correctamente en mi ordenador porque en él tengo instalado todo el sofware del que depende. Cuando uso el servidor de GitHub, el sistema operativo suele necesitar que instalemos estas dependencias antes de ejecutar el código. GitHub Actions permite cierta flexibilidad a la hora de seleccionar el software que viene instalado en el sistema operativo que vamos a usar (ej., R, compiladores de C++, dependencias de Linux, etc.). El problema es que muchas veces ni siquiera somos conscientes de cuántas dependencias requiere nuestro código. Con paciencia y muchas búsquedas de Google es posible solventar este problema.\nUn segundo inconveniente que encontré a la hora de implementar el bot en GitHub Actions tiene que ver con los tiempos: instalar todas las dependencias del código en el servidor cada 15 minutos (la configuración se pierde casi totalmente tras cada ejecución) es poco eficiente. Instalar las dependencias puede tardar más de 10 minutos (en el caso de este bot). Esto puede además hacer fallar en ocasiones el flujo de trabajo. GitHub Actions tampoco es lo más consistente del mundo, aunque no deja de ser gratis."
  },
  {
    "objectID": "posts/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "href": "posts/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Segundo intento (sale bien): Raspberry Pi",
    "text": "Segundo intento (sale bien): Raspberry Pi\nTras varios problemas en la ejecución del bot a través de GitHub Actions, decidí cambiar de método. Por razones ajenas al bot, hacía unos meses que tenía muerta de aburrimiento una Raspberry Pi 4 que compré con un amigo para jugar con ella. Este dispositivo es un mini-ordenador relativamente barato (~40€) que salió al mercado como herramienta educativa para enseñar a programar (ej., róbotica para niñes) pero que poco a poco ha ido tomando espacio en lugares de producción. Tiene mil posibilidades por su simplicidad y, en nuestro caso, por su bajo consumo: tener una Raspberry Pi funcionando todo el día apenas tiene impacto sobre el consumo de luz.\n\n\n\nAquí está alojado el @psicotuiterbot\n\n\nPrimero instalé el código en la Raspberry con sus dependencias: básicamente, cloné el repositorio de GitHub en una carpeta dentro de home/Documents/. Para ejecutar el código cada 15 minutos utilicé una función muy útil que incluye Linux (sistema operativo con el que funciona la Raspberry) llamado CRON. Simplemente consiste en un archivo en el que incluimos una serie de comandos que queremos que se ejecuten de forma periódica, junto con una código que indica la periodicidad de la ejcución de este comando. Aquí tienes unos ejemplos. Incluí cuatro comandos (cada uno en su propio archivo con extensión .sh, que denota comandos de Linux):\n# descarga el código de GitHub, por si ha habido cambios\ngit pull origin main\n# ejecuta el código principal del bot\nTZ=\"Spain/Madrid\" Rscript -e 'source(\"R/bot.R\")'\n# guarda los tweets detectados en un archivo y crea un gráfico\nRscript -e 'source(\"R/counts.R\")'\nrm Rplots.pdf\n# sube los nuevos datos a GitHub\ngit add .\ngit commit -m \"Update repository\"\ngit push\nEstos comandos se ejecutan en este orden cada 15 minutos."
  },
  {
    "objectID": "posts/renv-package/renv-package.html",
    "href": "posts/renv-package/renv-package.html",
    "title": "renv (o cómo usar paquetes de R sin ataques de pánico)",
    "section": "",
    "text": "A veces necesitamos instalar versiones diferentes del mismo paquete de R en proyectos diferentes. El paquete {renv} nos permite almacenar los paquetes de R de cada proyecto de forma independiente, evitando posibles conflictos entre proyectos. De paso, incrementará la reproducibilidad computacional de nuestro código.\n\n\n\nEl problema\nPonte en la siguiente situación: tienes entre manos un proyecto de R que necesita varios paquetes. Cada uno de estos paquetes depende, a su vez, de terceros paquetes. De hecho, dos paquetes pueden depender del mismo paquete, o incluso de versiones diferentes del mismo paquete. Si hay mala suerte, una de las versiones no será lo suficientemente reciente como para funcionar correctamente con ambos paquetes. Resultado: uno de los dos paquetes no funcionará.\n\n\n\nUsuarie de R promedio después de tirar dos horas a la basura intentando instalar los paquetes que necesita para trabajar en un proyecto de R que no tocaba desde hacía cuatro meses.\n\n\nEste problema se extiende al caso de que necesitemos versiones diferentes del mismo paquete en proyectos de R diferentes en los que estamos trabajando de forma simultánea en el mismo ordenador. Para hacerlos funcionar necesitaríamos instalar de nuevo la versión correspondiente del mismo paquete cada vez que cambiemos de proyecto.\n\n\nInstalando paquetes de R\nEn resumidas cuentas, cada vez que instalamos o actualizamos un paquete de R, lo hacemos para todos nuestros proyectos de R de forma global. Esto se debe a que por defecto R busca todos los paquete de R en la misma carpeta. Para ver dónde instala R tus paquetes puedes ejecutar el siguiente comando:\n.libPaths()\nEste comando te mostrará el directorio o directorios donde R instala sus paquetes por defecto. Si hay más de un directorio significa que, en caso de que no sea posible encontrar un paquete en el primer directorio, R lo buscará en el segundo, tercero, etc., hasta que te devuelva un error indicando que no has instalado ese paquete.\nSi accedes al primer directorio que muestra .libPaths() verás que cada paquete tiene una carpeta. Cada carpeta incluye el código de R, los datos y la documentación asociada a cada paquete (entre otras cosas). Cada vez que instalamos o actualizamos un paquete, se crea o reemplaza su carpeta correspondiente en nuestro directorio, es decir, en nuestra “biblioteca global” de paquetes de R.\n\n\n\nAsí es como tienes tu carpeta de paquetes de R. Que lo sé yo. Que te he visto. Vergüenza me daría a mí.\n\n\nHemos visto que esto no es ideal. ¿No será mejor tener una carpeta diferente para cada proyecto en la que instalamos sus paquetes de forma independente, sin afectar a los paquetes de otros proyectos? Sí. De hecho este procedimiento es estándar en otros lenguajes de programación como JavaScript1 o Julia 2. Existe un paquete de R que nos permite hacer esto: renv. Veamos cómo funciona.\n\n\nUsando renv\nPrimero hay que instalar renv. Como está incluido en el CRAN, podemos hacerlo usando install.packages():\ninstall.packages(\"renv\")\nAhora abrimos una sesión de R en la carpeta de nuestro proyecto. Digamos que nuestra carpeta tiene la siguiente estructura:\ndata\n |-some-data.csv\ndocs\n |-index.Rmd\n |-index.html\nR\n |-main.R\n |-functions.R\n.Rprofile\nAsí es la estructura de la mayoría de carpetas de mis proyectos de R. Tiene su razón de ser, pero eso es material para otro post. Lo importante es cómo cambiará esta estructura en unos momentos. La documentación de renv incluye los pasos para usar renv, pero explicaré los principales. Primero inicializaremos renv en nuestra consola de R:\nrenv::init()\nEsto creará una carpeta (renv) y un archivo (renv.lock) nuevos en nuestro directorio:\n.Rprofile\ndata\n    |-some-data.csv\ndocs\n    |-index.Rmd\n    |-index.html\nR\n    |-main.R\n    |-functions.R\nrenv\n    |-.gitignore\n    |-activate.R\n    |-library\n        |-...\n    |-local\n        |-...\n    |-settings.dcf\nrenv.lock\nNo necesitaremos modificar ni consultar nunca ningunos de los archivos creados, pero vamos a curiosear un poco. Al usar init(), renv ha echado un vistazo a los scripts de R de la carpeta (achivos con la extensión .R, como main.R y functions.R), y ha detectado los paquetes que necesita nuestro código para ejecutarse (puedes consultar las dependencias de tu proyecto usando renv::dependencies()). Por ejemplo, si main.R incluye library(dplyr) o dplyr::mutate(), detectará el paquete dplyr como una dependencia.\n\n\n\nrenv detectando tus dependencias.\n\n\nA continuación, renv ha instalado todas los paquetes necesarios en renv/library/. Si comparas esa carpeta con el directorio mostrado en .libPaths() (como hicimos hace un momento), verás que ambas carpetas son muy parecidas. Eso es porque ahora R buscará los paquetes que necesites en esa carpeta, y no en la “biblioteca global” de paquetes de R. Esa es la magia de renv: podrás instalar y actualizar paquetes de R de forma independiente para cada uno de tus proyectos. Para instalar nuevos paquetes deberás hacerlo usando la función renv::install(). Por ejemplo:\nrenv::install(\"tidyr\")\nEsta función es el equivalente a install.packages() en renv. esta función asumirá que el paquete que quieres se encuentra en CRAN y será allí donde lo buscará. Si el paquete que quieres instalar se encuentra alojado en otro sitio (o quieres instalar una versión experimental del mismo, en un repositorio de GitHub, por ejemplo), puedes indicar el repositorio de la siguiente forma:\nrenv::install(\"crsh/papaja\")\nSi echas un vistazo a renv.lock verás que incluye una lista de todas las dependencias de tu proyecto, en un formato un poco raro, con muchos paréntesis, y la extensión .lock. No necesitas entender este archivo, sólo que sigue un formato parecido al que usan otros leguajes de programación para hacer lo mismo. Es el equivalente al archivo package-lock.json de un proyecto de JavaScript o al archivo Manifest.toml de un proyecto de Julia. Si te fijas, verás que simplemente incluye información mínima para cada paquete: nombre, versión, origen y un código que lo identifica. Por ejemplo, el renv.lock de nuestro proyecto incluye lo siguiente:\n{\n  \"R\": {\n    \"Version\": \"4.0.4\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"dplyr\": {\n      \"Package\": \"dplyr\",\n      \"Version\": \"1.0.8\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"ef47665e64228a17609d6df877bf86f2\"\n    },\n    \"papaja\": {\n      \"Package\": \"papaja\",\n      \"Version\": \"0.1.0.9997\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"crsh\",\n      \"RemoteRepo\": \"papaja\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a231c3628ccf24359cc17f11a5bbc743e3fed920\",\n      \"Remotes\": \"tidymodels/broom\",\n      \"Hash\": \"3df0637229690f807616c46d3ff77113\"\n    },\n    \"tidyr\": {\n      \"Package\": \"tidyr\",\n      \"Version\": \"1.2.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"d8b95b7fee945d7da6888cf7eb71a49c\"\n    },\n  }\n}\nUna ventaja enorme de usar renv es que si descargas o copias y pegas esta carpeta en un ordenador diferente (en el que posiblemente tengas una colección de paquetes diferente a la del ordenador donde trabajaste con el proyecto por última vez), renv podrá consultar este archivo para instalar por tí los paquetes necesarios en sus versiones correspondientes. Esto se puede hacer usando el comando:\nrenv::restore()\nImportante: cuando instales nuevos paquetes usando renv::install(), el archivo renv.lock no se actualizará de forma automática. Para incluir los nuevos paquetes en este archivo, tendremos que usar el siguiente comando:\nrenv::snapshot()\nComo podrás imaginar, poder instalar los paquetes que necesita un proyecto en su versión adecuada resuelve uno de los problemas más frecuentes que amenazan la reproducibilidad computacional de nuestros proyectos.\n\n\n\nImaginando un mundo donde todo el mundo se preocupa lo suficiente por la reproducibilidad computacional de sus proyectos.\n\n\n\n\nConclusiones\nTe recomiendo empezar a usar renv en algún proyecto “de juguete” con el que puedas experimentar, e ir poco a poco incorporando esta rutina en tus proyectos por el bien de tu salud mental y de la de les demás. :smile:\n\n\n\n\n\nFootnotes\n\n\nEcha un vistazo a este post de Nikola Đuza: Ride Down Into JavaScript Dependency Hell↩︎\nEcha un vistazo a este post de Bogumił Kamiński: My practices for managing project dependencies in Julia↩︎"
  }
]