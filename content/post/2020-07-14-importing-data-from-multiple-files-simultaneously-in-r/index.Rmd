---
title: Importing data from multiple files simultaneously in R
author: Gonzalo García-Castro
date: '2020-07-05'
slug: importing-data-from-multiple-files-simultaneously-in-r
categories:
  - rstats
tags:
  - rstats
header:
  image: 'header.png'
---

## TL;DR

> We need to import several CSV or TXT files and merge them into one data frame in R. Regardless of what function we use to import the files, vectorising the operation using `purrr::map` in combination with `do.call` or `dplyr::bind_rows` is the most time-efficient method (~25 ms importing 50 files with 10,000 rows each), compared to *for loops* (~220ms) or using `lapply` (~123 ms). `data.table::fread` is the fastest function for importing data. Importing TXT files is slightly faster than importing CSV files.
## Why this post

To analyse data in any programming environment, one must first import some data. Sometimes, the data we want to analyse are distributed across several files in the same folder. I work with eye-tracking data from toddlers. This means that I work with multiple files that have many rows. At 120 Hz sampling frequency, we take ~8.33 samples per second. A session for one participants can take up to 10 minutes. So these files are somewhat big. These data also tend to be messy, requiring a lot of preprocessing. This means that I need to import the same large files many times during the same R session when wrangling my way through the data, which takes a few seconds. After some iterations, it can be annoying. I have decided to invest all my lost time into analysing what method for importing and merging large files is the fastest in R so that the universe and I are even again.

Below I provide several options for importing data from the different files, using base R and tidyverse, among other tools. I will compare how long it takes to import and merge data using each method under different circumstances. You can find the whole code here in case you want to take a look, reproduce it or play with it^[Ironically, this code is super inefficient and messy. It takes ages to run, and has been written by copy-pasting multiple times. I didn't feel like doing anything more elegant. Also, I don’t know how. Help yourself.].

## How can I import large files and merge them?

So we have some files in a folder. All files have the same number of columns, the same column names, and are in the same format. I assume that data are tabular (i.e., in the shape of a rectangle defined by rows and columns). I also assume that data are stored as Comma-Separated Values (.csv) or Tab-separated Text (.txt or .tsv), as these formats are the most reproducible.

We to import all files and bind their rows together to form a unique long data frame. There are multiple combinations of functions we can use. Each function comes with a different package and does the job in different ways. Next, I will show some suggestions, but first let's create some data. We are creating 50 datasets with 10 columns and 10,000 rows in .txt format. The variables included are numeric and made of 0s and 1s. There is also a column that identifies the data set. These files are created in a temporary directory using the `temp.dir` function for reproducibility. After closing you R session, this directory and all of its contents will disappear.

```{r echo=TRUE, message=FALSE, results="hide"}
n <- 50 # number of files/participants
n_obs <- 10000 # number of rows/observations in each file
filenames <- sprintf("dataset%03d", 1:n) # file names

files <- lapply(
  as.list(filenames),
  function(x) {
    data.frame(dataset = x, replicate(10, sample(0:1, n_obs, rep = TRUE))) # create data sets
  }
)
# export data sets as files
dat <- mapply(
  function(x, y){
    write.table(x, paste0(tempdir(), .Platform$file.sep, y,  ".txt"), sep = "\t", dec = ".", row.names = FALSE)
  },
  files, filenames
)
```

### Base R: for loops

*for loops* are one of the fundamental skills in many programming languages. The idea behind *for loops* is quite intuitive: take a vector or list of length *n*, and apply a series of functions to each element in order. First, to element 1 and then to element 2, and so on, until we get to element *n*. Then, the loop ends. We will first make a vector with the paths of our files, and then apply the `read.delim` function to each element of the vector (i.e., to each path). Every time we import a file, we store the resulting data frame as an element of a list. After the loop finishes, we merge the rows of all element of the list using a combination of the functions `do.call` and `rbind`.

```{r echo=TRUE, message=FALSE, results="hide"}
n <- 20 # number of files
filepaths <- list.files(tempdir(), full.names = TRUE, pattern = ".txt") # list files in folder
dat <- list() # pre-allocate the list of files

for (i in 1:length(filepaths)){
  dat[[i]] <- read.delim(filepaths[i])
}
dat <- do.call(rbind, dat) #  bind data frames together
```


### Base R: `lapply`

We will use the functions `read.delim` and `read.csv` in combination with the function `lapply`. The former are well known. The later is part of a family of functions (together with `sapply`, `mapply`, and some others I can't remember) that take two arguments: a list and a function, which will be applied over each element of the list in parallel (i.e., in a vectorised way).

```{r echo=TRUE, message=FALSE, results="hide"}
filepaths <- list.files(tempdir(), full.names = TRUE, pattern = ".txt")
dat <- lapply(filepaths, read.delim)
dat <- do.call(rbind, dat)
```

### tidyverse

The tidyverse is a family of packages that suggests a workflow when working in R. The use of pipes (`%>%`) is one of its signature moves, which allow you to chain several operations applied on the same object within the same block of code. In contrast, base R makes you choose between applying several functions to the same object in different blocks of code, or applying those functions in a nested way, so that the first functions you read are those applied the last to your object (e.g., `do.call(rbind, as.list(data.frame(x = "this is annoying", y = 1:100)))`). We will use a combination of the `dplyr` and `purrr` packages to import the files listed in a vector, using `read.delim` and `bind_rows`.

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE, results="hide"}
library(dplyr)
library(purrr)

filepaths <- list.files(tempdir(), full.names = TRUE, pattern = ".txt")
dat <- map(filepaths, read.delim) %>% 
  bind_rows()
```

### `data.table`

The function `rbindlist` function from the package `data.table` also allows to merge the datasets contained in a list. In combination with `fread` (from the same package), it can be very fast.

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE, results="hide"}
library(dplyr)
library(data.table)

filepaths <- list.files(tempdir(), full.names = TRUE, pattern = ".txt")
dat <- map(filepaths, fread) %>% 
  rbindlist()
```


## What method is the fastest?

I will compare how long each combination of importing, vectorising, and merging functions needs to import 50 data sets with 10 columns and 10,000 rows each. Additionally, I will compare the performance of each method when working with CSV (.csv) and TSV (.txt) files. For each method, I will repeat the process 100 times, measuring how long it takes from the moment we list the extant files in the folder to the moment we finish merging the data sets. Here are the results:

```{r timesplot, echo=FALSE, fig.cap="Mean time (and standard deviation) for each combination of methods and file formats across 100 replications", message=FALSE, warning=FALSE, paged.print=FALSE}
# load packages
library(tidyverse)
library(data.table)

# import data
dat <- fread("2020-07-05-importing-multiple-files.txt") %>% 
  as_tibble() %>% 
  group_by(package, format, vectorisation, merge) %>% 
  summarise(mean = mean(time), sd = sd(time), .groups = "drop")

ggplot(dat, aes(package, mean, colour = vectorisation)) +
  facet_grid(format~merge) +
  geom_line(aes(group = interaction(vectorisation, format)), size = 1) +
  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd), linetype = "solid", width = 0) +
  geom_point(size = 3.5) +
  labs(x = "Importing function", y = "Time (s)", colour = "Vectorisation function",
       shape = "File format", linetype = "File format") +
  scale_colour_manual(values = c("red", "#e8b941", "#42b3f5")) +
  scale_y_continuous(limits = c(0, 2)) +
  theme_minimal() +
  theme(
    axis.text = element_text(colour = "black"),
    axis.line = element_line(size = 0.75),
    panel.grid = element_line(colour = "grey"),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.position = "right",
    legend.box = "vertical"
  )
```
For more detail:

```{r timestab, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(gt)
dat %>%
  pivot_wider(names_from = "vectorisation", values_from = c("mean", "sd")) %>% 
  gt(groupname_col = c("merge", "format")) %>% 
  tab_spanner(label = md("**for loop**"), columns = matches("loop")) %>% 
  tab_spanner(label = md("**lapply**"), columns = matches("lapply")) %>% 
  tab_spanner(label = md("**purrr::map**"), columns = matches("purrr")) %>% 
  fmt_number(columns = matches("mean_|sd_")) %>% 
  cols_label(`mean_for loop` = md("*M*"),
             `sd_for loop` = md("*SD*"),
             `mean_lapply` = md("*M*"),
             `sd_lapply` = md("*SD*"),
             `mean_purrr::map` = md("*M*"),
             `sd_purrr::map` = md("*SD*")) %>% 
  tab_header("Time taken to import and merge",
             subtitle = "50 datasets with 10 columns and 10,000 rows each") %>% 
  summary_rows(columns = matches("mean|sd"),
               fns = list(Mean = c("mean")),
               formatter = fmt_number) %>% 
  tab_options(row_group.font.weight = "bold") 
```

Figure \@ref(fig:timesplot) and Table \@ref(tab:timestab) show the detailed timings The grand mean average time taken by all methods is ~2.12 seconds, but there are some differences.

* It doesn't really matter what function we use to merge data sets: both `do.call` and `dplyr::bind_rows` perform roughly similarly.
* What makes the biggest difference is what function we use to vectorise the importing operation across file names to import them. `purrr::map` is the fastest. Incredibly, is takes less than 0.3 seconds in all conditions. It is also the least sensitive to the format of the files and the function we use to import them.
* The next vectorising function in terms of temporal efficiency is `lapply`, which takes ~1.5 seconds. It performs slightly better when working with .txt files, in that when working with .csv files its performance depends on what method we use to import them: `data.table::fread` is much faster than its base and `readr` competitors. This [post](https://www.danielecook.com/speeding-up-reading-and-writing-in-r/) by Daniele Cook sheds some light into the advantage of `data.table` over other importing functions, also covering the `vroom` package, which this post doesn't cover.
* Using *for loops* looks like the least efficient method for iterating across data sets when importing data. It also shows a similar profile than `lapply`: `data.table::fread` performs a bit better than the rest.

## Conclusion

Under the scenario under which I have simulated the data, it seems that using `purrr::map` in combination with `do.call` or `dplyr::bind_rows` to merge data sets is the most efficient method in terms of time. When using said combination, it doesn't matter what function we use to import files, but `data.table::fread` seems like the best choice, as it is also the most flexible (take a look at the [documentation](https://github.com/Rdatatable/data.table) of `data.table` to see all the features it offers).

If I have time, I may add another two dimensions: number of rows in the files and number of files, although I dare say similar results are to be expected. If anything, I would say that differences may become greater as file size and number of files increase. Also, it would be interesting to test if pre-allocating the elements of the vector in the for loop speeds up the process (see here what I mean). We shall see.

Hope this was useful, if not interesting!

## Code

<script src="https://gist.github.com/gongcastro/35aa0ae28380fc0ff48c7df71b387b20.js"></script>