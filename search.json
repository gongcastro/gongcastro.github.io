[
  {
    "objectID": "blog/logistic-regression/logistic-regression.html",
    "href": "blog/logistic-regression/logistic-regression.html",
    "title": "Getting the most out of logistic regression",
    "section": "",
    "text": "Codelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(scales)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(ggdist)\nlibrary(palmerpenguins)\n\ntheme_set(\n    theme_minimal() +\n        theme(\n            axis.line = element_line(colour = \"black\", size = 0.65),\n            panel.grid = element_blank()\n        ))\n\nclrs &lt;- c(\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\")"
  },
  {
    "objectID": "blog/logistic-regression/logistic-regression.html#marginal-effects",
    "href": "blog/logistic-regression/logistic-regression.html#marginal-effects",
    "title": "Getting the most out of logistic regression",
    "section": "Marginal effects",
    "text": "Marginal effects\nDefining marginal effect is tricky. As it happens with many concepts and labels in statistics, the same label may be used to refer to different concepts, and several labels may be used interchangeably to refer to the same concept. Each subfield of science seems to use a somewhat intrinsic lexicon, which sometimes leads to confusion. I will adopt the terminology in the documentation of the {marginaleffects} R package (Arel-Bundock, 2022), in which a marginal effect is defined in the context of regression as:\n\nArel-Bundock, V. (2022). Marginaleffects: Marginal effects, marginal means, predictions, and contrasts. https://CRAN.R-project.org/package=marginaleffects\n\n\n\nThe {marginaleffects} R package\n\n\n\n[‚Ä¶] the association between a change in a regressor \\(x\\) and a change in the response \\(y\\). Put differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor \\(x\\).\n\nAccording to this definition, calculating the marginal effect of our predictor of interest flipper_length_mm means extracting its slope for a specific value of the predictor. For linear regression models, this is trivial: since the relationship between the predictor and the response variable is assumed linear, the slope is considered constant across the whole range of the values of the predictor, and therefore its marginal effect is identical for all of them.\nWe can prove this by looking at the estimates of our (linear) model in the logit scale. Let‚Äôs say that we are interested in finding out the slope of flipper_length_mm for its average, 190.1027397. A slope is just a difference. And a difference is a derivative. And the linear regression function, \\(y = \\beta_0 + \\beta_1 x\\), is a function that can be derived (see Equation¬†10).\n\\[\n\\begin{aligned}\ny &= \\beta_0 + \\beta_1 \\times \\text{Flipper length} \\\\\ny' &= \\beta_1 & \\text{First derivative}\n\\end{aligned}\n\\tag{10}\\]\nThe derivative of the linear regression equation is a constant. This constant corresponds to the regression coefficient of flipper_length_mm, which means that the difference in probability of being a male penguin is going to be same for two penguins whose flippers are 180 mm and 185 mm, respectively, and for two penguins whose flippers are 190 and 195 mm, respectively.\nTake a look at Figure¬†6. The difference in chances of being male between each pair of penguins, in the logit scale, is the same: 0.64, which corresponds to five times the flipper_length_mm regression coefficient because in both cases the difference in flipper length is not 1 mm but 5 mm (\\(5 \\times 0.1271 = 0.635\\)). But, again, any value in the logit scale is difficult to interpret by itself, so we are interested in translating this to the scale of probabilities.\n\nCode# predictions of model with unstandardised age\npoint_preds &lt;- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %&gt;% \n    mutate(sex_pred = predict(fit, ., type = \"link\"))\n\nggplot(my_data, aes(flipper_length_mm, sex_pred_logit)) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred_logit),\n              size = 1,\n              colour = clrs[1]) +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\",\n             label = paste0(round(point_preds[2,2]-point_preds[1,2], 3), \" increment\"),\n             x = point_preds[2,1],\n             y = mean(c(point_preds[1,2], point_preds[2,2])),\n             hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[3, 1], \n             y = point_preds[3, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\", \n             label = paste0(percent(point_preds[4, 2]-point_preds[3, 2]), \" increment\"),\n             x = point_preds[4, 1], \n             y = mean(c(point_preds[4, 2], point_preds[3, 2])),\n             hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[3, 1], \n             y = point_preds[3, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[1, 1], \n             y = point_preds[1, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male) [logit scale]\",\n         title = \"&lt;span style = 'color:#003f5c;'&gt;Slope (logit scale)&lt;/span&gt;\") +\n    guides(linetype = \"none\") +\n    \n    ggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    geom_point(colour = NA) +\n    geom_hline(yintercept = coef(fit)[\"flipper_length_mm\"],\n               size = 1, colour = \"#ffa600\") +\n    annotate(geom = \"label\", \n             label = paste0(\"Constant = \", round(coef(fit)[\"flipper_length_mm\"], 2)),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = round(coef(fit)[\"flipper_length_mm\"], 2),\n             vjust = -1.25) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length (logit scale)\",\n         title = \"&lt;span style = 'color:#ffa600;'&gt;Derivative (logit scale)&lt;/span&gt;\") +\n    scale_y_continuous(limits = c(0.05, 0.2)) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\n\n\n\nFigure¬†6: The derivative of a linear regression equation is a constant.\n\n\n\n\nWe have seen that regression coefficients do not behave identically for different values of their predictors when transformed into probabilities. The exact point of flipper_length_mm at which we calculate its marginal effect matters. This is because the derivative of the logistic function, which describes the behaviour of the probability scale we just moved to, is no longer a constant. See Equation¬†11.\n\\[\n\\begin{aligned}\ny &= \\frac{1}{1 + e^{(-\\beta x)}} & \\text{Logistic function} \\\\\ny' &= \\frac{\\beta_1 ¬∑ e^{\\beta_0 + \\beta_1 ¬∑ \\ \\text{Flipper length}} }{(1 + e^{-(\\beta_0 + \\beta_1 ¬∑ \\ \\text{Flipper length})})^2} & \\text{Derivative}\n\\end{aligned}\n\\tag{11}\\]\nThe derivative of the logistic function still takes into account the value of the predictor (\\(\\text{Flipper length}\\)), which means that the value of the derivative changes depending on such value. Let‚Äôs try to visualise this. First, we are going to implement the derivative of the logistic function as an R function that computes it for the fit model:\n\nlogistic_derivative &lt;- function(object, x, value) {\n    slope &lt;- coef(object)[x]\n    intercept &lt;- coef(object)[\"(Intercept)\"]\n    numerator &lt;- slope * exp(-(intercept + (slope * value)))\n    denominator &lt;- (1 + exp(-(intercept + (slope * value))))^2\n    y &lt;- numerator/denominator\n    names(y) &lt;- NULL\n    return(y)\n}\n\nFigure¬†7 shows how the derivative changes for each value of the predictor. As you can see, the difference in probability of being male is largest at around 190 mm, while such difference decreases as flipper_length_mm shifts away from 190 mm.\n\nCode# model predictions (on the probability scale by default)\nmy_data &lt;- mutate(\n    my_data,\n    derivative = logistic_derivative(\n        fit, \n        \"flipper_length_mm\",\n        flipper_length_mm\n    )\n)\n\npoint_preds &lt;- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %&gt;% \n    mutate(sex_pred = plogis(predict(fit, .)),\n           derivate = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\", \n             label = paste0(percent(point_preds[2,2]-point_preds[1,2]), \" increment\"),\n             x = point_preds[2,1],\n             y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[1, 1], \n             y = point_preds[1, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2]),\n                 linetype = \"dashed\") +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2]),\n                 linetype = \"dashed\") +\n    annotate(geom = \"text\",\n             label = paste0(percent(point_preds[4,2]-point_preds[3,2]), \" increment\"),\n             x = point_preds[4,1],\n             y = mean(c(point_preds[4,2], point_preds[3,2])), \n             hjust = -0.1) +\n    annotate(geom = \"text\", \n             label = \"5 cm difference\",\n             x = point_preds[3, 1], \n             y = point_preds[3, 2], \n             hjust = -0.1,\n             vjust = 1.5) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male)\",\n         title = \"&lt;span style = 'color:#003f5c;'&gt;Logistic curve&lt;/span&gt;,\n        &lt;span style = 'color:#ff6361;'&gt;intercept&lt;/span&gt;\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    \n    ggplot(my_data, aes(flipper_length_mm, \n                        logistic_derivative(fit, \"flipper_length_mm\", value = flipper_length_mm))) +\n    geom_point(colour = NA) +\n    geom_line(size = 1, colour = \"#ffa600\") +\n    geom_segment(aes(x = flipper_length_mm[which.max(derivative)],\n                     xend = flipper_length_mm[which.max(derivative)],\n                     y = min(derivative), yend = max(derivative)),\n                 colour = \"grey\", linetype = \"dotted\") +\n    annotate(geom = \"label\", label = paste0(\"Max = \", percent(max(my_data$derivative))),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = max(my_data$derivative)*0.5) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length  (probability scale)\",\n         title = \"&lt;span style = 'color:#ffa600;'&gt;Derivative&lt;/span&gt;\") +\n    scale_y_continuous(labels = percent) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n\n\n\n\n\n\nFigure¬†7: The derivative of a non-linear regression equation is not constant.\n\n\n\n\nThe maximum change in the probability of being male is 3%, which occurs at around 190 mm flipper length. There is a smarter way of calculating the maximum slope of flipper_length_mm. This value will always occur at the mid-point of the logistic curve, and it turns out that to find the derivative of the logistic function at the mid-point (i.e., for \\(x = x_0\\)), we only need to find \\(\\beta /4\\), where \\(\\beta\\) is the regression coefficient of our predictor of interest! This is called the divide-by-four-rule, and is a simple trick to report the coefficients of a logistic regression model in the scale of probabilities, and for meaningful values of the predictors (the value at which the slope is maximum). This way, we could just divide the regression coefficient of flipper_length_mm by four to get the maximum probability difference of being male between two penguins with flipper lengths \\(x\\) and \\(x + 1\\).\n\n\n\n\n\n\nThe divide-by-four rule\n\n\n\nDividing the coefficients of a logistic regression model (in the logit scale) gets us the maximum slope of the predictor in the probability scale. We mentioned that this has to do with the derivative of the logistic function at the mid-point.\nBut since we dropped that term some equations ago after setting it at zero, it is no longer clear how one would derive the logistic function in such way that the divide-by-four rule holds. Let‚Äôs go back to Equation¬†2, when the mid-point still appeared in our equation. We derive this formula:\n\\[\n\\begin{aligned}\n\\text{Logistic(x)} &= \\frac{1}{1 + e^{(-\\beta \\ ¬∑ \\ (x-x_0))}} & \\text{Logistic function}\\\\\n\\text{Logistic'(x)} &= \\frac{\\beta ¬∑ e^{-\\beta (x-x_0)}}{(1 + e^{-\\beta (x-x_0)})^2} & \\text{Derivative}\n\\end{aligned}\n\\]\nAnd since \\(x = x_0\\), \\(x-x_0 = 0\\). We can simplify the derivative now, knowing that \\(e^0 = 1\\).\n\\[\n\\text{Logistic'(x)} = \\frac{\\beta ¬∑ e^0}{(1 + e^0)^2} = \\frac{\\beta}{(1 + 1)^2} = \\frac{\\beta}{4}\n\\]\nTake a look at the previously cited blog post by TJ Mahr for a derivation that also includes the asymptote term in the logistic function.\n\n\nLet‚Äôs put the divide-by-four rule to test. The output of logistic_derivative() that we defined before should, when solved for the mid-point of the logistic curve, return an equivalent value to \\(\\beta_1 / 4\\), where \\(\\beta_1 = 0.1271495\\), and therefore return something close to 0.0317874. We don‚Äôt know what value of flipper_length_mm corresponds to the mid-point. In Figure¬†7 we calculated it my finding the value of flipper_length_mm for which logistic_derivative() returned the maximum value:\n\nderivative_values &lt;- logistic_derivative(fit, \"flipper_length_mm\", my_data$flipper_length_mm)\nmy_data$flipper_length_mm[which.max(derivative_values)]\n\n[1] 190\n\n\nFrom our data, we find that the maximum slope of flipper_length_mm occurs at 190. But finding the mid-point this way requires us to have already calculated the derivative of the logistic function. There is an alternative way to compute this mid-point from the estimated coefficients of the regression model. An additional benefit of computing the mid-point this way is that we are doing so by relying on model-projections, and therefore in a way that does not entirely rely on the range of values of the predictor for which we have computed the derivative of the logistic function. This method consists of Equation¬†12.\n\\[\nx_0 = -\\beta_0 / \\beta_1\n\\tag{12}\\]\nWhere \\(\\beta\\) is the intercept of the regression model, and \\(\\beta_1\\) is the regression coefficient of the predictor we are calculating the mid-point for. We can implement this formula in R as:\n\n# get point in x at the inflection point (where y = 0.5)\nget_mid &lt;- function(x) {\n    coefs &lt;- coef(x)[-1]\n    mid &lt;- coef(x)[1]/-coefs\n    return(mid)\n}\n\nget_mid(fit)\n\n(Intercept) \n   190.0882 \n\n\nUsing this function, we find that the mid-point is located at flipper length 190.0882 mm, pretty close to what we had estimated from our data. This value can sometimes be extremely interesting!\n\n\n\n\n\n\nA personal experience\n\n\n\nIn my PhD, I investigated how the age at which toddlers and children learn particular words is affected by participant-level (e.g., age, amount of linguistic exposure) and word-level characteristics (e.g., word length, lexical frequency). I used logistic regression to model the probability of a given toddler having learnt a word, adjusting for my predictors of interest, the most important of them being the age of the child. Older children are more likely to have learnt a given word than younger children.\nMy model returned, among others, a coefficient for age in the logit scale, but I wasn‚Äôt particularly interested in it, even after having transformed it to the probability scale. I was, however, more interested in finding the value of age at which most children were learning each word, which corresponded to the mid-point of the logistic curve for the age predictor!\n\n\nNow that we have calculated the mid-point of our logistic function, we can finally compare the divide-by-four rule against the actual value of the derivative of the logistic function. If we solve Equation¬†11 for \\(x_0 = 190.0882\\) using logistic_derivative(fit, \"flipper_length_mm\", 190.0882), we get that the mid-point is located at 0.0317874. If we use the divide-by-four rule, we get 0.0317874. Exactly the same!\nSo far, we have seen that the divide-by-four rule is a simple way to obtain the slope at the mid-point, which is a meaningful value: it tells us the upper limit of the distribution of the regression slope of the predictor. However, there might be other values of the predictor for which we might be interested in finding the slope of the coefficient. We might even want to compute the average of all slopes! These, an others, are different perspectives to adopt when thinking, computing, and reporting marginal effects.\nI cannot say anything that has not already been explained better by Andrew Heiss in his blog post: Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are (Heiss, 2022). I will only mention that to compute the marginal effects of your model, regardless of your strategy towards reporting marginal effects (maximum slope, average marginal effects, marginal effects at specific points), or the characteristics of your model (Gaussian vs.¬†binomial, logit vs.¬†probit, Bayesian vs.¬†frequentist, etc.), the marginaleffects R package will be useful. Take a look at its documentation and functions, and play with them.\n\nHeiss, A. (2022, May 20). Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are  andrew heiss. Andrew heiss. https://www.andrewheiss.com/blog/2022/05/20/marginalia/"
  },
  {
    "objectID": "blog/dpf-scholkmann/dpf-scholkmann.html",
    "href": "blog/dpf-scholkmann/dpf-scholkmann.html",
    "title": "Implementing the Differential Pathlength Factor (DPF) in Python: the Scholkmann method",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncycler = plt.cycler(\"color\", plt.cm.autumn(np.linspace(0, 1, 100)))\nplt.rcParams[\"axes.prop_cycle\"] = cycler\n\n\n\n\n\n\n\n\nTip\n\n\n\nTLDR: there you go.\ndef dpf_scholkman(\n    age: float or np.ndarray = 0.583, wavelength: float or np.ndarray = 760\n) -&gt; float or np.ndarray:\n    coefs = {\n        \"alpha\": 223.3,\n        \"beta\": 0.05624,\n        \"gamma\": 0.8493,\n        \"delta\": -5.723e-7,\n        \"epsilon\": 0.001245,\n        \"zeta\": -0.9025,\n    }\n    dpf = (\n        coefs[\"alpha\"]\n        + coefs[\"beta\"] * (age ** coefs[\"gamma\"])\n        + coefs[\"delta\"] * wavelength**3\n        + coefs[\"epsilon\"] * wavelength**2\n        + coefs[\"zeta\"] * wavelength\n    )\n    return dpf\n\n\nI have news! I‚Äôm starting my postdoc at the Institut de Recerca Hospital Sant Joan de D√©u (Barcelona, Spain) in September, joining Chiara Santolin‚Äôs new research group. We will be carrying out experimental series as part of Chiara‚Äôs ERC project GaLa (Gates to Language). This project involves newborns, infants, rats, and some computational modelling to investigate early language acquisition and its evolutionary roots. I‚Äôll be mostly involved with the neonates branch of the project, using fNIRS (functional near-infrared spectroscopy) to measure neonates‚Äô brain response to different audio stimuli.\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: \n\n\n\nI‚Äôm new to fNIRS, so I was very lucky to spend two months at Prof.¬†Emily Jones‚Äô group at the Center of Brain and Cognitive Development (CBCD) at Birkbeck University (London, UK), and two weeks at Prof.¬†Judit Gervain‚Äôs lab at Universit√† di Padova. Folks at CBCD and in Padova were extremely generous with their time and knowledge, and after coming back from these research stays I feel like returning some of my time to the fNIRS community. I will dedicate some upcoming blog posts to some of my progress in learning fNIRS and the underlying theory and associated preprocessing piplines, with the aim of making the learning curve a bit less steep for future newcomers. In this blog post, I will provide a very simple Python function to calculate the Differential Pathlength Factor (DPF) for an individual, given their age and the specific wavelength used, following Scholkman‚Äôs method (Scholkmann & Wolf, 2013).\nTo estimate relative changes in concentration of HbO and HbR, most NIRS devices‚Äîparticularly those that use a continuous wave (CW) system‚Äîshine a light into some tissue of interest at two wavelengths: one is more sensitive to changes in concentration of HbO, the other is more sensitive to changes in concentration of HbR. Light is shone from a source, travels through the tissue and is received by a detector. By estimating how much of the light gets attenuated, and by observing the spectral properties of the received light, NIRS provides an estimation of the presence of certain chromophores in the tissue (in our case, HbO and HbR). The modified Beer-Lambert Law (mBBL) (see Equation¬†1) describes how the light attenuation (\\(A\\)) changes for a particular wave length (\\(\\lambda\\)):\n\\[\n\\Delta A = \\varepsilon(\\lambda) \\cdot \\Delta c \\cdot d \\cdot \\text{DPF}(\\lambda)\n\\tag{1}\\]\nwhere \\(\\varepsilon\\) is the extinction coefficient of a molecule at a certain wavelength, \\(\\lambda\\) is the wavelength of the inyected light, \\(c\\) is the change in molecule concentration, \\(d\\) is the source-detector distance, and \\(\\text{DPF}\\) is the differential pathlength factor. The DPF specifies the increment in photon path length due to scattering: sometimes, photons ‚Äúbounce‚Äù against molecules also present in the medium, which increases the distance they travel through the biological tissues, ultimately influencing the intensity of the light that the detector receives at a particular wavelength. The DPF parameter takes a numeric value specified a priori by the researcher, based on prior knowledge about the properties of the tissue.\nConventional CW-based NIRS devices cannot determine exactly the true DPF, as this value is dependent on factors outside of the control or reach of the researcher. But some methods provide a reasonable approximation. One of them was presented by Scholkmann & Wolf (2013). The authors came up with an equation that takes into account the age of the participant, and the wavelength of the NIR light. These variables are then introduced in Equation¬†2:\n\\[\n\\text{DPF} (\\lambda, A)= \\alpha + \\beta A ^{\\gamma} + \\delta \\lambda ^{3} + \\varepsilon \\lambda ^{2} + \\zeta \\lambda\n\\tag{2}\\]\nwhere \\(\\lambda\\) is the wavelength, \\(A\\) is the age of the participant, and \\(\\alpha\\), \\(\\beta\\), \\(\\delta\\), \\(\\varepsilon\\), and \\(\\zeta\\) are the certain coefficients that were empirically estimated by Scholkmann & Wolf (2013) using a robust nonlinear least squares method, based on previously published datasets. The result is a formula in which one can introduce the age of the participant and the wavelength of interest, and obtain the estimated DPF:\n\\[\n\\text{DPF} (\\lambda, A)= 223.3 + 0.05624 A ^{0.8493} + ‚àí5.723 \\times 10^{‚àí7} \\lambda ^{3} + 0.001245 \\lambda ^{2} + ‚àí0.9025 \\lambda\n\\]\nI made a simple Python function to implement the equation:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl\n\n\ndef dpf_scholkman(\n    age: float or np.ndarray = 0.583, wavelength: float or np.ndarray = 760\n) -&gt; float or np.ndarray:\n    coefs = {\n        \"alpha\": 223.3,\n        \"beta\": 0.05624,\n        \"gamma\": 0.8493,\n        \"delta\": -5.723e-7,\n        \"epsilon\": 0.001245,\n        \"zeta\": -0.9025,\n    }\n    dpf = (\n        coefs[\"alpha\"]\n        + coefs[\"beta\"] * (age ** coefs[\"gamma\"])\n        + coefs[\"delta\"] * wavelength**3\n        + coefs[\"epsilon\"] * wavelength**2\n        + coefs[\"zeta\"] * wavelength\n    )\n    return dpf\n\n\nLet‚Äôs try it out:\n\n\nCode\nage = 24  # years\nwl = 760  # wavelength\ndpf_scholkman(age, wl)\n\n\n6.122136155212161\n\n\nLet‚Äôs reproduce some of the figures in Scholkmann & Wolf (2013).\n\nScholkmann, F., & Wolf, M. (2013). General equation for the differential pathlength factor of the frontal human head depending on wavelength and age. Journal of Biomedical Optics, 18(10), 105004‚Äì105004.\n\n\nCode\nage = np.linspace(0, 50, 100)\nwavelengths = np.linspace(690, 832, 100)\ndpf = np.vstack([dpf_scholkman(age, w) for w in wavelengths])\nfig, ax = plt.subplots(layout=\"constrained\")\nnorm = mpl.colors.Normalize(vmin=np.min(wavelengths), vmax=np.max(wavelengths))\ncycler = plt.cycler(\"color\", plt.cm.autumn(np.linspace(0, 1, 100)))\nx = np.vstack(np.array([age] * len(age))).transpose()\nfig.colorbar(\n    mpl.cm.ScalarMappable(norm=norm, cmap=\"autumn\"),\n    ax=ax,\n    orientation=\"horizontal\",\n    location=\"top\",\n    label=\"Wavelength (nm)\",\n    aspect=30,\n)\nplt.rcParams[\"axes.prop_cycle\"] = cycler\nax.plot(x, dpf.transpose())\nax.set_xlabel(\"Age (years)\")\nax.set_ylabel(\"DPF\")\nax.set_axisbelow(False)\nax.grid(color=\"gray\", alpha=0.1)\n\n\n\n\n\n\n\n\nFigure¬†2: Reproduction of Figure 1A in Scholkmann et al.¬†Differential pathlength as a function of the age of the participant (X-axis) and the wavelength (Y-axis).\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(layout=\"constrained\")\nnorm = mpl.colors.Normalize(vmin=np.min(age), vmax=np.max(age))\ncycler = plt.cycler(\"color\", plt.cm.autumn(np.linspace(0, 1, 100)))\nx = np.vstack(np.array([wavelengths] * len(wavelengths))).transpose()\nfig.colorbar(\n    mpl.cm.ScalarMappable(norm=norm, cmap=\"autumn\"),\n    ax=ax,\n    orientation=\"horizontal\",\n    location=\"top\",\n    label=\"Age (years)\",\n    aspect=30,\n)\nplt.rcParams[\"axes.prop_cycle\"] = cycler\nax.plot(x, dpf)\nax.set_xlabel(\"Wavelength (nm)\")\nax.set_ylabel(\"DPF\")\nax.set_axisbelow(False)\nax.grid(color=\"gray\", alpha=0.1)\n\n\n\n\n\n\n\n\nFigure¬†3: Reproduction of Figure 1B in Scholkmann et al.¬†Differential pathlength as a function of the wavelength (X-axis) and the age of the participant (Y-axis).\n\n\n\n\n\nWe can also visualise the behaviour of the DPF across ages and wavelengths as a 2D heatmap in Figure¬†4:\n\n\nCode\nnorm = mpl.colors.Normalize(vmin=np.min(dpf), vmax=np.max(dpf))\n\nfig, ax = plt.subplots(layout=\"constrained\")\nax.imshow(dpf.transpose(), cmap=\"autumn\", aspect=\"auto\")\nax.set_yticks(\n    np.linspace(0, len(wavelengths), 10, dtype=int),\n    labels=np.linspace(min(wavelengths), max(wavelengths), 10, dtype=int),\n)\nax.set_xticks(\n    np.linspace(0, len(age), 10, dtype=int),\n    labels=np.linspace(min(age), max(age), 10, dtype=int),\n)\nfig.colorbar(\n    mpl.cm.ScalarMappable(norm=norm, cmap=\"autumn\"),\n    ax=ax,\n    orientation=\"horizontal\",\n    location=\"top\",\n    label=\"DPF\",\n    aspect=30,\n)\nax.set_ylabel(\"Wavelength (nm)\")\nax.set_xlabel(\"Age (years)\")\nax.invert_yaxis()\nax.set_axisbelow(False)\nax.grid(color=\"gray\", alpha=0.1)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†4: DPF as a function of participant age and wavelengths.\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{garcia-castro,\n  author = {Garcia-Castro, Gonzalo},\n  title = {Implementing the {Differential} {Pathlength} {Factor} {(DPF)}\n    in {Python:} The {Scholkmann} Method},\n  url = {http://github.com/gongcastro/gongcastro.github.io/blog/dpf-scholkmann/dpf-scholkmann.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGarcia-Castro, G. (n.d.). Implementing the Differential Pathlength\nFactor (DPF) in Python: the Scholkmann method. http://github.com/gongcastro/gongcastro.github.io/blog/dpf-scholkmann/dpf-scholkmann.html"
  },
  {
    "objectID": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "href": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html",
    "title": "A primer on mixed-effects Models: theory and practice",
    "section": "",
    "text": "When I started my PhD I had to get familiar with Linear Mixed Models very well very quickly. Then I was asked to present what I learnt and prepare this informal tutorial for my colleagues, which I presented on March 10th, 2020 (yes, early pandemic :confounded:). This post shares the result.\nThis is not supposed to be taken as a formal guide to linear mixed-effects models, but rather as a semi-coherent compilation of notes and self-suggestions that I considered worth sharing with my lab mates during my early stages of in the PhD. Here are the slides:\nWhat I should be taken more seriously are (1) the references I suggest in the first slides (which I consider some of the best resources available to learn linear mixed-effects models), and (2) the memes, which I personally curated and even created to sweeten up the dreadful incoherence of the content of some of the slides (sorry about that).\nBefore the presentation I tweeted one of the animations I generated for it.\nThis tweet got some attention (for my usual numbers) and many kind folks have asked for the R code or the GIF file of the specific animation included in the tweet, so here they are (you‚Äôll also find them in the GitHub repository) in a perhaps more comfortable format, ready to be cloned or downloaded):"
  },
  {
    "objectID": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "href": "blog/primer-linear-mixed-models/primer-linear-mixed-models.html#session-info",
    "title": "A primer on mixed-effects Models: theory and practice",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.3.3 (2024-02-29 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] xaringanExtra_0.8.0\n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.3.3    fastmap_1.2.0     cli_3.6.3        \n [5] htmltools_0.5.8.1 tools_4.3.3       yaml_2.3.9        rmarkdown_2.27   \n [9] knitr_1.48        jsonlite_1.8.8    xfun_0.46         digest_0.6.36    \n[13] rlang_1.1.4       renv_1.0.5        evaluate_0.24.0"
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "",
    "text": "Install decktape and adapt this command to download your slides‚Äîwhich should be deployed on GitHub Pages‚Äîas PDF in your local machine:\ndecktape &lt;slides-url&gt; &lt;slides-file&gt;.pdf\n\n\n\n\n\n\n\n\n\n\nAn embarrasing confession\n\n\n\nSo it turns out one can download their RevealJS slides (in a HTML file) without using decktape, just by following the instructions in the Quarto documentation. In my defence, for whatever reason this did not work for me, whereas decktape did the job. I probably skipped a critical step.\nI‚Äôm too lazy to re-orient or rewrite this post, so I‚Äôll publish it any way. Read it as if it were the panicky reflections of a sleepless PhD student who got overconfident about their programming skills the night before their conference presentation. decktape might be still a convenient option to do the same job in a programmatic way, so there it goes."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#tldr",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#tldr",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "",
    "text": "Install decktape and adapt this command to download your slides‚Äîwhich should be deployed on GitHub Pages‚Äîas PDF in your local machine:\ndecktape &lt;slides-url&gt; &lt;slides-file&gt;.pdf\n\n\n\n\n\n\n\n\n\n\nAn embarrasing confession\n\n\n\nSo it turns out one can download their RevealJS slides (in a HTML file) without using decktape, just by following the instructions in the Quarto documentation. In my defence, for whatever reason this did not work for me, whereas decktape did the job. I probably skipped a critical step.\nI‚Äôm too lazy to re-orient or rewrite this post, so I‚Äôll publish it any way. Read it as if it were the panicky reflections of a sleepless PhD student who got overconfident about their programming skills the night before their conference presentation. decktape might be still a convenient option to do the same job in a programmatic way, so there it goes."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#some-context",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#some-context",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "Some context",
    "text": "Some context\nLast week, I presented an oral communication at the International Symposium of Psycholinguistics. I feel comfortable using Quarto, and I had already given a couple of presentations generating slides from Quarto before, so I decided to take the next step and use Quarto for this one as well.\nI chose RevealJS as the output format of my slides: it looks beautiful, you don‚Äôt need much knowledge of HTML or CSS to fix the layout of the slides, and it has a nice presenter view mode. In the end this is what my slides looked like:\n\n\n\n\n\n\n\n\nI‚Äôm quite happy with how they look, so I even made a Quarto extension to apply this format to future presentations more conveniently.\nThe main inconvenience of RevealJS (or any HTML output format) is that presenting from a computer difference from the one in which you created the slides requires moving a whole folder, as opposed to just moving the .pptx or .pdf file to the new machine.\nIf one moves only the resulting index.html that contains the slides, the presentation will lose all of its nice formatting, and won‚Äôt show any images or additional resources. From previous (bad) experiences, I knew that this could be a problem, and given that I would certainly have to present from the conference room‚Äôs machine, I decided to use GitHub Pages1 to deploy my slides on a website.\n1¬†Here is a nice tutorials on how to deploy your slides on GitHub PagesMy plan was to access the URL of the slides whenever I was about to present. But the day before the presentation, I started worrying about the internet not working and the conference venue at the wrong time I needed to export the slides to a PDF. A PDF never (well, rarely) fails."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#the-problem",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#the-problem",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "The problem",
    "text": "The problem\nExporting Quarto slides to PDF is not as simple as just adding the pdf: default line to the YAML header of the file, under format:, like this:\nformat:\n  revealjs: default\n  pdf: default\nThis will render the slides as a PDF document, not as slides. Also, much of the nice formatting would be lost, as the .scss file that contained the styling only applies to HTML.\nQuarto offers a PDF output format for slides, Beamer, which is pretty popular among LaTeX users. But again, generating the presentation in Beamer format would require re-doing the formatting from scratch, and doing so in LaTeX, which I‚Äôm not totally comfortable using. This was not an option. Neither it was to use the Power Point output format, as not only the format was lost, but also prevented any possibility of customising the layout of the slides.\n\n\n\n\n\nYou may recognise this aesthetic from Beamer slides. I personally don‚Äôt like it a lot, but I acknowledge the convenience of Beamer for LaTeX conoisseurs. I wish I was one of them. I‚Äôm not."
  },
  {
    "objectID": "blog/revaljs-to-pdf/revealjs-to-pdf.html#from-revealjs-to-pdf-printing-option-in-chrome",
    "href": "blog/revaljs-to-pdf/revealjs-to-pdf.html#from-revealjs-to-pdf-printing-option-in-chrome",
    "title": "Using decktape to convert Quarto slides from RevealJS to PDF",
    "section": "From RevealJS to PDF: printing option in Chrome",
    "text": "From RevealJS to PDF: printing option in Chrome\nI came across the Print to PDF section of the Presenting Slides article in the Quarto documentation:\n\n\nA snapshot of the instructions to print a RevealJS presentation to PDF in the Quarto documentation (see Chrome‚Äôs printing menu on the right, prompted byu Ctrl + P.\n\nAlthough this looked promising, the output of this method was not what I expected. The layout had been moved around a bit, and some formatting options were not preserved. I this presentation was aimed at my colleagues, it would have been ok-ish, but it was not for a conference presentation.\nAt this point I was already considering staying up all night re-doing the presentation in Power Point, and forgetting about Quarto for a while. In the last minute, when reading the RevealJS documentation, I found the suggestion of using decktape:\n\n\nMy salvation\n\nI was not very hopeful about this. decktape is written in JavaScript, which I‚Äôm not very familiar with, and I feared wasting too much time trying to make it work instead of just doing everything from scratch in Power Point. But it worked!. These are the steps I followed, according to the dektape docs.\n\nInstall Node.js\n\nInstall npm from the Node.js console:\n\nnpm install -g npm\nThen following the instructions in the decktape repository:\n\nInstall decktape from the Node.js console:\n\nnpm install -g decktape\n\nFinally, using the decktape command to download my slides deployed on Github Pages as a PDF:\n\ndecktape https://gongcastro.github.io/ips_2023_trajectories slides.pdf\nAnd it did the job, preserving the layout, SCSS formatting, and even the links!"
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html",
    "href": "blog/upfthesis/upfthesis.html",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "",
    "text": "So I submitted my thesis (üéâ). As anticipated, the submission process was frustrating‚Äîthis is a canonical event. Though, it was less of a pain thanks Quarto, which took care of the formatting. I spent a considerable amount of time preparing this template, which was a risky move, given the tight schedule I was on1, but it paid off in the end.\nIn this post, I describe how the upfthesis Quarto template works, and perhaps more importantly, how it can be adjusted to align with the dissertation format required by other universities. First, take a look at how it looks once rendered [link here, in case it does not show up]:\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#the-format",
    "href": "blog/upfthesis/upfthesis.html#the-format",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "The format",
    "text": "The format\nUPF currently requires the following sections, according to the guidelines listed by Guies BibTIC:\n\nCover: provided by the university, to be requested as a ticket (through CAU)2\nTitle page\nDedication (optional)\nAcknowledgements (optional)\nAbstract (English and Catalan)\nPreface\nTable of contents\nList of figures (optional)\nList of templates (optional)\nBody of the thesis\nBibliography\nGlossary (optional)\n\n2¬†Mind that they take from two to five working days to send it over.The guide also specifies the accepted formats (A4 or B5), fonts (Times New Roman, Arial, or Garamond), font size, and margins. Whenever possible, the upfthesis extension allows the user to choose between the options. Since my LaTeX skills are limited, I constrained some other options for convenience, but help is welcome making the template more flexible.\nThe main workhorse behind the upfthesis template is the Quarto books format, to which I added some tweaks adjust to the required format taking advantage of the LaTeX template offered by the secretariat. In addition, I used some LaTeX packages to define useful functions (I‚Äôll dig into this later).\nThe default output format of the upfthesis template is a PDF (upfthesis-pdf), which is the end product required by the thesis secretatiat. It is also possible to render the dissertation in Microsoft Word format (upfthesis-docx), Open Document format (upfthesis-odt), and HTML (upfthesis-html). While the upfthesis output in .docx and .odt formats is not submission-ready yet (word in progress), I found it useful for having my supervisor adding comments and for tracking changes across versions. The HTML format might be useful for publishing the dissertation as a website (work in progress)."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#installing",
    "href": "blog/upfthesis/upfthesis.html#installing",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Installing ‚¨áÔ∏è",
    "text": "Installing ‚¨áÔ∏è\nThis extension requires Quarto to be 1.3.0 or higher. I also recommend an updated version of MikTeX complications during compilation.\nquarto use template gongcastro/upfthesis\nThis will install the extension and create an set up the structure of the project. Take a look at the source code in the GitHub repository . If you are used to working from RStudio, I recommend you create a new RStudio project in the same folder."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#template-structure",
    "href": "blog/upfthesis/upfthesis.html#template-structure",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Template structure üìÅ",
    "text": "Template structure üìÅ\nThe downloaded directory contains several sub-directories and files:\n\n_thesis: contains the rendered thesis dissertation in whichever formats you have specified (e.g., thesis.pdf, thesis.docx)\n_quarto.yml: this is a YAML file with the global settings of the thesis. Here you can change the title, authors, department, etc. You can change most settings as indicated in the Book Options section of the Quarto reference. Some of these settings are internally set in the _extension.yml file; feel free to change them if you know what you are doing.\nchapters/ contains the Quarto (.qmd) files with the body of the thesis. Each section embedded in a different file. The chapters included are based on the Theses: Parts and content section of the Guies BibTIC UPF website. Some of these sections might not be required. If so, you may keep them from being included in the rendered output by removing them from the _quarto.yml file (chapters:) section. For example, here I am commenting out the Glossary section, which is not required:\n\nchapters:\n  - index.qmd\n  - chapters/01-introduction.qmd\n  - chapters/02-chapter-1.qmd\n  - chapters/03-chapter-2.qmd\n  - chapters/04-discussion.qmd\n  - chapters/05-bibliography.qmd\n  #- chapters/06-glossary.qmd\n\nNote that the numeric prefix in each .qmd file is only there for convenience: the order in which the files appear in the rendered output is determined by the order in which they appear in chapters:. Also, you may place these files wherever you want in the directory (e.g., in the base directory), as long as you indicate their right paths in chapters:.\n\n\nimg/ contains some images included in the example template. I personally find it more convenient to store all figures in a folder like this). Note that the .qmd files will look for the images (and any other refered resource) in the same folder they are located. In this template repository, the .qmd files are locating in the chapters/ folder, so images are found in the parent directory using double dots ../img/image.png.\n\n\nüí° You may create as many folders as you find convenient to store files that will be used in your dissertation. The img/ folder is one of those folders you may delete or rename. Just remember to change the file paths referring to the affected files accordingly.\n\n\nindex.qmd is an empty Quarto document that must be present in the main directory. Do not move or rename it. Any content inside this document will be rendered as a separate chapter right after the Table of Contents, which is usually not what one wants.This file must also be listed in chapters: in the _quarto.yml file.\nreferences.bib contains the BibTex references of the thesis. I you are using a reference manager (I strongly recommend using Zotero) you can export you library and replace this file with you desired references. You can also introduce you references manually in BibTex format (e.g., by copy-pasting them from Google Scholar). If you have more tha one .bib document or your .bib document is named differently, you can do the appropriate changes in the _quarto.yml file. For instance, this is how you can indicate more than one .bib file:\n\nbibliography:\n  - references.bib\n  - other-references.bib\n\n.gitignore: if you are a Git users, you might find this file convenient to avoid committing unwanted documents to your Git history.\napa.csl contains the code necessary to format the citations and bibliography in APA style. You may replace this file by whichever other style you find convenient by replacing apa.csl with the corresponding file from the citation-style-language repository. Remember to change the path in _quarto.yml if necessary."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#using-the-template",
    "href": "blog/upfthesis/upfthesis.html#using-the-template",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Using the template üöÄ",
    "text": "Using the template üöÄ\nTo render your dissertation to a PDF, you can use the Quarto command line in your console (Command Prompt/Power Shell in Windows, Terminal in MacOS, and command line in Linux):\nquarto render\nThis will render the thesis in PDF and Word formats by default. You can control in which format the theris is rendered this way:\nquarto render --to upfthesis-pdf\nIf you don‚Äôt want to render the thesis is either document ever, you can change the default behaviour in the _quarto.yml file, by changing:\nformat:\n  upfthesis-pdf: default\n  upfthesis-docx: default\nTo:\nformat:\n  upfthesis-pdf: default"
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#tricks-and-comments",
    "href": "blog/upfthesis/upfthesis.html#tricks-and-comments",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Tricks and comments üí°",
    "text": "Tricks and comments üí°\n\nHorizontal pages: this template defines the \\blandscape and \\elandscape commands (from the lscape LaTeX package) to flip pages to landscape mode. This wway some figures or tables may take more space and be easier to read. This is especially convenient for B5 format.\nLogo in colour: To use the logo in colour, just replace the logo.png file in the main directory with the official logo in colour.\nLow-level sections: for some reason, I have not been able to add section headers lower than 5 (e.g., 6, 7). I recommend reducing the number of sub-sections. This is issue is currently open (#7) and help is appreciated."
  },
  {
    "objectID": "blog/upfthesis/upfthesis.html#related-extensions",
    "href": "blog/upfthesis/upfthesis.html#related-extensions",
    "title": "upfthesis: a Quarto template for thesis dissertations at UPF",
    "section": "Related extensions",
    "text": "Related extensions\nSome folks have alreafy provided more generic templates for thesis dissertations, or have developed an extension for their specifc university. Take a look at them for inspiration for customising your own, or for learning nice tricks. Here are some of them, together with some blog posts in covering the topic.\n\nquarto-thesis by nmfs-opensci\nEnough Markdown to Write a Thesis by Richard J Telford\nWriting a dissertation in Quarto by rich Posert\nSome Quarto PDF formatting tips, with particular reference to thesis writing\nMonash Quarto Template by Rob J Hyndman"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Industry CV (PDF)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\nOctober 2018 - Present\nPhD, Biomedicine; Universitat Pompeu Fabra (Barcelona, Spain); Supervisor: N√∫ria Sebastian-Galles.\nOctober 2017 - July 2018\n\nMSc, Neurosciences; University of Barcelona (Barcelona, Spain). Dissertation: Phonemic Contrast Perception: A Segmentation Study on Monolingual and Bilingual Infants; Supervisors: N√∫ria Sebastian-Galles and Chiara Santolin.\n\nSeptember 2013 - July 2017\n\nBSc, Psychology; University of Oviedo (Oviedo, Spain). Dissertation: Effects of Environmental Enrichment on Attention, Spatial Reference Memory, and Cytochrome C Oxidase Activity; Supervisors: Azucena Begega and Marcelino Cuesta."
  },
  {
    "objectID": "cv.html#contributions",
    "href": "cv.html#contributions",
    "title": "CV",
    "section": "Contributions",
    "text": "Contributions\n\nArticles\nSantolin, C., Garc√≠a-Castro, G., Zettersten, M., Sebastian-Galles, N., Saffran, J. (2020). Experience with research paradigms relates to infants‚Äô direction of preference. Infancy, 00, 1‚Äì8.  URL  OSF  PsyArxiv   DOI  GitHub\nSampedro-Piquero, P., √Ålvarez-Su√°rez, P., Moreno-Fern√°ndez, R., Garc√≠a-Castro, G., Cuesta, M., Begega, A. (2018). Environmental enrichment results in both brain connectivity efficiency and selective improvement in different behavioral tasks. Neuroscience, 388, 374-383.  URL   Google Scholar"
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "CV",
    "section": "Conference presentations",
    "text": "Conference presentations\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2022). A longitudinal exploration of bilingual toddler‚Äôs processing of cognates. Presented at the International Congress on Infant Studies.  URL\nGarcia-Castro, G., Franco-Mart√≠nez, A., Rodr√≠guez-Prada, C., Castillejo, I., Sebastian-Galles, N. (2022). Estimando curvas de adquisici√≥n l√©xica en la infancia mediante un modelo de bayesiano de TRI. Presented at the XVII Congreso de Metodolog√≠a de las Ciencias Sociales y de la Salud.  URL"
  },
  {
    "objectID": "cv.html#proceedings",
    "href": "cv.html#proceedings",
    "title": "CV",
    "section": "Proceedings",
    "text": "Proceedings\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2022). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Proceedings of the 46th annual Boston University Conference on Language Development.  URL   DOI\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D., Sebastian-Galles, N., Plunkett, K. (2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presented at the Boston University Conference in Language Development, held online (Boston, United States).\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., Plunkett, K. (2021). An alternative approach to defining cross-linguistic phonological similarity using a model of monolingual speech recognition. Presentation at the Architectures and Mechanisms for Language Processing conference, held online (Paris, France).  URL   DOI\n\nPosters\nGarcia-Castro, G., Siow, S., Lepadatu, I., Guillen, N., Avila-Varela, D. S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The emergence of inhibitory links in the developing lexicon: insights from bilingual participants. Poster presented at the Workshop in Infant Language Development (San Sebasti√°n/Donostia, Spain).  URL\nZacharaki, K. E., Garcia-Castro, G., Sebastian-Galles, N. (June, 2022). Selective attention to the mouth of signing faces. Poster presented at the Workshop in Infant Language Development (San Sebasti√°n/Donostia, Spain).  URL\nZacharaki, K. E., Garcia-Castro, G., Sebastian-Galles, N. (June, 2022). Selective attention to the mouth of signing faces. Poster presented at the Workshop in Infant Language Development (San Sebasti√°n/Donostia, Spain).  URL\nSiow, S., Guillen, N., Lepadatu, I., Garcia-Castro, G., Avila-Varela, D. S., Sebastian-Galles, N., Plunkett, K. (August, 2021). A study of linguistic distance and infant vocabulary trajectories using bilingual CDIs of English and one additional language. Presentation at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., Plunkett, K. (August, 2021). The impact of cognateness on bilingual lexical access: a longitudinal priming study. Poster presented at the Lancaster Conference on Infant and Early Child Development, held online (Lancaster, United Kingdom).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (June, 2021). The role of cognateness in nonnative spoken word recognition. Poster presented at the XI International Symposium of Psycholinguistics (Madrid, Spain, held online).\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N. and Plunkett, Kim (July, 2020). The role of lexical similarity on bilingual parallel activation: A priming study in toddlers. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nGarcia-Castro, G., Avila-Varela, D. S., and Sebastian-Galles, N. (July, 2020). Does phonological overlap across translation equivalents predict earlier age of acquisition?. Poster presented at the International Congress on Infant Studies (Edimburgh, United Kingdom, held online). proceedings\nSiow, S., Garcia-Castro, G., Sebastian-Galles, N., and Plunkett, K. (August, 2019). The impact of phonology (cognateness) on the bilingual lexicon: Parallel cross-language phonological priming. Poster presented at the Lancaster Conference on Infant and Early Child Development (Lancaster, United Kingdom).\nGarcia-Castro, G., Marimon, M., Santolin, C., and Sebastian-Galles, N. (June, 2019). Encoding new word forms when contrastive phonemes are interchanged: A preliminary study on 8-month-old infants. Poster presented at the Workshop in Infant Language Development (Potsdam, Germany). https://doi.org/10.17605/OSF.IO/GYKUH\nGarc√≠a-Castro, G., √Ålvarez-Su√°rez, P., Garc√≠a-Abad, N., Cuesta, M., and Begega, A. (June, 2017). Pro-cognitive effects of environmental enrichment on attention and spatial memory: hippocampal metabolic activity in a Wistar rat model. Poster presented at the 4th International Congress on Health and Aging Research (Murcia, Spain).\nGarc√≠a-Abad, N., Santirso, M., Garc√≠a-Castro, G., and √Ålvarez-Su√°rez, P. (June, 2017). Efectos del omega-3 en las redes implicadas en la memoria de trabajo espacial en ratas Wistar. Poster presented at the 3rd National Congress of Psychology (Oviedo, Spain)\n\n\nInvited talks\nGarcia-Castro, G., Siow, S., Sebastian-Galles, N., and Plunkett, K. (March, 2022). The role of cognateness during word recognition in a non-native language. Talk given at the Psycholinguistics Coffee (University of Edinburgh, held online).  URL\nGarcia-Castro, G., Siow, S., Lepadatu, I., Guillen, N., Avila-Varela, D., Sebastian-Galles, N. y Plunkett, K. (January 2022). Parallel activation and the developing bilingual lexicon: a longitudinal study on word recognition. Talk given at the LACRE research group (Cardiff University, held online).\n\n\nScientific dissemination\nZacharaki, K., Garc√≠a-Castro, G. (2019, October). ‚ÄúDescubriendo la mente de los beb√©s‚Äù Talk presented at 13a Festa de la Ci√®ncia, Barcelona, Spain. [Link] [Video]\nGarc√≠a-Castro, G., Avila-Varela (2019, October). ‚ÄúEstudiando la mente de los beb√©s: Desde el lenguaje hasta la l√≥gica‚Äù Talk presented at Centre C√≠vic, Barcelona, Spain. [Link] [Video])\n\n\nRepositories\nGarc√≠a-Castro, G., Santolin, C., Marimon, M., and Sebastian-Galles, N. (2019, October 22th). Segmentation: Catalan and Spanish natural speech segmentation at 8 months of age. https://doi.org/10.17605/OSF.IO/42GUP)\nSantolin, C., Garc√≠a-Castro, G., Zettersten, M., Sebastian-Galles, N., & Saffran, J. (2020, March 5). Flip: Experience with research paradigms relates to infants‚Äô direction of preference. https://doi.org/10.17605/OSF.IO/G95UB"
  },
  {
    "objectID": "cv.html#others",
    "href": "cv.html#others",
    "title": "CV",
    "section": "Others",
    "text": "Others\n\nAnimal research\nResearch Staff Certificate for animal research, issued on 15/05/2018 by the Direcci√≥ General de Pol√≠tiques Ambientals i Medi Natural.\n\n\nSkills\n\nData processing: I particularly enjoy wrangling my way through messy data using the tidyverse family of packages. with some time, I can also google my way through Python.\nData analysis: Linear mixed models using R, both frequentist (lme4) and Bayesian (brms); reproducible reports using RMarkdown. I can also perform acoustic analysis on stimuli using Praat, and one of its R interfaces (PraatR).\nData visualisation using ggplot2. I can make animations using gganimate, and I‚Äôm currently learning to make maps, and Shiny Apps.\nDesigning experiments and questionnaires: I can program lab-based experiments using Matlab (PsychToolbox-3) and Python (Psychopy), online experiments using the PsychoPy/PsychoJS/Pavlovia workflow, and design reproducible online questionnaires using formR.\nGit/GitHub/Bitbucket\nJASP/SPSS"
  },
  {
    "objectID": "cv.html#languages",
    "href": "cv.html#languages",
    "title": "CV",
    "section": "Languages",
    "text": "Languages\n\nSpanish (native speaker)\nEnglish: C1\nFrench: C1"
  },
  {
    "objectID": "cv.html#short-courses",
    "href": "cv.html#short-courses",
    "title": "CV",
    "section": "Short courses",
    "text": "Short courses\nAugust 2018\n\nData Science: Multiple imputation in practice, Utrecht Summer School, Utrecht University. Introduction to Multiple Imputation as a powerful tool for dealing with missing data in experimental studies.\n\nAugust 2018\n\nData Science: Statistical Programming with R, Utrecht Summer School, Utrecht University.\n\nAugust 2016\n\nIntroduction to Neuroscience: From Molecule to Behavior, Radboud Summer School, Radboud University Nijmegen. Hands-on training at research labs of the Donders Institute for Brain and Cognition.\n\nAugust 2016\n\nHow to Improve the Quality and Translatability of Preclinical Animal Studies, Radboud Summer School, Radboud University Nijmegen. Course organized by SYRCLE institute, Radboud University Nijmegen and Radboud University Medical Centre. Theory and hands-on experience on Systematic Reviews and Meta-Analysis in preclinical animal studies. Overview of the main software available to conduct a Meta-Analysis and basic formation on academic reading and writing."
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html",
    "href": "blog/psicotuiterbot/psicotuiterbot.html",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "",
    "text": "Follow @psicotuiterbot"
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#escribiendo-el-c√≥digo",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#escribiendo-el-c√≥digo",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Escribiendo el c√≥digo",
    "text": "Escribiendo el c√≥digo\nUn bot de Twitter no es m√°s que un c√≥digo que se ejecuta autom√°ticamente de forma peri√≥dica (ej., cada 15 minutos) y realiza una acci√≥n en Twitter a trav√©s de una cuenta (hacer un RT o responder a un tweet). Para poder realizar esta acci√≥n a trav√©s del c√≥digo, es necesario tener acceso a la API de Twitter. API es un acr√≥nimo para Application Programming Interface y como dice su nombre, es una plataforma desde la que podemos interactuar con una aplicaci√≥n (en este caso Twitter) a trav√©s de programaci√≥n con una serie de comandos que el equipo de Twitter ha dise√±ado la API ha definido.\nNunca hab√≠a hecho un bot de Twitter, pero sab√≠a de la existencia de bastantes tutoriales para hacerlo. La mayor√≠a de los bots de Twitter (y por tanto de los tutoriales) est√°n escritos en Python y JavaScript, pero yo me encuentro algo m√°s c√≥modo con R. Adem√°s gran parte de Psicotuiter (especialmente quienes est√°n relacionades con la metodolog√≠a) tambi√©n est√° m√°s familiarizado con R. Mi intenci√≥n era hacer el bot lo m√°s trasparente y accesible para la comunidad, as√≠ que me decant√© por R. En un pricipio segu√≠ el tutorial de Matt Dray, en el que utiliza el paquete de R {rtweet} para interactuar con la API de Twitter. Hay m√°s tutoriales que usan rtweet para crear un bot con R. Pero a diferencia de otros, este tutorial explicaba c√≥mo usar GitHub actions para ejecutar el c√≥digo de forma peri√≥dica. Ahora explico esto √∫ltimo. Mientras tanto, vamos al c√≥digo de R.\nPrimero cre√© un repositorio de GitHub donde alojar el c√≥digo (GitHub es como un Google Drive especializado en c√≥digo donde adem√°s podemos hacer control de versiones de los archivos que subimos). Puedes echar un vistazo al c√≥digo de R en la carpeta R/. El c√≥digo principalmente recoge los √∫ltimos tweets que se han escrito en las √∫ltimos 6 horas mencionando #psicotuiter o #psicotwitter y los retuitea. No me meter√© en detalle a explicar c√≥mo funciona el c√≥digo, pero aqu√≠ va un peque√±o resumen del programa. Si tienes curiosidad te recomiendo explorar el repositorio de GitHub, que contiene todo lo necesario para hacer funcionar el bot:\nPrimero cargamos el paquete de R {dplyr}, que utilizamos en bastantes ocasiones en el programa, ajustamos un peque√±o detalle relacionado con el comportamiento de la API de Twitter, e instalamos los paquetes necesarios (en caso de que hayan cambiado) usando el pqeute de R {renv} (si no lo conoces y te interesa la reproducibilidad computacional tienes que echarle un vistazo).\n# bot R code\nlibrary(dplyr)\noptions(httr_oob_default = TRUE)\n# restore packages\nrenv::restore()\nA continuaci√≥n extraemos las credenciales que necesitamos para acceder a la API de Twitter (las paso a como variables de entorno para evitar hacerlas p√∫blicas, ya que eso der√≠a acceso a cualquiera a la cuenta de Twitter del bot).\n# authenticate Twitter API\nmy_token &lt;- rtweet::create_token(\n    app = \"psicotuiterbot\",  # the name of the Twitter app\n    consumer_key = Sys.getenv(\"TWITTER_CONSUMER_API_KEY\"),\n    consumer_secret = Sys.getenv(\"TWITTER_CONSUMER_API_KEY_SECRET\"),\n    access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n    access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"), \n    set_renv = FALSE\n)\nLuego extraemos los tweets usando rtweet, filtramos los que sean relevantes y no contengan posible contenido ofensivo (y alg√∫n otro filtro m√°s).\nlibrary(dplyr)\n# define hashtags\nhashtags_vct &lt;- c(\"#psicotuiter\", \"#psicotwitter\", \"#Psicotuiter\", \"#Psicotwitter\", \"#PsicoTuiter\", \"#PsicoTwitter\")\nhashtags &lt;- paste(hashtags_vct, collapse = \" OR \")\nhate_words &lt;- unlist(strsplit(Sys.getenv(\"HATE_WORDS\"), \" \")) # words banned from psicotuiterbot (separated by a space)\nblocked_accounts &lt;- unlist(strsplit(Sys.getenv(\"BLOCKED_ACCOUNTS\"), \" \")) # accounts banned from psicotuiterbot (separated by a space)\ntime_interval &lt;- lubridate::now(tzone = \"UCT\")-lubridate::minutes(120)\n# get mentions to #psicotuiter and others\nall_tweets &lt;- rtweet::search_tweets(\n    hashtags, \n    type = \"recent\", \n    token = my_token, \n    include_rts = FALSE, \n    tzone = \"CET\"\n) \nstatus_ids &lt;- all_tweets %&gt;% \n    filter(\n        !(screen_name %in% gsub(\"@\", \"\", blocked_accounts)),\n        created_at &gt;= time_interval, # 15 min\n        !grepl(paste(hate_words, collapse = \"|\"), text), # filter out hate words\n        stringr::str_count(text, \"#\") &lt; 4, # no more than 3 hashtags\n        lang %in% c(\"es\", \"und\") # in Spanish or undefined language\n    ) %&gt;% \n    pull(status_id)\n# get request ID\nrequest_tweets &lt;- rtweet::get_mentions(\n    token = my_token, \n    tzone = \"CET\"\n) \nFinalmente, hacemos RT uno a uno usando rtweet (si ya hab√≠amos hecho RT a uno de ellos simplemente se ignora).\nif (nrow(request_tweets) &gt; 0) {\n    request_ids &lt;- request_tweets %&gt;% \n        filter(\n            created_at &gt;= time_interval, # 15 min\n            grepl(\"@psicotuiterbot\", text),\n            grepl(\"rt|RT|Rt\", text),\n            !grepl(paste(hate_words, collapse = \"|\"), text) # filter out hate words\n        ) %&gt;% \n        pull(status_in_reply_to_status_id)\n    \n    # get requested IDS\n    if (length(request_ids) &gt; 0) {\n        requested_ids &lt;- rtweet::lookup_statuses(request_ids, token = my_token) %&gt;% \nposterior_preds &lt;- expand.grid(any1 = seq(min(dat$any1), max(dat$any1), by = 0.25),\n                               any2 = seq(min(dat$any2), max(dat$any2), by = 0.25),\n                               mes = unique(dat$mes)) %&gt;% \n  add_fitted_draws(fit3, n = 10) %&gt;% \n  ungroup() %&gt;% \n  left_join(select(dat, any, mes, temperatura) \n            mutate(intercept = fixef(fit3)$Estimate[1])\n            rowwise(y = fixef(fit3)[1,1] + any2)\n            ggplot(posterior_preds, aes(any1, .value, colour = any1, group = interaction(any2, .draw))) +\n              facet_wrap(~mes) +\n              geom_point(size = 0.1)\n            \n            \n                     if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::restore()\n      - name: Create and post tweet\n        run: Rscript R/bot.R\nLa gran ventaja de usar este sistema es que no necesitamos usar nuestro ordenador personal, ya que usamos el que que GitHub nos asigna (un servidor no deja de ser un ordenador). Pero tiene varios inconvenientes. El primero es que el proceso de establecer un workflow en GitHub Actions suele requerir varios intentos (en mi caso muchos). Esto suele deberse a problemas de reproducibilidad computacional: el c√≥digo funciona correctamente en mi ordenador porque en √©l tengo instalado todo el sofware del que depende. Cuando uso el servidor de GitHub, el sistema operativo suele necesitar que instalemos estas dependencias antes de ejecutar el c√≥digo. GitHub Actions permite cierta flexibilidad a la hora de seleccionar el software que viene instalado en el sistema operativo que vamos a usar (ej., R, compiladores de C++, dependencias de Linux, etc.). El problema es que muchas veces ni siquiera somos conscientes de cu√°ntas dependencias requiere nuestro c√≥digo. Con paciencia y muchas b√∫squedas de Google es posible solventar este problema.\nUn segundo inconveniente que encontr√© a la hora de implementar el bot en GitHub Actions tiene que ver con los tiempos: instalar todas las dependencias del c√≥digo en el servidor cada 15 minutos (la configuraci√≥n se pierde casi totalmente tras cada ejecuci√≥n) es poco eficiente. Instalar las dependencias puede tardar m√°s de 10 minutos (en el caso de este bot). Esto puede adem√°s hacer fallar en ocasiones el flujo de trabajo. GitHub Actions tampoco es lo m√°s consistente del mundo, aunque no deja de ser gratis."
  },
  {
    "objectID": "blog/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "href": "blog/psicotuiterbot/psicotuiterbot.html#segundo-intento-sale-bien-raspberry-pi",
    "title": "@psicotuiterbot: Un bot de Twitter para Psicotuiter",
    "section": "Segundo intento (sale bien): Raspberry Pi",
    "text": "Segundo intento (sale bien): Raspberry Pi\nTras varios problemas en la ejecuci√≥n del bot a trav√©s de GitHub Actions, decid√≠ cambiar de m√©todo. Por razones ajenas al bot, hac√≠a unos meses que ten√≠a muerta de aburrimiento una Raspberry Pi 4 que compr√© con un amigo para jugar con ella. Este dispositivo es un mini-ordenador relativamente barato (~40‚Ç¨) que sali√≥ al mercado como herramienta educativa para ense√±ar a programar (ej., r√≥botica para ni√±es) pero que poco a poco ha ido tomando espacio en lugares de producci√≥n. Tiene mil posibilidades por su simplicidad y, en nuestro caso, por su bajo consumo: tener una Raspberry Pi funcionando todo el d√≠a apenas tiene impacto sobre el consumo de luz.\n\n\n\nAqu√≠ est√° alojado el @psicotuiterbot\n\n\nPrimero instal√© el c√≥digo en la Raspberry con sus dependencias: b√°sicamente, clon√© el repositorio de GitHub en una carpeta dentro de home/Documents/. Para ejecutar el c√≥digo cada 15 minutos utilic√© una funci√≥n muy √∫til que incluye Linux (sistema operativo con el que funciona la Raspberry) llamado CRON. Simplemente consiste en un archivo en el que incluimos una serie de comandos que queremos que se ejecuten de forma peri√≥dica, junto con una c√≥digo que indica la periodicidad de la ejcuci√≥n de este comando. Aqu√≠ tienes unos ejemplos. Inclu√≠ cuatro comandos (cada uno en su propio archivo con extensi√≥n .sh, que denota comandos de Linux):\n# descarga el c√≥digo de GitHub, por si ha habido cambios\ngit pull origin main\n# ejecuta el c√≥digo principal del bot\nTZ=\"Spain/Madrid\" Rscript -e 'source(\"R/bot.R\")'\n# guarda los tweets detectados en un archivo y crea un gr√°fico\nRscript -e 'source(\"R/counts.R\")'\nrm Rplots.pdf\n# sube los nuevos datos a GitHub\ngit add .\ngit commit -m \"Update repository\"\ngit push\nEstos comandos se ejecutan en este orden cada 15 minutos."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Implementing the Differential Pathlength Factor (DPF) in Python: the Scholkmann method\n\n\n\n\n\n\n\n\n\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nupfthesis: a Quarto template for thesis dissertations at UPF\n\n\n\n\n\n\nr\n\n\nquarto\n\n\nthesis\n\n\nbook\n\n\npdf\n\n\n\nI wrote my thesis dissertation in Quarto, using a custom template. Here I illustrate how it works and some tips.\n\n\n\n\n\nJan 10, 2024\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nUsing decktape to convert Quarto slides from RevealJS to PDF\n\n\n\n\n\n\nr\n\n\nquarto\n\n\nrevealjs\n\n\npresentation\n\n\npdf\n\n\n\nUsing decktape to export my Quarto slides to PDF saved my life right before a submission deadline. I might save yours too.\n\n\n\n\n\nJun 8, 2023\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nGetting the most out of logistic regression\n\n\n\n\n\n\nr\n\n\ntidyverse\n\n\nregression\n\n\nstatistics\n\n\nlogistic\n\n\n\nLogistic regression models provide information way beyond a p-value. Using the {palmerpenguins} dataset, I review the relationship between the logistic and the logit functions, and how to interpret the coefficients of a logistic regression model, capitalising on marginal effects.\n\n\n\n\n\nJan 22, 2023\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nrenv (o c√≥mo usar paquetes de R sin ataques de p√°nico)\n\n\n\n\n\n\nr\n\n\npackage\n\n\nreproducibility\n\n\nrenv\n\n\n\n{renv} es un paquete de R que permite instalar paquetes de R gestionar sus versiones para proyectos de forma independiente. Aqu√≠ resumo para qu√© se utiliza y c√≥mo funciona.\n\n\n\n\n\nFeb 27, 2022\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\n@psicotuiterbot: Un bot de Twitter para Psicotuiter\n\n\n\n\n\n\nr\n\n\nrtweet\n\n\ntwitter\n\n\nraspberry pi\n\n\nbot\n\n\ngithub\n\n\n\nHe creado un bot de Twitter que hace RT a cualquier menci√≥n a #psicotuiter. El c√≥digo est√° escrito en R usando el paquete {rtweet} para interactuar con la API de Twitter, y est√° alojado en una Raspberry Pi que hace las veces de servidor ejecutando el c√≥digo cada 15 minutos usando CRON.\n\n\n\n\n\nDec 29, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nCreando un paquete de R: una gu√≠a informal\n\n\n\n\n\n\nr\n\n\npackage\n\n\ntwitter\n\n\ntutorial\n\n\n\nEn el canal de Twitch de Psicometries hicimos un directo en el que explicamos c√≥mo se ha creado el paquete de R {psicotuiteR}, indicando cada paso lo mejor que hemos podido para que puedas replicarlo, contribuir al mismo paquete o incluso crear tu propio paquete en el futuro.\n\n\n\n\n\nNov 14, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nExploring probability distributions through animations in Julia\n\n\n\n\n\n\njulia\n\n\ndistribution\n\n\nanimation\n\n\nstatistics\n\n\nprobability\n\n\n\nVisualising what different probability distributions look like under different parameters can be helpful when picking a likelihood function for you Bayesian analysis. I present some animations generated with Julia using Distributions.jl\n\n\n\n\n\nOct 4, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising polynomial regression\n\n\n\n\n\n\nr\n\n\nstatistic\n\n\nregression\n\n\nanimation\n\n\nggplot\n\n\n\nThe outputs of polynomial regression can be difficult to interpret. I generated some animated plots to see how model predictions change across different combinations of coefficients for 1st, 2nd, and 3rd degree polynomials.\n\n\n\n\n\nJan 21, 2021\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nHow similar is the word ‚Äúmask‚Äù across languages?\n\n\n\n\n\n\nr\n\n\nlinguistics\n\n\nphonology\n\n\nggplot2\n\n\ntranslation\n\n\n\nUsing the Levenshtein distance to quantify the orthographic and phonlogical similarity between translation equivalents of the word mask across multiple languages.\n\n\n\n\n\nNov 20, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nImporting data from multiple files simultaneously in R\n\n\n\n\n\n\nr\n\n\ntidyverse\n\n\nbase\n\n\npurrr\n\n\nimport\n\n\n\nA comparison between base R and Tidyverse methods for importing data from multiple files\n\n\n\n\n\nJul 5, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\n\n\n\n\n\n\nA primer on mixed-effects Models: theory and practice\n\n\n\n\n\n\nr\n\n\nstatistics\n\n\nmultilevel\n\n\nmixed models\n\n\nanimations\n\n\n\nSlides from a tutorial on mixed-effects models I presented to my research group.\n\n\n\n\n\nMar 31, 2020\n\n\nGonzalo Garcia-Castro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a cognitive scientist interested in the application of Bayesian modelling in the investigation of early language acquisition and its evolutionary roots. I am currently working as a post-doctoral researcher at the Institut de Rercerca Sant Joan de D√©u (Barcelona, Spain), were I use brain imaging (fNIRS), behavioural, and computational techniques to study how newborns, infants, and non-human animals perceive speech.\nI particularly enjoy building code-based workflows, ensuring that my projects can be run by others and my future self, from data collection to visualisation and reporting. I mainly use Twitter (@gongcastro) for sharing research outputs, programming tips or data visualisations. You can also follow me on GitHub for some portfolio projects like:\n\n{bvq}: R package for downloading and processing vocabulary data from 10-40 month-old children living in the Metropolitan Area of Barcelona (Spain) [website]\n{comidistar}: R package containing the scores given to products in the blind taste tests of El Comidista\npsicotuiterbot: A Twitter bot programmed in R using {rtweet} that automatically retweets mentions of the hasthtags #psicotuiter or #psicotwitter in Spanish"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html",
    "href": "blog/animated-distributions/animated-distributions.html",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "",
    "text": "In the last couple of years I went down the rabbit hole of Bayesian statistics. Although I‚Äôve made considerable progress, as compared to where I started, I still (and will) struggle understand some basic concepts beyond some shallow, abstract idea about what they mean. One of the first challenges I encountered in my first steps was becoming aware of how many probability distributions there are, and the fact that one should pick one should carefully chose which one to use when trying to estimate some parameter.\nBen Lambert‚Äôs excellent book A Student‚Äôs Guide to Bayesian Statistics offers a chart showing many of the most popular distributions, and how they relate to each other (pp.¬†145, see Figure¬†1 below for a similar chart). They then describe each distributions very plain terms, which is to be grateful for (many introductory books get lost in the details when describing likelihood distributions). However, when the moment arrives to pick a distribution in practice, I still struggle to see the risks and benefits of using each distribution. Will it adequately cover the sampling space of my parameter? Can I parameterise it so it allocates most of the probability around the regions I consider more likely? Am I actually able to interpret the values of the parameters of the distribution and map them to my research question?\nMy first step to answer these questions is to explore what a given distribution looks like under different parameters. This might give us a hint on the range of values it covers and the ‚Äúshape‚Äù of the likelihood function. As I mentioned in previous posts, I struggle to grasp statistical/mathematical concepts in absence of a good visualisation. I generated some animations in Julia using the Distributions.jl package, which provides a substantial amount of implemented likelihood functions. The only reason I chose Julia is that I‚Äôm trying to learn it step by step and I found this silly project a nice opportunity to do so. You can see the code at the end of this article or on the accompanying GitHub repository. I posted this one Twitter (see below) and got a good response, so I decided to extend a bit the contents. Enjoy! :)"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#normal-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#normal-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Normal distribution",
    "text": "Normal distribution\nWikipedia\nWell, we all know this one. This is a continuous distribution that covers the whole range of real numbers. Why is it used so often? Because most data we find in real life are frequently quite plausible under a normal distribution. Critically, the normal distributions makes very few assumptions about the data we are trying to model. Why is it so? As Richard McElreath explains in Statistical Rethinking1 (section 4.1), there are many ways a data generating mechanism might spit out normal data, even when each of the individual observations are not drawn from a normal distribution. This is because when the data are the result of adding (or multiplying), the resulting distribution tends to converge to a normal one. As McElreath points out, many phenomena we observe in nature is the result of adding multiple small factors together (e.g., someone¬¥s height is the result of adding multiple genetic and environmental factors together). Basically, whenever you are not sure about what distribution suits your data better, the normal distribution might be a good first approximation.\n1¬†You can also take a look a his excellent lecture on his YouTube channel, which cover much of the contents in the book!\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\big(\\frac{x-\\mu}{\\sigma}\\big)^2}\n\\]\nWhere \\(f(x)\\) us the probability density function, is the standard deviation, and is the mean.\n\n\n\n\n\n\nFigure¬†2: The normal distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#gamma-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#gamma-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nWikipedia\n\\[\nf(x) = \\frac{\\lambda(\\lambda x)^{\\alpha - 1}  \\;e^{-\\lambda x}}{\\Gamma(\\alpha)}\n\\]\nThis distribution only covers positive values, and has two parameters. It is in important one, as many other distribution, like the exponential, are specific cases of this one. In applied contexts, this distribution is used to model waiting times, and the incidence of some diseases, among others. In general, any continuous variable whose mode is expected to be closer rather than farther from zero can potentially be modelled by a Gamma distribution.\n\n\n\n\n\n\nFigure¬†3: The Gamma distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#inverse-gamma-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#inverse-gamma-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Inverse-gamma distribution",
    "text": "Inverse-gamma distribution\nWikipedia\nThis distribution is a reciprocal transformation of the Gamma distribution, and has similar properties. In can‚Äôt find many uses for this distribution if not as a conjugate prior for the variance of a normal distribution.\n\\[\nf(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1}  \\;exp\\Bigg(-\\frac{\\beta}{x}\\Bigg)\n\\]\n\n\n\n\n\n\nFigure¬†4: The inverse-gamma distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#exponential-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#exponential-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nWikipedia\nThis distribution is continuous and cover only positive values. It places most of the likelihood around zero, which makes it very convenient for modelling small positive quantities such as small distances or time intervals or standard deviations. Its shape depends on only one parameter, \\(\\lambda\\), which makes it simpler to parameterise (as compared to others like the Beta distribution). It has an inconvenient, though: it is a bit difficult to interpret what the value of this parameter means in the context of our research question (i.e., theory). While one can interpret the mean and standard deviation of the normal distribution as where the most likely value of the distribution lies and its associated uncertainty, respectively, the \\(\\lambda\\) parameter is not that trial to interpret: it doesn‚Äôt relate to the most likely values of the distribution linearly, and it cannot be interpreted a a rate or any other occurrence metric. What works for me is to take a look at the resulting distribution and see whether it captures my expectations about the data I‚Äôm about to observe.\n\\[\nf(x) = \\lambda e^{-\\lambda x}\n\\]\n\n\n\n\n\n\nFigure¬†5: The exponential distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#student-t-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#student-t-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Student-t distribution",
    "text": "Student-t distribution\nWikipedia\n\\[\nf(x) = \\frac{\\Gamma((\\upsilon+1)/2)}{\\sqrt{\\upsilon \\pi}  \\;\\Gamma (\\upsilon /2)}\n\\]\n\n\n\n\n\n\nFigure¬†6: The Student‚Äôs t distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#beta-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#beta-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Beta distribution",
    "text": "Beta distribution\nWikipedia\n\\[\nf(x) = \\frac{x^{\\alpha-1} \\;(1-x) \\;x^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\n\n\n\n\n\n\nFigure¬†7: The Beta distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#cauchy-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#cauchy-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Cauchy distribution",
    "text": "Cauchy distribution\nWikipedia\n\\[\nf(x) = \\frac{1}{\\pi \\gamma \\Bigg[1 + \\big( \\frac{x-x_0}{\\gamma} \\big)^2 \\Bigg]}\n\\]\n\n\n\n\n\n\nFigure¬†8: The Cauchy distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#frechet-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#frechet-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Frechet distribution",
    "text": "Frechet distribution\nWikipedia\n\\[\nf(x) = \\alpha \\; x^{-1-\\alpha} e^{-x^{-\\alpha}}\n\\]\n\n\n\n\n\n\nFigure¬†9: The Frechet distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#pareto-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#pareto-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Pareto distribution",
    "text": "Pareto distribution\nWikipedia\n\\[\nf(x) = \\frac{\\alpha  \\;x_{m}^{\\alpha}}{x^{\\alpha+1}}  \\;\\text{for}  \\; x &gt; x_m\n\\]\n\n\n\n\n\n\nFigure¬†10: The pareto distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#weibull-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#weibull-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Weibull distribution",
    "text": "Weibull distribution\nWikipedia\n\\[\nf(x) = \\lambda \\alpha(\\lambda x)^{\\alpha-1} \\; e^{-(\\lambda x)^{\\alpha}}\n\\]\n\n\n\n\n\n\nFigure¬†11: The Weibull distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#tri-weight-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#tri-weight-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Tri-weight distribution",
    "text": "Tri-weight distribution\nWikipedia\n\n\n\n\n\n\nFigure¬†12: The tri-weight distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#rayleigh-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#rayleigh-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Rayleigh distribution",
    "text": "Rayleigh distribution\nWikipedia\nSimilar to Gamma or Poisson, the Rayleigh likelihood function covers all positive values and relies on only one parameter, \\(\\sigma\\), to determine its shape and location. Lower values of \\(\\sigma\\) make the distribution lie closer to zero and be less disperse. Despite its seemingly less interesting appearance, this distributions is extensively used for modelling vibrational data such as water displacement, assessing the quality of railroads, or analysing MRI data2.\n2¬†See https://www.sciencedirect.com/topics/engineering/rayleigh-distribution\\[\nf(x) = \\frac{x}{\\sigma^2} \\; e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\n\n\n\n\n\nFigure¬†13: The Rayleigh distribution."
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#wigner-semi-circle-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#wigner-semi-circle-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Wigner semi-circle distribution",
    "text": "Wigner semi-circle distribution\nWikipedia\nThis distribution is, well, a semicircle when \\(R = 1\\), were \\(R\\) is the radius of the semi-circle. All values of the sampling space outside the [-R, R] interval have zero probability. I can‚Äôt say much more than that. I have no idea why this distribution even exists, and I have struggled to find documentation about it in the reasonably long period of time I searched for it. I suspect this distribution might be useful for geo-spatial modelling, where one wants to actually model the shape of an object. Otherwise, I‚Äôm lost.\n\\[\nf(x) = \\frac{2}{\\pi R^2} \\; \\sqrt{R^2 - x^2} \\\\ \\text{where} \\; \\pi \\; \\text{is the actual number } \\pi \\text{, not a parameter}\n\\]\n\n\n\n\n\n\nFigure¬†14: Wigner‚Äôs semicircle distribution.\n\n\n\nInterestingly, this distribution can be considered a particular case3 of the Beta distribution, so that where \\(\\alpha = \\beta = 3/2\\), then \\(X = 2RY ‚Äì R\\).\n3¬†See https://handwiki.org/wiki/Wigner_semicircle_distribution"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#triangular-distribution",
    "href": "blog/animated-distributions/animated-distributions.html#triangular-distribution",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Triangular distribution",
    "text": "Triangular distribution\nWikipedia\nAs a curiosity, there is such thing as a triangular distribution. It does look triangular, as can be seen in the Wikipedia article linked above. This distribution is not implemented yet in Distributions.jl. As with the semi-circle distribution, I struggle to see any context where this distribution might be useful beyond geo-spatial modelling.\n\\[\nf(x) = \\begin{cases}\n    0  & \\text{for}  \\; x &lt; \\alpha, \\\\\n    \\frac{2(x-a)}{(b-a)(c-a)} & \\text{for} \\; a \\leq x &lt; c, \\\\\n    \\frac{2}{b-a} &  \\text{for} \\; x = c, \\\\\n    \\frac{2(b-x)}{(b-a)(b-c)} & \\text{for} \\; c &lt; x \\leq b, \\\\\n    0 & \\text{for} \\; b &lt; x\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "blog/animated-distributions/animated-distributions.html#some-final-remarks",
    "href": "blog/animated-distributions/animated-distributions.html#some-final-remarks",
    "title": "Exploring probability distributions through animations in Julia",
    "section": "Some final remarks",
    "text": "Some final remarks\nSome well-known distributions are missing (e.g., Dirichlet, Wishart, Beta-binomial). This is because they have not yet been implemented in Distributions.jl, and I sadly lack the knowledge to contribute on this. Still feel free to explore the Wikipedia or other resources to get an idea of how these distributions look like in what contexts they are useful!"
  },
  {
    "objectID": "blog/renv-package/renv-package.html",
    "href": "blog/renv-package/renv-package.html",
    "title": "renv (o c√≥mo usar paquetes de R sin ataques de p√°nico)",
    "section": "",
    "text": "A veces necesitamos instalar versiones diferentes del mismo paquete de R en proyectos diferentes. El paquete {renv} nos permite almacenar los paquetes de R de cada proyecto de forma independiente, evitando posibles conflictos entre proyectos. De paso, incrementar√° la reproducibilidad computacional de nuestro c√≥digo.\n\n\nEl problema\nPonte en la siguiente situaci√≥n: tienes entre manos un proyecto de R que necesita varios paquetes. Cada uno de estos paquetes depende, a su vez, de terceros paquetes. De hecho, dos paquetes pueden depender del mismo paquete, o incluso de versiones diferentes del mismo paquete. Si hay mala suerte, una de las versiones no ser√° lo suficientemente reciente como para funcionar correctamente con ambos paquetes. Resultado: uno de los dos paquetes no funcionar√°.\n\n\n\nUsuarie de R promedio despu√©s de tirar dos horas a la basura intentando instalar los paquetes que necesita para trabajar en un proyecto de R que no tocaba desde hac√≠a cuatro meses.\n\n\nEste problema se extiende al caso de que necesitemos versiones diferentes del mismo paquete en proyectos de R diferentes en los que estamos trabajando de forma simult√°nea en el mismo ordenador. Para hacerlos funcionar necesitar√≠amos instalar de nuevo la versi√≥n correspondiente del mismo paquete cada vez que cambiemos de proyecto.\n\n\nInstalando paquetes de R\nEn resumidas cuentas, cada vez que instalamos o actualizamos un paquete de R, lo hacemos para todos nuestros proyectos de R de forma global. Esto se debe a que por defecto R busca todos los paquete de R en la misma carpeta. Para ver d√≥nde instala R tus paquetes puedes ejecutar el siguiente comando:\n.libPaths()\nEste comando te mostrar√° el directorio o directorios donde R instala sus paquetes por defecto. Si hay m√°s de un directorio significa que, en caso de que no sea posible encontrar un paquete en el primer directorio, R lo buscar√° en el segundo, tercero, etc., hasta que te devuelva un error indicando que no has instalado ese paquete.\nSi accedes al primer directorio que muestra .libPaths() ver√°s que cada paquete tiene una carpeta. Cada carpeta incluye el c√≥digo de R, los datos y la documentaci√≥n asociada a cada paquete (entre otras cosas). Cada vez que instalamos o actualizamos un paquete, se crea o reemplaza su carpeta correspondiente en nuestro directorio, es decir, en nuestra ‚Äúbiblioteca global‚Äù de paquetes de R.\n\n\n\nAs√≠ es como tienes tu carpeta de paquetes de R. Que lo s√© yo. Que te he visto. Verg√ºenza me dar√≠a a m√≠.\n\n\nHemos visto que esto no es ideal. ¬øNo ser√° mejor tener una carpeta diferente para cada proyecto en la que instalamos sus paquetes de forma independente, sin afectar a los paquetes de otros proyectos? S√≠. De hecho este procedimiento es est√°ndar en otros lenguajes de programaci√≥n como JavaScript1 o Julia 2. Existe un paquete de R que nos permite hacer esto: renv. Veamos c√≥mo funciona.\n1¬†Echa un vistazo a este post de Nikola ƒêuza: Ride Down Into JavaScript Dependency Hell2¬†Echa un vistazo a este post de Bogumi≈Ç Kami≈Ñski: My practices for managing project dependencies in Julia\n\nUsando renv\nPrimero hay que instalar renv. Como est√° incluido en el CRAN, podemos hacerlo usando install.packages():\ninstall.packages(\"renv\")\nAhora abrimos una sesi√≥n de R en la carpeta de nuestro proyecto. Digamos que nuestra carpeta tiene la siguiente estructura:\ndata\n |-some-data.csv\ndocs\n |-index.Rmd\n |-index.html\nR\n |-main.R\n |-functions.R\n.Rprofile\nAs√≠ es la estructura de la mayor√≠a de carpetas de mis proyectos de R. Tiene su raz√≥n de ser, pero eso es material para otro post. Lo importante es c√≥mo cambiar√° esta estructura en unos momentos. La documentaci√≥n de renv incluye los pasos para usar renv, pero explicar√© los principales. Primero inicializaremos renv en nuestra consola de R:\nrenv::init()\nEsto crear√° una carpeta (renv) y un archivo (renv.lock) nuevos en nuestro directorio:\n.Rprofile\ndata\n    |-some-data.csv\ndocs\n    |-index.Rmd\n    |-index.html\nR\n    |-main.R\n    |-functions.R\nrenv\n    |-.gitignore\n    |-activate.R\n    |-library\n        |-...\n    |-local\n        |-...\n    |-settings.dcf\nrenv.lock\nNo necesitaremos modificar ni consultar nunca ningunos de los archivos creados, pero vamos a curiosear un poco. Al usar init(), renv ha echado un vistazo a los scripts de R de la carpeta (achivos con la extensi√≥n .R, como main.R y functions.R), y ha detectado los paquetes que necesita nuestro c√≥digo para ejecutarse (puedes consultar las dependencias de tu proyecto usando renv::dependencies()). Por ejemplo, si main.R incluye library(dplyr) o dplyr::mutate(), detectar√° el paquete dplyr como una dependencia.\n\n\n\nrenv detectando tus dependencias.\n\n\nA continuaci√≥n, renv ha instalado todas los paquetes necesarios en renv/library/. Si comparas esa carpeta con el directorio mostrado en .libPaths() (como hicimos hace un momento), ver√°s que ambas carpetas son muy parecidas. Eso es porque ahora R buscar√° los paquetes que necesites en esa carpeta, y no en la ‚Äúbiblioteca global‚Äù de paquetes de R. Esa es la magia de renv: podr√°s instalar y actualizar paquetes de R de forma independiente para cada uno de tus proyectos. Para instalar nuevos paquetes deber√°s hacerlo usando la funci√≥n renv::install(). Por ejemplo:\nrenv::install(\"tidyr\")\nEsta funci√≥n es el equivalente a install.packages() en renv. esta funci√≥n asumir√° que el paquete que quieres se encuentra en CRAN y ser√° all√≠ donde lo buscar√°. Si el paquete que quieres instalar se encuentra alojado en otro sitio (o quieres instalar una versi√≥n experimental del mismo, en un repositorio de GitHub, por ejemplo), puedes indicar el repositorio de la siguiente forma:\nrenv::install(\"crsh/papaja\")\nSi echas un vistazo a renv.lock ver√°s que incluye una lista de todas las dependencias de tu proyecto, en un formato un poco raro, con muchos par√©ntesis, y la extensi√≥n .lock. No necesitas entender este archivo, s√≥lo que sigue un formato parecido al que usan otros leguajes de programaci√≥n para hacer lo mismo. Es el equivalente al archivo package-lock.json de un proyecto de JavaScript o al archivo Manifest.toml de un proyecto de Julia. Si te fijas, ver√°s que simplemente incluye informaci√≥n m√≠nima para cada paquete: nombre, versi√≥n, origen y un c√≥digo que lo identifica. Por ejemplo, el renv.lock de nuestro proyecto incluye lo siguiente:\n{\n  \"R\": {\n    \"Version\": \"4.0.4\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"dplyr\": {\n      \"Package\": \"dplyr\",\n      \"Version\": \"1.0.8\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"ef47665e64228a17609d6df877bf86f2\"\n    },\n    \"papaja\": {\n      \"Package\": \"papaja\",\n      \"Version\": \"0.1.0.9997\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"crsh\",\n      \"RemoteRepo\": \"papaja\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a231c3628ccf24359cc17f11a5bbc743e3fed920\",\n      \"Remotes\": \"tidymodels/broom\",\n      \"Hash\": \"3df0637229690f807616c46d3ff77113\"\n    },\n    \"tidyr\": {\n      \"Package\": \"tidyr\",\n      \"Version\": \"1.2.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"d8b95b7fee945d7da6888cf7eb71a49c\"\n    },\n  }\n}\nUna ventaja enorme de usar renv es que si descargas o copias y pegas esta carpeta en un ordenador diferente (en el que posiblemente tengas una colecci√≥n de paquetes diferente a la del ordenador donde trabajaste con el proyecto por √∫ltima vez), renv podr√° consultar este archivo para instalar por t√≠ los paquetes necesarios en sus versiones correspondientes. Esto se puede hacer usando el comando:\nrenv::restore()\nImportante: cuando instales nuevos paquetes usando renv::install(), el archivo renv.lock no se actualizar√° de forma autom√°tica. Para incluir los nuevos paquetes en este archivo, tendremos que usar el siguiente comando:\nrenv::snapshot()\nComo podr√°s imaginar, poder instalar los paquetes que necesita un proyecto en su versi√≥n adecuada resuelve uno de los problemas m√°s frecuentes que amenazan la reproducibilidad computacional de nuestros proyectos.\n\n\n\nImaginando un mundo donde todo el mundo se preocupa lo suficiente por la reproducibilidad computacional de sus proyectos.\n\n\n\n\nConclusiones\nTe recomiendo empezar a usar renv en alg√∫n proyecto ‚Äúde juguete‚Äù con el que puedas experimentar, e ir poco a poco incorporando esta rutina en tus proyectos por el bien de tu salud mental y de la de les dem√°s. :smile:\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{garcia-castro2022,\n  author = {Garcia-Castro, Gonzalo},\n  title = {Renv (o C√≥mo Usar Paquetes de {R} Sin Ataques de P√°nico)},\n  date = {2022-02-27},\n  url = {http://github.com/gongcastro/gongcastro.github.io/blog/renv-package/renv-package.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGarcia-Castro, G. (2022, February 27). renv (o c√≥mo usar paquetes de\nR sin ataques de p√°nico). http://github.com/gongcastro/gongcastro.github.io/blog/renv-package/renv-package.html"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Gonzalo Garc√≠a-Castro",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html",
    "title": "Visualising polynomial regression",
    "section": "",
    "text": "When modelling data using regression, sometimes the relationship between input variables and output variables is not very well captured by a straight line. A standard linear model is defined by the equation\n\\[y_i = \\beta_{0} + \\beta_{1}x_{i}\\]\nwhere \\(\\beta_{0}\\) is the intercept (the value of the input variable \\(x\\) where the output variable \\(y=0\\)), and where \\(\\beta_{1}\\) is the coefficient of the input variable (how much \\(y\\) increases for every unit increase in \\(x\\)). To illustrate this, let‚Äôs imagine we are curious abut what proportion of the students in a classroom are paying attention, and how this proportion changes as minutes pass. We could formalise our model as\n\\(y_i = \\beta_{0} + \\beta_{1} Time_i\\)\nLet‚Äôs generate some data to illustrate this example. Let‚Äôs say that, at the beginning of the lesson, almost 100% of the students are paying attention, but that after some time stop paying attention. Right before the end of the class, students start paying attention again.\nThe attention paid by the students did not decay linearly, but first dropped and rose up again, following a curvilinear trend. In these cases, we may want to perform some transformation on some input variables to account for this non-linear relationship. One of these transformations are polynomial transformations. In this context, when we talk about applying a polynomial function to a set of values, we usually mean exponentiating it by a positive number larger than 1. The power by which we exponentiate our variable defines the degree of the polynomial we are obtaining. Exponentiating our variable to the power of 2 will give us its second-degree polynomial. Exponentiating it by 3 will give us its third-degree polynomial, and so on. Back to our classroom example, we could add a new term to our regression equation: the second-degree polynomial of the input variable \\(Time\\), or even a third degree polynomial if we wanted to test to what extend our model follows a more complex pattern. Our regression trend will not be linear any more, but curvilinear. Let‚Äôs take a look at the anatomy of polynomials from a visual (and very informal perspective). Our model would look like this:\n\\[\ny_i = \\beta_{0} + \\beta_{1} Time_i + \\beta_{2} Time_{i}^2 + \\beta_{3} Time_{i}^3\n\\]\nAdding polynomial terms to our regression offers much flexibility to researchers when modelling this kind of associations between input and output variables. This practice is, for example, common in Cognitive Science when analysing repeated measures data such as eye-tracking data, where we register what participants fixated in a screen during a trial under several conditions. Polynomial regression could be considered as of the main techniques in the more general category of Growth Curve Analyis (GCA) methods. If you are interested in learning GCA, you should take a look at Daniel Mirman‚Äôs ‚ÄúGrowth Curve Analysis and Visualization Using R‚Äù [book].\nPowerful as this technique is, it presents some pitfalls, especially to newbies like me. For instance, interpreting the outputs of a regression model that includes polynomials can tricky. In our example, depending on the values of the coefficients \\(\\beta_{1}\\), \\(\\beta_2\\) and \\(\\beta_3\\)‚Äìthe first-degree and second-degree polynomials of \\(Time\\)‚Äìthe shape of the resulting curve will be different. The combination of values that these two coefficient can take is infinite, and so is the number of potential shapes our curve can adopt. Interpreting how the values of these coefficients affect the shape of our model, and more importantly, their interaction with other predictors of interest in the model can be difficult without any kind of visualisation. The aim of this post is to visualise how the regression lines of a regression model changes with the degree of its polynomials. For computational constraints, and to make visualisation easier, I will only cover one, two, and three-degree polynomials. I will generate plots for multiple combinations of the coefficients of these polynomials using the base R function poly() to generate polynomials, the R package ggplot2() to generate plots, and the gganimate R package to animate the plots. I will briefly describe what is going on in each plot, but I hope the figures are themselves more informative than anything I can say about them!"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#intercept",
    "title": "Visualising polynomial regression",
    "section": "Intercept",
    "text": "Intercept\nFirst, let‚Äôs start with how the value of the intercept (\\(\\beta_0\\)) changes the regression line for polynomials of different degree (1st, 2nd, and 3rd). I set the rest of the coefficients to arbitrary values for simplicity (\\(\\beta_1 = \\beta_2 = \\beta_3 = 1\\)). As you can see, regardless of the order of the polynomials involved in the model, increasing the intercept makes the line be higher in the Y-axis, and decreasing the value of the intercept makes the line be lower in the Y-axis. Simple as that.\n\nThe interpretation of the intercept is similar to how we interpret it in standard linear regression models. It tells us the value of \\(y\\) when all predictors are set to 0 (in our case \\(Time = 0\\)). As we will discuss later, what that means in practice depends on what that zero means for the other coefficients, that is, how we coded them. For now, let‚Äôs continue adding more terms to the equation."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#linear-term-adding-a-1st-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Linear term: adding a 1st-order polynomial",
    "text": "Linear term: adding a 1st-order polynomial\nNow let‚Äôs see how a linear model (with only a 1st degree polynomial) changes as we vary the value of \\(\\beta_1\\), the coefficient of the linear term \\(Time\\). As you can see, nothing special happens, the line just gets steeper, meaning that for every unit increase in \\(x\\), \\(y\\) increases (or decreases, depending on the sign) in \\(\\beta_1\\) units. When the coefficient equals zero, there is no increase nor decrease in \\(y\\) for any change in \\(x\\).\n\nWhen \\(\\beta_1=0\\), the resulting line is completely horizontal, parallel to the X-axis. This is what a model with just an intercept (\\(y = \\beta_{0}\\)) would look like. We generalise this to say that the linear model we just visualised is exactly the same as adding a 2nd and a 3rd degree polynomial to the model with their correspondent coefficients set to zero (\\(\\beta_2 = 0\\) and \\(\\beta_3 = 0\\), respectively)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#quadratic-adding-a-2nd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Quadratic: adding a 2nd-order polynomial",
    "text": "Quadratic: adding a 2nd-order polynomial\nNow things get a bit more interesting. When we add a second degree polynomial (\\(Time^2\\)), the line is not linear any more. If the coefficient of the 2nd-order polynomial (\\(\\beta_2\\)) is positive, the curve will go down and up in that order. When \\(\\beta_2 &lt; 0\\), the curve goes up and then down. When \\(\\beta_2 = 0\\), the curve turns out the be a line whose slope is defined by \\(\\beta_1\\), just like in the previous example.\n\nImportantly, varying the value of the coefficient of 1st-order polynomials (\\(\\beta_1\\)) also changes the shape of the curve: more positive values of \\(\\beta_1\\) make the curve ‚Äúfold‚Äù at higher values of \\(x\\). As you can see, when \\(\\beta_1 &lt; 0\\) (left panel, in blue), the point at which the curve starts increasing or decreasing occurs more to the left. When \\(\\beta_2 &gt; 0\\), this change occurs more to the right."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#cubic-adding-a-3rd-order-polynomial",
    "title": "Visualising polynomial regression",
    "section": "Cubic: adding a 3rd-order polynomial",
    "text": "Cubic: adding a 3rd-order polynomial\nFinally, let‚Äôs complicate things a bit more by adding a third-order polynomial. Now the curve will ‚Äúfold‚Äù two times. The magnitude of \\(\\beta_3\\) (the coefficient of the 3rd-degree polynomial) determines how distant both folding points are in the y-axis. When \\(\\beta_3\\) is close to zero, both folding points get closer, resembling the shape we‚Äôve seen in a model with just a 2nd-degree polynomial. In fact, when \\(\\beta_3 = 0\\), we get the same plot (compare the panel to the right-upper corner to the plot in the previous section). The sign of \\(\\beta_3\\) also determines whether the curve goes down-up-down or up-down-up: down-up-down if \\(\\beta_3 &lt; 0\\), and up-down-up if \\(\\beta_3 &gt; 0\\).\nThe magnitude of \\(\\beta_2\\) (the coefficient of the 2rd-degree polynomial) determines the location of the mid-point between both folding points. For more positive values of \\(\\beta_2\\) this point is located higher in the y-axis, while for more negative values of \\(\\beta_2\\), this point is located lower in the y-axis. This value is a bit difficult to put in perspective in our practical example. Probably \\(\\beta_1\\) is more informative: \\(\\beta_1\\) changes the value of \\(x\\) at which the curve folds. More negative values of \\(\\beta_1\\) make the curve fold at lower values of \\(x\\), while more positive values of \\(\\beta_1\\) make the curve fold at higher values of \\(x\\)."
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#conclusion",
    "title": "Visualising polynomial regression",
    "section": "Conclusion",
    "text": "Conclusion\nThere are way more things to say about polynomial regression, and it‚Äôs more than likely that I sacrifice accuracy for simplicity. After all, the aim of generating these animations was helping myself understand the outputs of polynomial models a bit more easily in the future. I hope it helps others too. If you consider something is misleading or inaccurate, please let me know! I‚Äôm the first interested in getting it right. Cheers!"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#just-the-code",
    "title": "Visualising polynomial regression",
    "section": "Just the code",
    "text": "Just the code"
  },
  {
    "objectID": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "href": "blog/visualising-polynomial-regression/visualising-polynomial-regression.html#session-info",
    "title": "Visualising polynomial regression",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.3.3 (2024-02-29 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.3.3    fastmap_1.2.0     cli_3.6.3        \n [5] htmltools_0.5.8.1 tools_4.3.3       yaml_2.3.9        rmarkdown_2.27   \n [9] knitr_1.48        jsonlite_1.8.8    xfun_0.46         digest_0.6.36    \n[13] rlang_1.1.4       renv_1.0.5        evaluate_0.24.0"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "",
    "text": "En este tutorial en colaboraci√≥n con Alicia Franco-Mart√≠nez 1, explicaremos c√≥mo se ha creado el paquete de R {psicotuiteR}. En el canal de Twitch de Alicia, Psicometries (al que tambi√©n deb√©is suscribiros), hicimos un directo en el que intentamos mostrar por encima lo que tratar√© en este tutorial. Trataremos de explicar cada paso lo mejor que podamos para que puedas replicarlo, contribuir al mismo paquete o incluso crear tu propio paquete en el futuro."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qu√©-es-el-paquete-psicotuiter",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qu√©-es-el-paquete-psicotuiter",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "¬øQu√© es el paquete {psicotuiteR}?",
    "text": "¬øQu√© es el paquete {psicotuiteR}?\n\n\nEl paquete psicotuiteR es un paquete muy simple que hemos creado para que la gente de la comunidad de #psicotuiter, en Twitter, pudiera experimentar con √©l a√±adiendo funciones o jugando con los datos que incluye. Pod√©is ver m√°s informaci√≥n sobre el paquete en su p√°gina web. La comunidad es psicotuiter es un grupo de castellanoparlantes que hablan, entre otras cosas, sobre psicolog√≠a y salud mental en Twitter."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qu√©-es-un-paquete-de-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qu√©-es-un-paquete-de-r",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "¬øQu√© es un paquete de R?",
    "text": "¬øQu√© es un paquete de R?\nEn el manual R packages de Hadley Wickham y Jenny Bryan, se describe un paquete de R as√≠:\n\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others.\n\nA m√≠ me gusta definirlo como un grupo de funciones documentadas que se agrupan siguiendo el formato que el alto consejo jedi2 ha dictado.\n2¬†La gente que gestiona CRAN, vaya. Tienen cierta fama de boomers.Por cierto, R packages es el mejor recurso que existe en este momento para aprender a hacer paquetes de R. No hay nada en este tutorial que no est√© incluido ah√≠‚Äìincluso mejor explicado‚Äìa excepci√≥n de alg√∫n que otro comentario autodespectivo al pie de p√°gina."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#por-qu√©-hacer-un-paquete-de-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#por-qu√©-hacer-un-paquete-de-r",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "¬øPor qu√© hacer un paquete de R?",
    "text": "¬øPor qu√© hacer un paquete de R?\nCuando programamos, es com√∫n que necesitemos ejecutar la misma l√≠nea de c√≥digo varias veces. Cuando esto ocurre, en lugar de escribir y ejecutar la misma l√≠nea de c√≥digo una y otra vez en la consola de R, podemos escribir un script. Un script de R es un archivo con la extensi√≥n .R3 que contiene las diferentes l√≠neas de c√≥digo que queremos ejecutar.\n3¬†Por convenci√≥n se suele usar la ‚ÄúR‚Äù may√∫scula, aunque la mayor√≠a de los sistemas operativos son indiferentes a que escribamos las extensiones en may√∫sculas o min√∫sculas. Hagas lo que hagas, trata de ser consistente.A veces el mismo script contiene l√≠neas de c√≥digo muy parecidas que ejecutamos para aplicar, por ejemplo, la misma funci√≥n sobre objetos diferentes. Por ejemplo:\nlm(x ~ y, data = df1)\nlm(x ~ y, data = df2)\nEn el bloque de c√≥digo de arriba, df1 y df2 son objetos de tipo data.frame con variables similares pero diferentes observaciones. Idealmente, no deber√≠amos repetir la misma l√≠nea de c√≥digo m√°s de una vez. Digo idealmente porque la alternativa es escribir una funci√≥n. No siempre merece la pena hacer una funci√≥n, por mucho que la gente m√°s purista insista. Si est√°s leyendo esto doy por hecho que te interesa hacer tu c√≥digo m√°s conciso y replicable.\nEn todo caso es generalmente recomendable definir una serie de funciones antes de trabajar sobre tus datos. Una funci√≥n es un conjunto de comandos que se ejecutan en orden cuando damos la orden4. Estos comandos se agrupan bajo el nombre que asignemos a la funci√≥n. As√≠ podemos condensar nuestro c√≥digo en funciones para que sea m√°s conciso.\n4¬†Definici√≥n mediocre pero obligada. Lo siento.En un √∫ltimo nivel de abstracci√≥n, nivel cerebro gal√°ctico, est√° agrupar nuestras funciones en un paquete. Hacer esto tiene unos cuantos beneficios:\n\nNo tendremos que definir las funciones cada vez que abramos un script: bastar√° con cargar el paquete y sus funciones estar√°n disponibles para usarlas.\nSer√° m√°s f√°cil compartir nuestro c√≥digo con otras personas: es com√∫n que nuestras funciones requieran, tener instalados ciertos paquetes externos. Por ejemplo, si mi funci√≥n usa la funci√≥n mutate del paquete dplyr, quien quiera usar mi funci√≥n deber√° tener instalado dplyr (a veces es inluso necesario tener instalada la misma versi√≥n del paquete). Este es uno de las principales amenazas a la reproducibilidad de nuestro c√≥digo. Un paquete de R, sin embargo, se asegura que, durante la instalaci√≥n, se instalen las dependencias necesarias en el ordenador de la otra persona,\nPodremos documentar las funciones f√°cilmente (se acabaron los comentarios escuestos e indescifrables en nuestros scripts) y hacer esta documentaci√≥n accesible a quien use nuestro paquete al ejecutar ?mifuncion (puedes ejecutar ?mean para ver un ejemplo de c√≥mo se ver√° nuestra documentaci√≥n).\n\nHay m√°s motivos por los que puede ser buena idea crear un paquete de R, como por ejemplo trabajar con tu propio c√≥digo de forma m√°s c√≥moda y reproducible5, para compartir y documentar bases de datos (a veces nuestro paquete no incluir√° ninguna funci√≥n, sino √∫nicamente unos datos y su documentaci√≥n) o para aprender R m√°s a fondo y sus entresijos. Yo he aprendido m√°s intentando hacer paquetes de R que en cualquer tutorial.\n5¬†Tu yo futuro te lo agradecer√° y si alg√∫n d√≠a se puede viajar en el tiempo lo har√°s para darte un beso en la frente por ello."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qu√©-necesito-para-hacer-un-paquete-de-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#qu√©-necesito-para-hacer-un-paquete-de-r",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "¬øQu√© necesito para hacer un paquete de R?",
    "text": "¬øQu√© necesito para hacer un paquete de R?\nPara seguir este tutorial, y en general para crear un paquete de R necesitaremos instalar en nuestro ordenador6:\n6¬†En el momento de escribir este tutorial yo estoy usando R 4.0.4, RStudio 1.4.1103, rmarkdown 2.11.1, devtools 2.4.2 y usethis 2.0.1. Echa un vistazo a esta gu√≠a para ver c√≥mo instalar una versi√≥n espec√≠fica de un paquete de R.\nR\nRStudio\nLos siguientes paquetes de R: devtools, usethis y rmarkdown.\n\nPuedes instalar estos paquetes as√≠:\ninstall.packages(\"devtools\", \"usethis\", \"rmarkdown\")\nCuando tengamos todo instalado, reiniciaremos nuestra sesi√≥n de RStudio y despu√©s cargaremos devtools y usethis (usaremos rmarkdown m√°s adelante):\nlibrary(devtools)\nlibrary(usethis)"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#inicializando-el-paquete",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#inicializando-el-paquete",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "Inicializando el paquete",
    "text": "Inicializando el paquete\nSalvo contadas excepciones7, las personas de bien utilizaremos RStudio para programar en R, en lugar de usar √∫nicamente la consola como hacen les psic√≥patas. Trabajar en un proyecto de RStudio nos facilitar√° mucho la vida al hacer un paquete de R y deber√≠as hacerlo casi siempre que programas en R (como m√≠nimo, te ahorrar√° mucho tiempo buscando documentos dentro de carpetas)8.\n7¬†Por ejemplo, si tienes el suficiente tiempo libre como para configurar una sesi√≥n de R medianamente funcional en Visual Studio Code.8¬†Nunca est√° dem√°s ver este tutorial de Danielle Navarro sobre c√≥mo organizar un repositorioPara crear un paquete tenemos dos opciones: hacerlo a trav√©s de RStudio (ver tutorial) o usando usethis en nuestra consola. A mi me gusta la segunda opci√≥n:\ncreate_package(path = \"psicotuiteR\") # abre nueva ventana\nEste comando crear√° una nueva carpeta. Lo har√° dentro de tu repositorio de inicio, el cual puedes consultar ejecutando getwd() en tu consola (asumiendo que nos has cambiado el directorio por tu cuenta previamente). Esta carpeta contendr√° varios archivos y una carpeta:\n.gitignore\n.Rbuildignore\nDESCRIPTION\nNAMESPACE\npsicotuiteR.Rproj\nR\n\n\n\n\n\n\nImportant\n\n\n\nLos nombres de la mayor√≠a de archivos y carpetas en este directorio son importantes. Trata de no cambiarlos si no es imprescindible.\n\n\nUno de los archivos en la carpeta tiene el mismo nombre que el paquete y la extensi√≥n .Rproj. Cada vez que queramos trabajar en nuestro paquete es recomendable abrir la sesi√≥n de RStudio haciendo doble click sobre el archivo .Rproj. Veremos qu√© son el resto de archivos m√°s adelante.\nPues bien: t√©cnicamente, ¬°ya tenemos un paquete de R! Si ejecutamos el c√≥digo de abajo, el paquete se instalar√° como si fuera uno m√°s en nuestro directorio de paquetes de R.\ndevtools::install()\nEncontrar√°s la carpeta de instalaci√≥n junto a la de los dem√°s paquetes que hays instalado en tu ordenador. Puedes consultar d√≥nde se instalan tus paquetes de R ejecutando .libPaths() en tu consola. La primera ruta que aparezca ser√° donde encontrar√°s tu paquete instalado (en mi caso, encontrar√© la carpeta ~/Documents/R/win-library/4.0/psicotuiteR).\nLa funci√≥n que hemos ejecutado, install() del paquete devtools simula lo que otra persona har√≠a al ejecutar la funci√≥n install.packages() si nuestro paquete estuviera disponible en CRAN. Ahora mismo, podr√≠amos cargar nuestro paquete con library(psicotuiteR) y trabajar con √©l. Por supuesto, nuestro paquete a√∫n est√° vac√≠o. En las pr√≥ximas secciones veremos c√≥mo a√±adir funciones, entre otras cosas, a nuestro paquete.\nAntes de hacer eso, ah√≠ va un truco: cuando a√±adimos o hacemos cambios en el paquete, necesitaremos actualizar nuestra sesi√≥n y cargar el paquete de nuevo y volver a ejecutar las funciones para comprobar que todo funciona correctamente. En lugar de ejecutar install() cada vez que queremos ver si nuestro paquete funciona, podemos ejecutar load_all() (tambi√©n del paquete devtools) sin siquera reiniciar la sesi√≥n y as√≠ cargar de nuevo el paquete actualizado como si alguien hubiera cargado el paquete usando library(). Los contenidos del paquete que se cargar√°n usando load_all() son los que hay en el nuestro repositorio (desde el que estamos desarrollando el paquete), y no desde el directorio en el que se instala el paquete si ejecutamos install(). ¬°Esto es mucho m√°s r√°pido y eficiente!\nOtro truco: puesto que vamos a utilizar las funciones de los paquetes devtools y usethis a menudo‚Äìy por lo tanto vamos a necesitar cargar estos paquetes mediante library(devtools) y library(usethis) cada vez que iniciemos una sesi√≥n de R‚Äì podr√≠a ser recomendable a√±adir esos dos comandos a nuestro archivo .Rprofile (ver esta gu√≠a para m√°s informaci√≥n sobre .Rprofile. Las l√≠neas de c√≥digo que contenga ese archivo ser√°n ejecutadas en cada inicio de sesi√≥n que hagamos en nuestro proyecto de RStudio. Podemos hacer esto usando las siguientes funciones de usethis:\nlibrary(usethis)\nuse_usethis() \nuse_devtools()\nEsto crear√° un archivo llamado .Rprofile en nuestro directorio y a√±adir√°, entre otras cosas muy √∫tiles, los comandos library(usethis) y library(devtools)."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#description",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#description",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "DESCRIPTION",
    "text": "DESCRIPTION\nEl primer archivo que vamos a explicar se llama DESCRIPTION. La mayor parte de la informaci√≥n general (o meta-informaci√≥n) de nuestro paquete se encuentra en este archivo: autor√≠a, ajustes generales, dependencias, etc. Puedes consultar el DESCRIPTION de psicotuiteR para hacerte una idea de c√≥mo se ve cuando est√° editado. ¬°Presta especial atenci√≥n a c√≥mo hemos indicado la autor√≠a!\nDESCRIPTION es uno de los pocos archivos que tendremos que editar tanto a mano como mediante otras funciones del paquete usethis. Tendremos que cambiar el t√≠tulo, autor√≠a y descripci√≥n del paquete a mano (adem√°s de alg√∫n otro campo), mientras que, por ejemplo, los campos Imports, Depends y Suggests ser√°n rellenados m√°s adelante mediante la funci√≥n use_package(), de usethis. Cuando hablemos de dependencias, m√°s adelante, comentaremos un par de cosas saobre estos tres campos."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#funciones-en-r",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#funciones-en-r",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "Funciones en R",
    "text": "Funciones en R\nLas funciones de R son el cuerpo principal de un paquete de R. Contienen el c√≥digo que se ejecutar√° en nuestras funciones y su correspondiente documentaci√≥n. Las funciones de por s√≠ no tienen gran misterio. Las puedes hacer m√°s simples o m√°s complejas. Generalmente, por motivos de legibilidad suele ser mejor mantener nuestras funciones lo m√°s simples que podamos. Es mejor escribir muchas funciones que hacen tareas peque√±as que pocas funciones que lo hacen todo. No tienes por qu√© hacer disponibles todas las funciones que escribas: puedes mantener algunas funciones para uso interno dentro de otras funciones que s√≠ est√°n disponibles al p√∫blico (ahora veremos c√≥mo). Hay muchos tutoriales para aprender a hacer funciones en R. Por ejemplo este. Merece la pena ganar algo de confianza en poder hacer funciones de R: empodera mucho y te hace entender muchos de los errores que te encontrar√°s a lo largo de tu vida programando en R. Ah√≠ va un ejemplo de funci√≥n muy simple:\nprint_name &lt;- function(\n    author = \"Gon\"\n){\n    print(author)\n}\nEsas l√≠neas de c√≥digo definen la funci√≥n print_name(). Esta funci√≥n incluye un argumento llamado author, que, por defecto, toma el valor \"Gon\". Si definimos la funci√≥n y ejecutamos print_name() en nuestra consola de R, nos devolver√° el valor `‚ÄúGon‚Äù.\nGuardaremos esta funci√≥n en un archivo con la extensi√≥n .R dentro de la carpeta R/ en nuestro repositorio principal (el nombre que pongamos a este archivo da igual, pero trata de que sea informativo de su contenido). Yo llamar√© a este archivo print_name.R. Ahora nuestro directorio se ve as√≠:\n.gitignore\n.Rbuildignore\nDESCRIPTION\nNAMESPACE\npsicotuiteR.Rproj\nR\n    |-print_name.R"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#documentaci√≥n",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#documentaci√≥n",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "Documentaci√≥n",
    "text": "Documentaci√≥n\nAhora vamos a documentar esa funci√≥n. R utiliza un tipo de lenguaje parecido a LaTeX para escribir la documentaci√≥n de un paquete. Este lenguaje se llama R documentation. T√©cnicamente, podr√≠amos escribir toda la documentaci√≥n de cada funci√≥n de nuestro paquete en este lenguaje y guardar cada archivo en la carpeta man/ con la extensi√≥n .Rd. En esta carpeta es donde R espera encontrar la documentaci√≥n. Gracias a Dios (y a la buena voluntad de la comunidad de R), podemos incluir los contenidos de esos archivos como si fueran comentarios en nuestros scripts de R, encima de cada funci√≥n. La funci√≥n document() de devtools se encargar√° de generar todos los .Rd necesarios en la carpeta man/ por nosotrxs. Volveremos a esto m√°s adelante. Por ahora, observa un ejemplo de funci√≥n documentada:\n#' Print author\n#' @export print_name\n#' @usage print_author()\n#' @import dplyr\n#' @importFrom tidyr drop_na\n#' @description Print the name of the author of the package we are developing\n#' @param author Name of the package author\n#' @author Gonzalo Garcia-Castro\nprint_name &lt;- function(\n    author = \"Gon\"\n){\n    print(author)\n}\nEn la parte de abajo encontramos las mismas l√≠neas de c√≥digo que hemos visto antes. En la parte de arriba encontramos la documentaci√≥n de la funci√≥n. Estas l√≠neas de c√≥digo est√°n precedidas por el s√≠mbolo #'. La ap√≥strofe indica que no es un comentario cualquiera, sino parte de la documentaci√≥n de la funci√≥n. En la mayor√≠a de estas l√≠neas indicamos mediante el s√≠mbolo @ qu√© campo estamos describiendo (autor√≠a, descripci√≥n, un argumento, etc.).\nPor ejemplo, la l√≠nea de abajo indica que estamos describiendo uno de los argumentos de la funci√≥n (por alguna raz√≥n que desconozco, se ha decidido refererirse a los argumentos como ‚Äú@param‚Äù y no como @arg‚Äú). La primera l√≠nea se asume que es el t√≠tulo de la funci√≥n y por eso no hace falta indicar @title antes de \"Print author\".\n#' @param author Name of the package author\nUna vez hayamos rellenado la documentaci√≥n de nuestra funci√≥n, ejecutaremos document() y se generar√° autom√°ticamente un archivo llamado print_name.Rd en la carpeta man/. Podemos comprobar que la documentaci√≥n se ha guardado correctamente ejecutando ?print_name. Se deber√≠a abrir la ventana Help en uno de los paneles de RStudio. Algunos consejos cuando rellenes la documentaci√≥n de tus funciones:\n\nExplica las cosas con claridad, pero no tengas miedo de extenderte o repetir las cosas. M√°s documentaci√≥n siempre es mejor que menos documentaci√≥n, siempre que se expliquen las cosas con claridad y se mantenga cierto sistema en la estructura de la documentaci√≥n.\nEcha un vistazo a la documentaci√≥n de las funciones princpiales de tus paquetes favoritos. Es la mejor forma de saber c√≥mo documentar un funci√≥n y qu√© campos son los m√°s importantes.\n\nVi√±etas y art√≠culos\nUna forma m√°s elaborada de documentar un paquete es crear vi√±etas. Una vi√±eta (o vignette, en ingl√©s) es un documento en el que se explica con detalle c√≥mo se trabaja con el paquete, como si fuera un tutorial. Las vi√±etas son particularmente √∫tiles para quienes usan el paquete por primera vez, y deber√≠an ilustrar, como m√≠nimo, alg√∫n ejemplo sobre c√≥mo se usan las funciones. Un buen ejemplo de vi√±eta es esta, del paquete {dplyr}, en la que indican c√≥mo usar la funci√≥n group_by(). Para crear una vi√±eta ejecutaremos el siguiente comando:\nuse_vignette(\n  name = \"print-name\" # as√≠ se llamar√° el archivo,\n  title = \"Imprimiendo un nombre\" # as√≠ se titular√° la vi√±eta\n)\nEste comando crear√° una carpeta llamada vignettes/ y crear√° un archivo con la extensi√≥n .Rmd (Rmarkdown). Los archivos Rmarkdown son una mezcla entre un archivo Markdown (que tienen la extensi√≥n .md) y un script de R. Markdown es un lenguaje de edici√≥n de textos bastante sencillo (desde luego m√°s sencillo que LaTeX). Rmarkdown permite intercalar bloques de c√≥digo de R en medio del texto. Cuando renderizamos el documento, esos bloques de c√≥digo se ejecutan y el resultado se incluye como texto o imagen. Este tipo de documentos son muy √∫tiles para crear informes y actualizar los datos que incluyen con s√≥lo un click. Si quieres aprender a user Rmarkdown (100% recomendado), el manual R Markdown: The Definitive Guide de Yihui Xie es el mejor recurso. Una vez escribamos nuestra vi√±eta podremos incluirla en nuestra documentaci√≥n ejecutando:\nbuild_vignettes()\ndocument()\nComo dato curioso, este mismo tutorial que est√°s leyendo es una vi√±eta incluida en el paquete psicotuiteR. Muy meta todo, ¬øverdad?"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#dependencias-y-otras-pesadillas",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#dependencias-y-otras-pesadillas",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "Dependencias y otras pesadillas",
    "text": "Dependencias y otras pesadillas\nCon frecuencia nuestras funciones de R depender√°n, a su vez, de funciones de otros paquetes. Considera la siguiente funci√≥n:\n#' Una funci√≥n que a√±ade una variable\n#' @param x Una serie de caracteres\n#' @returns El n√∫mero incluido en x\nextrae_numeros &lt;- function(x){\n  y &lt;- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nComo indica la documentaci√≥n, esta funci√≥n devuelve los n√∫meros incluidos en x, una serie de caracteres que introducimos como argumento, en formato num√©rico. Para ello utiliza la funci√≥n str_extract(). Esta funci√≥n pertenece al paquete {stringr}. Tal y como est√° escrita esta funci√≥n, cuando instalemos el paquete y la ejecutemos no dar√° un error: R nos indicar√° que la funci√≥n str_extract no existe. Podr√≠a ocurr√≠rsenos usar algo como stringr::str_extract o library(stringr) dentro de la funci√≥n. Pero esto no es buena idea porque muchas veces no funcionar√°. Necesitamos incluir la funci√≥n str_extract y stringr como dependencias de nuestro paquete, para que cuando alguien instale el paquete esa funci√≥n se encuentre disponible para el paquete cuando ejecutemos nuestra funci√≥n extrae_numeros(). Para indicar una dependencia, debemos hacerlo en dos pasos:\n\nEjecutar use_package(\"stringr\") para incluir stringr en la lista de paquetes que deben instalarse junto al nuestro. Al hacerlo, ver√°s que stringr ha sido incluido en el campo Imports del archivo DESCRIPTION.\nIncluir la funci√≥n str_extract del paquete stringr en la documentaci√≥n de la funci√≥n de la siguiente forma:\n\n#' @importFrom stringr str_extract\nAhora nuestra funci√≥n deber√≠a verse as√≠:\n#' Una funci√≥n que a√±ade una variable\n#' @param x Una serie de caracteres\n#' @importFrom stringr str_extract\n#' @returns El n√∫mero incluido en x\nextrae_numeros &lt;- function(x){\n  y &lt;- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nLa funci√≥n str_extract estar√° disponible y nuestra funci√≥n podr√° ejecutarse sin problemas. Otra opci√≥n ser√≠a importar el paquete stringr al completo, con todas sus funciones. En lugar de indicar (importFrom?) con el nombre del paquete y la funci√≥n que queremos importar, indicar√≠amos @import y solamente nombre del paquete: #' @import stringr. Sin embargo, casi siempre es mejor indicar las dependencias una a una, aunque sea repetitivo: as√≠ ser√° m√°s f√°cil detectar de d√≥nde viene cada funci√≥n que usamos. Si vamos a usar varias funciones del mismo paquete, podemos indicarlas una debajo de otra:\n#' @importFrom stringr str_extract\n#' @importFrom stringr str_detect\n#' @importFrom stringr str_replace\nS√≥lo en el caso de que utilicemos much√≠simas funciones del mismo paquete en nuestra funci√≥n tendr√° cierto sentido importar el paquete al completo. Las dependencias de nuestro paquete s√≥lo estar√° disponibles cuando ejecutemos document() y se actualice la documentaci√≥n. Esto es porque document() no s√≥lo construye la documentaci√≥n del paquete (los archivos .Rd en man/), sino que tambi√©n modifica el archivo NAMESPACE para enumerar las dependencias. Echa un vistazo a NAMESPACE de psicotuiteR. Este archivo ha sido generado al ejecutar document(). Una de nuestras funciones incluye #' @import janitor clean_names y por tanto esta funci√≥n ha sido incluida en el NAMESPACE.\nHemos aprendido a indicar dependencias de otros paquetes en nuestro c√≥digo atrav√©s de la documentaci√≥n: funciones que nuestras propias funciones necesitan. Para hacer nuestras funciones disponibles en el paquete, necesitaremos incluirlas tambi√©n en el NAMESPACE, pero no como dependencias, sino como exportaciones: en lugar de incluirlas en la documentaci√≥n usando @importFrom, lo haremos usando #' @export. Volviendo al ejemplo anterior: #' @export extrae_numeros. Nuestra funci√≥n se ver ahora as√≠:\n#' Una funci√≥n que a√±ade una variable\n#' @export extrae_numeros\n#' @param x Una serie de caracteres\n#' @importFrom stringr str_extract\n#' @returns El n√∫mero incluido en x\nextrae_numeros &lt;- function(x){\n  y &lt;- as.numeric(str_extract(x, \"\\\\d\"))\n  return(y)\n}\nLa segunda l√≠nea indica que est√° funci√≥n debe estar disponible en nuestro paquete con el nombre extrae_numeros. Aunque es posible exportar la funci√≥n con un nombre diferente al que le hemos asignado en el script (R nos dar√° un aviso, pero no un error), es importante que as√≠ sea.\nPodr√≠as preguntarte: ¬øqu√© sentido tiene crear funciones si no van a estar disponibles para quien use el paquete? Pues porque a veces es recomendable hacer funciones peque√±as para uso interno que, aunque su funcion no es de gran inter√©s para el p√∫blico, ayuden a otras funciones m√°s complejas que s√≠ tienen sentido que use el p√∫blico. Sea como sea, t√©cnicamente s√≠ se puede acceder a este tipo de funciones usando el operador :::.\nSi quieres hacer la prueba, ve a la consola de R y compara las sugerencias que salen cuando escribes usethis:: y usethis:::. Todas esas funciones que salen en el segundo caso no est√°n disponibles cuando cargamos el paquete usando library() porque las personas que han creado el paquete usethis han considerado que no las necesitamos, aunque s√≠ las necesitan las funciones que s√≠ usamos.\nEn resumen, cuando ejecutemos document(), se incluir√°n en el NAMESPACE las funciones que hayamos indicado en la documentaci√≥n mediante #' @export. Nunca modificaremos NAMESPACE archivo a mano, sino que cambiaremos sus contenidos modificando la documentaci√≥n de nuestras funciones en el script de R y luego ejecutaremos document(). Con toda probabilidad, la mayor√≠a de los problemas que vas a encontrar al desarrollar un paquete de R (tambi√©n los m√°s frustrantes) se deban a sus dependencias. Con el tiempo aprender√°s a detectar estos problemas y entender por qu√© ocurren. Te damos un abrazo por adelantado: been there, done that.\nHay dos dependencias un poco especiales: si usamos pipes, ‚Äúpipas‚Äù‚Äìo como quiera Dios que se traduzca al espa√±ol‚Äìen nuestras funciones, en lugar de indicar # @importFrom dplyr %&gt;% en cada funci√≥n, podremos ejecutar use_pipe() y esta funci√≥n se encargar√° de incluir esta dependencia en la documentaci√≥n por nosotros. Lo har√° en un script llamado con el nombre de nuestro paquete, en la carpeta R/, que crear√° autom√°ticamente. Si alguna de nuestras funciones devuelve un objeto en forma de data.frame y queremos usar la funci√≥n tibble, del paquete tibble para que quede mejor, tambi√©n podemos usar la funci√≥n use_tibble(), que har√° lo mismo que use_pipe(), aladiendo tibble a la lista de dependencias.\nPor √∫ltimo, es recomendable mantener las dependencias de paquetes al m√≠nimo. Cada vez que incluimos una dependencia corremos el riesgo de que alguno de los paquetes de los que dependemos cambie de versi√≥n y nuestro paquete deje de funcionar porque una de las funciones de ese paquete que utilizamos ha cambiado. CRAN impone un l√≠mite de dependencias en 20."
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#datos-internos-datos-externos-datos-brutos",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#datos-internos-datos-externos-datos-brutos",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "Datos internos, datos externos, datos brutos",
    "text": "Datos internos, datos externos, datos brutos\nMuchos paquetes incluyen objetos del tipo data.frame. Por ejemplo, el paquete {dplyr} contiene el objeto starwars. Este objeto es un data.frame con informaci√≥n sobre muchos personajes del universo de Star Wars. Podemos acceder a este objeto as√≠ dplyr::starwars.\n\n\n# A tibble: 6 √ó 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky‚Ä¶    172    77 blond      fair       blue            19   male  mascu‚Ä¶\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu‚Ä¶\n3 R2-D2         96    32 &lt;NA&gt;       white, bl‚Ä¶ red             33   none  mascu‚Ä¶\n4 Darth Va‚Ä¶    202   136 none       white      yellow          41.9 male  mascu‚Ä¶\n5 Leia Org‚Ä¶    150    49 brown      light      brown           19   fema‚Ä¶ femin‚Ä¶\n6 Owen Lars    178   120 brown, gr‚Ä¶ light      blue            52   male  mascu‚Ä¶\n# ‚Ñπ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nEste objeto est√° documentado: si ejecutamos ?dplyr::starwars se abrir√° el panel de documentaci√≥n de RStudio. Este tipo de objetos son muy √∫tiles para mostrar ejemplos de uso de las funciones de un paquete. Algunos paquetes incluso han sido dise√±ados con el √∫nico prop√≥sito de compartir una base de datos documentada, como por ejemplo el paquete {palmerpenguins}, que apenas contiene el objeto penguins: una base de datos sobre la anatom√≠a demlos ping√ºinos de la Isla de Palmer. En esta secci√≥n veremos c√≥mo incluir una base de datos como √©sta en nuestro paqeute de R y c√≥mo documentarla.\nDatos internos y externos\nPodemos incluir un data.frame por defecto en nuestro paquete de dos formas: como un objeto interno o como un objeto externo. Hacerlo de la primera forma es el equivalente a crear una funci√≥n sin exportarla al NAMESPACE: nos sirve para utilizarla dentro de las funciones internas del paquete, pero no ser√° inmediatamente visible para quien cargue el paquete usando library (aunque igualmente podr√° hacerlo usando el operador :::). Cuando incluimos el data.frame como objeto externo, s√≠ se podr√° acceder a √©l cuando cargemos el paquete (o mediante el operador ::).\nPara guardar un data.frame de cualquiera de las dos formas, primero debemos tenerlo definido entre las variables de nuestra sesi√≥n de R. Por ejemplo, sup√≥n que hemos definido el data.frame clima (disponible en el paquete psicotuiteR):\n\n\n# A tibble: 6 √ó 37\n     id lugar  genero     psico1    psico2 psico3 tw1    tw2   tw_pers tw_divulg\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt;     &lt;int&gt;\n1     1 Europa Mujer      Posgrado       3      4 Mas d‚Ä¶ Vari‚Ä¶       4         3\n2     2 Europa Hombre     Posgrado       1      5 Mas d‚Ä¶ Vari‚Ä¶       4         2\n3     3 Europa No binarie Posgrado       4      3 Menos‚Ä¶ Vari‚Ä¶       4         2\n4     4 Europa Hombre     Posgrado       4      3 Mas d‚Ä¶ Vari‚Ä¶       5         5\n5     5 Europa Hombre     Posgrado       5      1 Mas d‚Ä¶ Vari‚Ä¶       4         2\n6     6 Europa Hombre     Profesion      5      3 Mas d‚Ä¶ Vari‚Ä¶       1         4\n# ‚Ñπ 27 more variables: tw_anonim &lt;int&gt;, tw_tuits &lt;int&gt;, tw_hilos_div &lt;int&gt;,\n#   tw_hilos_ot &lt;int&gt;, tw_rt &lt;int&gt;, tw_like &lt;int&gt;, tw_resp &lt;int&gt;,\n#   exp_tw1 &lt;int&gt;, exp_tw2 &lt;int&gt;, exp_tw3 &lt;int&gt;, exp_tw4 &lt;int&gt;, comp1 &lt;chr&gt;,\n#   comp2_verg &lt;int&gt;, comp2_quediran &lt;int&gt;, comp2_calidad &lt;int&gt;,\n#   comp2_error &lt;int&gt;, comp2_conf &lt;int&gt;, comp2_nunca &lt;int&gt;, comp3_divulg &lt;int&gt;,\n#   comp3_apren &lt;int&gt;, comp3_deb_cien &lt;int&gt;, comp3_deb_poli &lt;int&gt;,\n#   comp3_pers &lt;int&gt;, comp3_nunca &lt;int&gt;, psico_tw1 &lt;chr&gt;, psico_tw2 &lt;int&gt;, ‚Ä¶\n\n\nUtilizaremos la funci√≥n use_data() del paquete usethis para incluirlo en nuestro paquete. Si queremos exportarlo al NAMESPACE especificaremos internal = FALSE en los argumentos de la funci√≥n (opci√≥n por defecto). Si no queremos exportarlo especificaremos internal = TRUE:\nuse_data(clima) # objeto externo\nuse_data(clima, internal = TRUE) # objeto interno\nEsta funci√≥n crear√° (en caso de que no exita ya) una carpeta en nuestro directorio llamada data/ y guardar√° el data.frame como un archivo .rds (como un objeto de R), en nuestro caso lo guardar√° con el nombre clima.rds. En el caso de que el objeto ya exista, debemos incluir overwrite = TRUE en los argumentos para que nos permita sobreescribirlo.\nuse_data() tambi√©n crear√° un archivo llamado clima.R en la carpeta R. Como la extensi√≥n indica, este archivo es un script de R como el que usamos para las funciones. Debemos documentar nuestros datos en este archivo usando Roxigen, tal y como hacemos para las funciones. Echa un vistazo al archivo clima.R del paquete psicotuiteR. Ver√°s que s√≥lo contiene documentaci√≥n, excepto por la l√≠nea final, que contiene \"clima\". No necesita nada m√°s que el nombre del objeto bajo el cual exportaremos el data.frame. De lo dem√°s se encargar√°, como siempre la funci√≥n document().\nDatos brutos\nHay una forma m√°s de incluir datos, aun m√°s reproducible que la anterior: incluir los datos brutos, como un archivo .csv, .txt, .tsv, .xlsx, etc. Tambi√©n podemos incluir en nuestro paquete el script de R con el c√≥digo de que hemos usando para procesar los datos contenidos en el archivo y para llegar a la forma final del objeto que guaradamos mediante use_data(). La funci√≥n use_raw_data() del paquete usethis se encarga de esto.\nuse_raw_data()\nEsta funci√≥n crear√° dos carpetas (en caso de que no existan ya): inst/ y data-raw/. La carpeta inst/ (abreviatura de installation) de un paquete de R incluye archivos externos (por ejemplo, .txt, scripts de Python, Stan, C++‚Ä¶) que queremos que est√©n disponibles cuando alguien cargue el paquete, pero no son archivos que normalmente se incluyan en un paquete. Es recomendable guardar el archivo con nuestros datos brutos en la carpeta inst/. En la carpeta data-raw/ se crear√° un archivo con la extensi√≥n .R. En este archivo (que se abrir√° autom√°ticamente cuando ejecutemos use_data_raw()) escribiremos el c√≥digo que procesa los datos y los deja como queremos que se guarden en el paquete. La √∫ltima l√≠nea del script (incluida por defecto) guarda el objeto resultante como datos externos mediante la funci√≥n use_data(). As√≠, cada vez que queramos actualizar el objeto, s√≥lo tendremos que ejecutar este c√≥digo.\nArchivos externos e inst/\nComo hemos mencionado, cualquier archivo que queramos incluir en nuestro paquete y no tenga la extensi√≥n .R o .rds, deber√≠a estar dentro de la carpeta inst/. Cuando alguien instale el paquete, los archivos de esta carpeta se mover√°n al directorio principal del paquete. Puedes hacer la prueba ejecutando la funci√≥n install() de devtools. Cuando lo hayas hecho ve a la carpeta del paquete (recuerda que los paquetes se instalan en la primera ruta que indique .libPaths()). Cualquier archivo que hayas dejado en la carpeta inst/ aparecer√° en el directorio principal ahora. A veces querremos acceder a estos archivos dentro de nuestras funciones. Esto puede ser un poco complicado. La pr√°ctica m√°s recomendable es hacerlo usando la funci√≥n:\nsystem.file(\"clima.csv\", package = \"psicotuiteR\")`\nEn la l√≠nea de c√≥digo anterior, estaremos accediendo al archivo clima.csv, que hemos incluido en la carpeta inst/, pero que al instalar el paquete se encontrar√° en ~/Documents/R/win-library/4.0 (al menos en mi ordenador).\n\n\n\n\n\n\nImportant\n\n\n\nSi la funci√≥n no encuentra ese archivo devolver√° \"\", en lugar de un error. Esto puede llevar a confusi√≥n. Si quieres que la funci√≥n devuelva un error si no encuentra el archivo, a√±ade mustWork = TRUE a los argumentos:\nsystem.file(\"clima.csv\", package = \"psicotuiteR\", mustWork = TRUE)`"
  },
  {
    "objectID": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#manteniendo-y-compartiendo-el-paquete",
    "href": "blog/creando-paquete-r-psicotuiter/creando-paquete-r-psicotuiter.html#manteniendo-y-compartiendo-el-paquete",
    "title": "Creando un paquete de R: una gu√≠a informal",
    "section": "Manteniendo y compartiendo el paquete",
    "text": "Manteniendo y compartiendo el paquete\nGit y GitHub\nUn paquete de R puede volverse complejo en poco tiempo: scripts con muchas funciones, funciones que dependen de otras funciones, funciones dependen de funciones de otros paquetes‚Ä¶ Es f√°cil liarla. Git es una buena herramienta para controlar c√≥mo va cambiando el paquete. Te permite llevar la cuenta de c√≥mo ha cambiado cada archivo dentro del paquete, poder volver a una versi√≥n espec√≠fica del mismo archivo, o tener diferentes versiones del mismo paquete funcionando a la vez de forma independiente. Este √∫ltimo punto es especialmente √∫til si queremos ‚Äújugar‚Äù con una versi√≥n de prueba del paquete mientras otras personas se pueden descargar una versi√≥n estable del mismo. Todas estas utilidades se conocen como control de versiones, una versi√≥n sofisticada y menos cortoplacista de crear un mill√≥n de archivos de similar contenido y nombres incrementalmente m√°s creativos con el fin de no perder contenido.\nPor otro lado, GitHub es una red social que permite almacenar y compartir repositorios mediante control de versiones mediante Git9. Varias personas pueden acceder al repositorio cuando est√° alojado en GitHub (un paquete de R, en nuestro caso) y sugerir cambios como si estuvieran trabajando sobre dicho paquete en un s√≥lo ordenador. Por desgracia, el uso de git o GitHub est√° fuera del alcance de este tutorial por varios motivos10. El mejor manual que conozco (y tambi√©n el m√°s accesible) para aprender a usar Git y GitHub (especialmente para quien ya trabaja en R) es Happy Git and GitHub for the useR, de Jenny Bryan. Aprender Git no siempre es f√°cil pero siempre merece la pena11.\n9¬†No es la √∫nica plataforma disponible para hacer esto: Gitlab y Bitbucket, entre otras, hacen lo mismo, aunque son menos populares. Adem√°s, por si esto calma alguna conciencia inquita, no son propiedad de Microsoft, al contrario de GitHub.10¬†El primero de todos siendo que no tengo la confianza suficiente como para hacerlo (yo mismo la l√≠o con Git cada d√≠a). El segundo es que aunque stuviera esa confianza, me dar√≠a infinita pereza hacerlo. Hacedme caso y echad un vistazo el libro que recomiendo.11¬†En mi honesta, humilde, ignorable opini√≥n.12¬†M√°s que un paquete de R, devtools es una collecci√≥n de funciones de otros paquetes que han sido agrupadas por su utilidad a la hora de desarrollar paquetes (‚Äúdevtools‚Äù es la abreviatura de developer tools). La funci√≥n install_github pertenece, originalmente, al paquete {remotes}Git y GitHub cumplen una funci√≥n muy especial para quienes hacemos un paquete en R: la funci√≥n install_github, del paquete {devtools}12, permite instalar un paquete sin necesidad de que est√© publicado en CRAN. Hablaremos en otro tutorial sobre CRAN, pero por ahora nos interesa saber que podemos compartir cualquier paquete a trav√©s de GitHub usando install_github, pero para poder instalarlo usando install.packages, como normalmente hacemos, ese paquete necesita estar publicado en CRAN. Publicar en CRAN requiere un proceso de revisi√≥n que en ocasiones es d√≠ficil solventar (y a veces innecesario). Para compartir nuestro paquete sin necesidad de pasar por ese calvario, lo haremos a trav√©s de GitHub.\nPara hacerlo, primero debemos crear un usuario de GitHub, crear un nuevo repositorio, hacer click en el bot√≥n verde que dice Code y finalmente copiar el enlace que aparece.\nEn nuestra sesi√≥n de R, ejecutamos las siguientes l√≠neas de c√≥digo:\nuse_git() # esta l√≠nea inicializa Git en el repositorio\nuse_git_remote(name = \"origin\", url = \"https://github.com/gongcastro/psicotuiteR.git\") # sustituye ese link por el que hayas copiado de GitHub\nuse_github_ignore() # crea un archivo llamado .gitignore que indica a Git qu√© archivos ignorar\ngit_vaccinate() # a√±ade m√°s cosas a .gitignore para evitar subir informaci√≥n sensible a GitHub\nPoniendo a prueba el paquete con {testthat} y test()\nLos cambios que introducimos en nuestras funciones de R pueden provocar que fallos que a veces no detectamos inmediatamente. Algunos fallos no producen un error, sino que hacen que nuestras funciones se comporten de forma diferente a la que esperamos. Por ejemplo, un data.frame que devuelve nuestra funci√≥n podr√≠a contener una variable con una clase character en lugar de logical. Este comportamiento indeseado podr√≠a pasar desapercibido cuando probemos las funciones que hemos cambiado. Para detectar estos problemas debemos poner a prueba todo el c√≥digo cada vez que hacemos cambios. Hacer esto de forma manual cada vez puede ser muy tedioso. El paquete {testthat} se encarga de hacer esto por nosotros.\nSi ejecutamos use_testhat, se crear√° una carpeta llamada tests en nuestro repositorio principal. Dentro de esta carpeta, hay otra carpeta llamada testthat. Cualquier script de R que guardemos en esa carpeta se ejecutar√° autom√°ticamente cuando ejecutemos test() en nuestra consola. Estos scripts deber√≠an seguir tener el siguiente contenido:\ntest_that(\"Los datos de clima se cargan correctamente\", {expect_error(data(\"clima\"), NA)})\nEn esta l√≠nea de c√≥digo estamos creando un test mediante la funci√≥n test_that (del paquete {testhat}). Primero incluimos un mensaje que indique qu√© estamos ‚Äútesteando‚Äù en espec√≠fico (en este caso, que podemos cargar el dataset clima sin error). La funci√≥n expect_error ejecuta el c√≥digo de dentro, y eval√∫a si el resultado se corresponde con lo que indiquemos en el segundo argumento (en nuestro caso NA, que significa que no hay fallo). Para ver con m√°s detalle c√≥mo poner a prueba el paquete que has creado y aprender buenas pr√°cticas en este tema puedes ver la documentaci√≥n del paquete {testhat}."
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "",
    "text": "The ubiquity of masks has given psycholinguists a frequent-ish stimulus to use in experiments. This word is more form-similar across languages than one may think. I gathered a big-ish dataset with translation equivalents of the word mask across ~110 languages. I tweeted about this today, and wanted to dedicate some more lines to nuance.\nHere‚Äôs the data:\nLanguage\nOrthography\n\nOrthography\n(romanisation)\n\nPhonology (IPA)\n\n\n\nAfrikaans\nmasker\nmasker\n/m…ëÀêsk/\n\n\nAlbanian\nmask√´\nmask√´\n/maska…æi ùa/\n\n\nAmharic\n·å≠·àù·â•·àç\nch'imibili\n/t É'imibili/\n\n\nArabic\nŸÇŸÜÿßÿπ\nqunae\n/qinaÀê ï/\n\n\nArmenian\n’§’´’¥’°’Ø\ndimak\n/dim…ëk/\n\n\nAzerbaijani\nmaska\nmaska\n/maska/\n\n\nBangla\n‡¶Æ‡ßÅ‡¶ñ‡ßã‡¶∂\nmukhosh\n-\n\n\nBasque\nmaskara\nmaskara\n/maska…æa/\n\n\nBelarusian\n–º–∞—Å–∫–∞\nmaska\n/maska/\n\n\nBengali\n‡¶Æ‡¶æ‡¶∏‡ßç‡¶ï\nmƒÅska\n/mask/\n\n\nBosnian\nmaska\nmaska\n/maska/\n\n\nBulgarian\n–º–∞—Å–∫–∞\nmaska\n/mask…ô/\n\n\nCatalan\nmascareta\nmascareta\n/m…ôsk…ô…æ…õt…ô/\n\n\nCebuano\nmaskara\nmaskara\n/maska…æa/\n\n\nChichewa\namabisa\namabisa\n/ama…ìisa/\n\n\nChinese Simplified\nÊâãÊúØÂè£ÁΩ©\nsh«íush√π k«íuzh√†o\n/ Ç…ôÃåu Ç ∑uÃÄ k ∞…ôÃåu à ÇaÃÄu/\n\n\nChinese Traditional\nÊâãË°ìÂè£ÁΩ©\nsh«íush√π k«íuzh√†o\n/ Ç…ôÃåu Ç ∑uÃÄ k ∞…ôÃåu à ÇaÃÄu/\n\n\nCorsican\nmaschera\nmaschera\n/maske…æa/\n\n\nCroatian\nmaska\nmaska\n/m√†ska/\n\n\nCzech\nmaska\nmaska\n/maska/\n\n\nDanish\nmaske\nmaske\n/masg…ô ¬†/\n\n\nDutch\nmasker\nmasker\n/m…ësk…ôr/\n\n\nEnglish\nmask\nmask\n/m…ëÀêsk/\n\n\nEsperanto\nmasko\nmasko\n/masko/\n\n\nEstonian\nnaamio\nnaamio\n/naÀêmio/\n\n\nTagalog (Filipino)\nmaskara\nmaskara\n/maska…æa/\n\n\nFinnish (Suomi)\nnaamio\nnaamio\n/n…ëÀêmio/\n\n\nFinnish (Suomi)\nmaskara\nmaskara\n/m…ësk…ër…ë/\n\n\nFinnish (Suomi)\nripsiv√§ri\nripsiv√§ri\n/ripsiÀå ã√¶ri/\n\n\nFrench\nmasque\nmasque\n/m…ëÀêsk/\n\n\nFrisian\nmasker\nmasker\n/masker/\n\n\nGalician\nm√°scara\nm√°scara\n/maska…æa/\n\n\nGeorgian\n·Éú·Éò·É¶·Éê·Éë·Éò\nnighabi\n/ni…£…ëbi/\n\n\nGerman\nMaske\nmaske\n/mask…ô/\n\n\nGreek\nŒºŒ¨œÉŒ∫Œ±\nm√°ska\n/maska/\n\n\nGujarati\n‡™Æ‡™π‡´ã‡™∞‡´Å‡™Ç\nmahor≈©\n-\n\n\nHausa\nabin rufe fuska\nabin rufe fuska\n-\n\n\nHawaiian\npale maka\npale maka\n-\n\n\nHebrew\n◊û◊°◊õ◊î\nmasekh√°h\n-\n\n\nHindi\n‡§Æ‡•Å‡§ñ‡•å‡§ü‡§æ\nmukhauta\n-\n\n\nHungarian\nmaszk\nmaszk\n/m…ísk/\n\n\nIcelandic\ngr√≠ma\ngr√≠ma\n/kriÀêma/\n\n\nIgbo\nmkpu\nmkpu\n/mkkÕ°p~…ìÃ•u/\n\n\nIndonesian\ntopeng\ntopeng\n/top…õ≈ã/\n\n\nIndonesian\nmasker\nmasker\n/mask…ôr/\n\n\nIrish\nmasc\nmasc\n/m…ëÀêsk/\n\n\nItalian\nmaschera\nmascherina\n/maske…æina/\n\n\nJapanese\n„Éû„Çπ„ÇØ\nmasuku\n/mas…Øk…Ø/\n\n\nJavanese\nmask\nmask\n-\n\n\nKannada\n‡≤Æ‡≤∏‡≥Å‡≤ï‡≥Å\nmukhavƒÅ·∏ça\n-\n\n\nKazakh\n–º–∞—Å–∫–∞\nmaska\n/maska/\n\n\nKhmer\n·ûö·ûî·û∂·üÜ·ûÑ\nrbang\n-\n\n\nKinyarwanda\nagapfukamunwa\nagapfukamunwa\n/agapfukamu≈ãwa/\n\n\nKorean\nÎßàÏä§ÌÅ¨\nmaseukeu\n/maÃ†s ∞…Økx…Ø/\n\n\nKurdish (Kurmanji)\nberr√ª\nberr√ª\n/beru/\n\n\nKyrgyz\n–º–∞—Å–∫–∞\nmaska\n-\n\n\nLao\n‡∫´‡∫ô‡ªâ‡∫≤‡∫Å‡∫≤‡∫Å\nnƒÅ kƒÅk\n-\n\n\nLatvian\nmaska\nmaska\n/maska/\n\n\nLithuanian\nkaukƒó\nkaukƒó\n/k√¢Àë äÃØke/\n\n\nLuxembourgish\nmask\nmask\n/mask/\n\n\nMacedonian\n–º–∞—Å–∫–∞\nmaska\n/maska/\n\n\nMalay\ntopeng\ntopeng\n/top…õ≈ã/\n\n\nMalay\n⁄©ÿØŸàŸÇ\nkedok\n/kedok/\n\n\nMalayalam\n‡¥Æ‡¥æ‡¥∏‡µç‡¥ï‡µç\nmƒÅsk\n-\n\n\nMaltese\nmaskra\nmaskra\n/maskra/\n\n\nMƒÅori¬†\nmaruhƒÅ\nmaruhƒÅ\n/ma…æuha/\n\n\nMarathi\n‡§≤‡§™‡§µ‡•Ç\nlapav≈´\n-\n\n\nMongolian\n–º–∞—Å–∫\nmask\n/mask/\n\n\nMyanmar (Burmese)\n·Äô·Äª·ÄÄ·Ä∫·Äî·Äæ·Ä¨·Äñ·ÄØ·Ä∂·Ä∏\nmyakhnahpum\n/mj…õ înÃ•…ôp ∞√≥ ä…¥/\n\n\nNepali\n‡§Æ‡•Å‡§ñ‡§µ‡§ü‡§æ\nmukhava·π≠ƒÅ\n-\n\n\nNorwegian\nmaskara\nmaskara\n/maskara/\n\n\nOdia\n‡¨Æ‡¨æ‡¨∏‡≠ç‡¨ï\nmƒÅska\n-\n\n\nPashto\nŸÖÿßÿ≥⁄©\nm√¢sk-h√¢\n-\n\n\nPersian (Farsi)\nŸÜŸÇÿßÿ® ÿ≤ÿØŸÜ\nmask\n/mask/\n\n\nPolish\nmaska\nmaska\n/mask…îÃÉ/\n\n\nPortuguese\nm√°scara\nm√°scara\n/maska…æa/\n\n\nPunjabi\n‡®Æ‡®æ‡®∏‡®ï\nmƒÅsaka\n-\n\n\nRomanian\nmasca\nmasca\n/maska/\n\n\nRussian\n–º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å\nmaskirovat'\n/m…ôsk ≤…™r…êÀàvat ≤/\n\n\nSamoan\nufimata\nufimata\n-\n\n\nScots Gaelic\nmasg\nmasg\n/mas…°/\n\n\nSerbian\n–º–∞—Å–∫a\nmaska\n-\n\n\nSesotho\npata\npata\n-\n\n\nShona\nchifukidzo\nchifukidzo\n-\n\n\nSindhi\nŸÖÿßÿ≥⁄™\nnutarian\n-\n\n\nSinhala\n‡∑Ä‡∑ô‡∑É‡∑ä‡∂∏‡∑î‡∑Ñ‡∑î‡∂´\nvesmuhu·πáa\n-\n\n\nSlovak\nmaskova≈•\nmaskova≈•\n/maskovat/\n\n\nSlovenian\nmaska\nmaska\n/maska/\n\n\nSomali\nmaaskaro\nmaaskaro\n-\n\n\nSpanish\nm√°scara\nm√°scara\n/maska…æa/\n\n\nSundanese\ntop√©ng\ntop√©ng\n/top…õ≈ã/\n\n\nSwahili\nmask\nmask\n/mask/\n\n\nSwedish (Svenska)\nmask\nmask\n/mask/\n\n\nTajik\n–Ω–∏“õ–æ–±\nniqo ô\n-\n\n\nTamil\n‡ÆÆ‡ØÅ‡Æï‡ÆÆ‡ØÇ‡Æü‡Æø\nmukam≈´·π≠i\n-\n\n\nTatar\n–º–∞—Å–∫–∞\nmaska\n/maska/\n\n\nTelugu\n‡∞Æ‡±Å‡∞∏‡±Å‡∞ó‡±Å\nmusugu\n-\n\n\nThai\n‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å\nn√¢ak√†ak\n/naÀêÀ•À©kaÀêkÃöÀ®À©/\n\n\nTurkish\nmaske\nmaske\n/maskÃü ∞e/\n\n\nTurkmen\nmaska\nmaska\n-\n\n\nUkrainian\n–º–∞—Å–∫—É–≤–∞—Ç–∏\nmaskuvaty\n/m…ësk…ê/\n\n\nUrdu\nŸÖÿßÿ≥⁄©\nmask\n-\n\n\nUyghur\nmask\nmask\n-\n\n\nUzbek\nniqob\nniqob\n-\n\n\nVietnamese\nkh·∫©u trang\nkh·∫©u trang\n/k ∞…ôwÀ®À©À¶  àaÀê≈ãÀßÀß/\n\n\nWelsh\nmwgwd\nmwgwd\n/m ä…° äd/\n\n\nXhosa\nimaski\nimaski\n-\n\n\nYiddish\n◊û◊ê÷∑◊°◊ß◊¢\nmaske\n-\n\n\nYoruba\nboju-ab·∫π\nboju-ab·∫π\n-\n\n\nZulu\nimaski\nimaski\n-\n\n\n\nhttps://drive.google.com/file/d/18SeJTiM2-JXR9SOqEg22wdkvNL3OxG3u/view?usp=sharing\nTo compute the similarity of each pair of translation equivalents, I followed Floccia et al.‚Äôs (2018) procedure. For each pair of translation equivalents, I computed their Levenshtein distance as the number of insertions, deletions and replacements a string character has to go through to become identical to the other, and then divided this value by the number of characters of the longest of the two strings, so that all values range between 0 and 1. To compute the Levenshtein distance, I used the stringdist() function of the stringdist R package."
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#orthographic-distance",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "Orthographic distance",
    "text": "Orthographic distance\nI first computed the orthographic distance between each pair of translation equivalents. Since some word forms make use of different alphabets, I first romanised all word forms. By romanised, I mean that I searched for the transcription of each word form in the Roman alphabet, and used it as input to compute the Levenshtein distance for each pair of translation equivalents. Here‚Äôs how orthographically similar (the romanisations of) the translations of mask are (N = 110 pairs):"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#phonological-distance",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "Phonological distance",
    "text": "Phonological distance\nThe phonological similarity/distance may be more informative. This time I searched for or generated with the help of a native speaker a phonological IPA transcription of each word-form. I then used this transcription as input to compute the phonological similarity of each pair of translation equivalents. A pitfall in this process is the fact that phonemes are almost never identical across languages, so even the common phoneme /m/ could vary slightly on its pronunciation in two languages. If this difference is encoded in the IPA transcription (as different characters), the Levenshtein distance will be inflated. For this reason, I simplified some IPA transcriptions to preserve this similarity. I also removed tones. This is terribly wrong from a linguistics perspective, but it‚Äôs the only way I see to be able to play with some reliable data. Also I‚Äôm no linguist, so you have no power here.\n\nHere‚Äôs the same analysis performed on phonological transcriptions of a subset of those languages (N = 75 pairs, those I could find a reliable IPA transcription for or could find help from a native speaker):"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#onsets",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "Onsets",
    "text": "Onsets\nMost of the times, the phonological overlap comes from onset graphemes/phonemes. This is how many word-forms start with each onset:"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#some-disclaimers",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "Some disclaimers:",
    "text": "Some disclaimers:\nI tried ensuring that words referred to surgical masks (instead of other types of masks) with help from native speakers. Wrong translations may still have slipped in (or be just wrong). I wish I had time to double-check all of them (I did this for fun).\nThis analysis is probably affected by selection bias. I suspect many dissimilar translations are missing due to not being included in the translation apps I used (e.g.¬†Google Translate). Feel free to contribute missing entries or make corrections!"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#code-and-data",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "Code and data",
    "text": "Code and data"
  },
  {
    "objectID": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "href": "blog/mask-similarity-across-languages/mask-similarity-across-languages.html#session-info",
    "title": "How similar is the word ‚Äúmask‚Äù across languages?",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.3.3 (2024-02-29 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] htmltools_0.5.8.1 kableExtra_1.4.0  knitr_1.48        gt_0.10.1        \n [5] readxl_1.4.3      lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n [9] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[13] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] sass_0.4.9        utf8_1.2.4        generics_0.1.3    renv_1.0.5       \n [5] xml2_1.3.6        stringi_1.8.4     hms_1.1.3         digest_0.6.36    \n [9] magrittr_2.0.3    evaluate_0.24.0   grid_4.3.3        timechange_0.3.0 \n[13] fastmap_1.2.0     cellranger_1.1.0  jsonlite_1.8.8    fansi_1.0.6      \n[17] viridisLite_0.4.2 scales_1.3.0      stringdist_0.9.12 cli_3.6.3        \n[21] rlang_1.1.4       commonmark_1.9.1  munsell_0.5.1     withr_3.0.0      \n[25] yaml_2.3.9        parallel_4.3.3    tools_4.3.3       tzdb_0.4.0       \n[29] colorspace_2.1-0  vctrs_0.6.5       R6_2.5.1          lifecycle_1.0.4  \n[33] htmlwidgets_1.6.4 pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.5     \n[37] glue_1.7.0        systemfonts_1.1.0 xfun_0.46         tidyselect_1.2.1 \n[41] rstudioapi_0.16.0 rmarkdown_2.27    svglite_2.1.3     compiler_4.3.3   \n[45] markdown_1.13"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#tldr",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#tldr",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "TLDR",
    "text": "TLDR\n\nWe need to import several CSV or TXT files and merge them into one data frame in R. Regardless of what function we use to import the files, vectorising the operation using purrr::map in combination with do.call or dplyr::bind_rows is the most time-efficient method (~25 ms importing 50 files with 10,000 rows each), compared to for loops (~220ms) or using lapply (~123 ms). data.table::fread is the fastest function for importing data. Importing TXT files is slightly faster than importing CSV files."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#why-this-post",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Why this post",
    "text": "Why this post\nTo analyse data in any programming environment, one must first import some data. Sometimes, the data we want to analyse are distributed across several files in the same folder. I work with eye-tracking data from toddlers. This means that I work with multiple files that have many rows. At 120 Hz sampling frequency, we take ~8.33 samples per second. A session for one participants can take up to 10 minutes. So these files are somewhat big. These data also tend to be messy, requiring a lot of preprocessing. This means that I need to import the same large files many times during the same R session when wrangling my way through the data, which takes a few seconds. After some iterations, it can be annoying. I have decided to invest all my lost time into analysing what method for importing and merging large files is the fastest in R so that the universe and I are even again.\nBelow I provide several options for importing data from the different files, using base R and tidyverse, among other tools. I will compare how long it takes to import and merge data using each method under different circumstances. You can find the whole code here in case you want to take a look, reproduce it or play with it1.\n1¬†Ironically, this code is super inefficient and messy. It takes ages to run, and has been written by copy-pasting multiple times. I didn‚Äôt feel like doing anything more elegant. Also, I don‚Äôt know how. Help yourself."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#how-can-i-import-large-files-and-merge-them",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "How can I import large files and merge them?",
    "text": "How can I import large files and merge them?\nSo we have some files in a folder. All files have the same number of columns, the same column names, and are in the same format. I assume that data are tabular (i.e., in the shape of a rectangle defined by rows and columns). I also assume that data are stored as Comma-Separated Values (.csv) or Tab-separated Text (.txt or .tsv), as these formats are the most reproducible.\nWe to import all files and bind their rows together to form a unique long data frame. There are multiple combinations of functions we can use. Each function comes with a different package and does the job in different ways. Next, I will show some suggestions, but first let‚Äôs create some data. We are creating 50 datasets with 10 columns and 10,000 rows in .txt format. The variables included are numeric and made of 0s and 1s. There is also a column that identifies the data set. These files are created in a temporary directory using the temp.dir function for reproducibility. After closing you R session, this directory and all of its contents will disappear.\nBase R: for loops\nfor loops are one of the fundamental skills in many programming languages. The idea behind for loops is quite intuitive: take a vector or list of length n, and apply a series of functions to each element in order. First, to element 1 and then to element 2, and so on, until we get to element n. Then, the loop ends. We will first make a vector with the paths of our files, and then apply the read.delim function to each element of the vector (i.e., to each path). Every time we import a file, we store the resulting data frame as an element of a list. After the loop finishes, we merge the rows of all element of the list using a combination of the functions do.call and rbind.\nBase R: lapply\n\nWe will use the functions read.delim and read.csv in combination with the function lapply. The former are well known. The later is part of a family of functions (together with sapply, mapply, and some others I can‚Äôt remember) that take two arguments: a list and a function, which will be applied over each element of the list in parallel (i.e., in a vectorised way).\nTidyverse\nThe tidyverse is a family of packages that suggests a workflow when working in R. The use of pipes (%&gt;%) is one of its signature moves, which allow you to chain several operations applied on the same object within the same block of code. In contrast, base R makes you choose between applying several functions to the same object in different blocks of code, or applying those functions in a nested way, so that the first functions you read are those applied the last to your object (e.g., do.call(rbind, as.list(data.frame(x = \"this is annoying\", y = 1:100)))). We will use a combination of the dplyr and purrr packages to import the files listed in a vector, using read.delim and bind_rows.\ndata.table\nThe function rbindlist function from the package data.table also allows to merge the datasets contained in a list. In combination with fread (from the same package), it can be very fast."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#what-method-is-the-fastest",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "What method is the fastest?",
    "text": "What method is the fastest?\nI will compare how long each combination of importing, vectorising, and merging functions needs to import 50 data sets with 10 columns and 10,000 rows each. Additionally, I will compare the performance of each method when working with CSV (.csv) and TSV (.txt) files. For each method, I will repeat the process 100 times, measuring how long it takes from the moment we list the extant files in the folder to the moment we finish merging the data sets. Here are the results:\n\n\n\n\n\n\n\nFigure¬†1: Mean time (and standard deviation) for each combination of methods and file formats across 100 replications\n\n\n\n\nFor more detail:\n\n\n\nTable¬†1: Execution times\n\n\n\n\n\n\n\nTime taken to import and merge\n\n\n50 datasets with 10 columns and 10,000 rows each\n\n\n\npackage\nfor loop\nlapply\npurrr::map\n\n\nM\nSD\nM\nSD\nM\nSD\n\n\n\n\ndo.call - .csv\n\n\n\nbase\n1.34\n0.06\n1.11\n0.07\n0.15\n0.03\n\n\n\ndata.table\n1.33\n0.39\n0.88\n0.04\n0.17\n0.02\n\n\n\nreadr\n1.38\n0.05\n1.07\n0.13\n0.15\n0.02\n\n\nmean\n‚Äî\n1.35\n0.17\n1.02\n0.08\n0.15\n0.02\n\n\ndplyr::bind_rows - .csv\n\n\n\nbase\n1.35\n0.16\n0.99\n0.14\n0.15\n0.01\n\n\n\ndata.table\n1.14\n0.04\n0.84\n0.06\n0.16\n0.01\n\n\n\nreadr\n1.25\n0.14\n0.91\n0.04\n0.14\n0.00\n\n\nmean\n‚Äî\n1.25\n0.11\n0.91\n0.08\n0.15\n0.01\n\n\ndo.call - .txt\n\n\n\nbase\n1.18\n0.04\n0.88\n0.04\n0.17\n0.02\n\n\n\ndata.table\n1.17\n0.04\n0.80\n0.05\n0.15\n0.02\n\n\n\nreadr\n1.18\n0.05\n0.80\n0.05\n0.15\n0.02\n\n\nmean\n‚Äî\n1.18\n0.05\n0.82\n0.05\n0.15\n0.02\n\n\ndplyr::bind_rows - .txt\n\n\n\nbase\n1.13\n0.03\n0.84\n0.06\n0.20\n0.02\n\n\n\ndata.table\n1.19\n0.13\n0.77\n0.03\n0.14\n0.01\n\n\n\nreadr\n1.13\n0.04\n0.77\n0.03\n0.14\n0.01\n\n\nmean\n‚Äî\n1.15\n0.07\n0.79\n0.04\n0.16\n0.01\n\n\n\n\n\n\n\n\n\nFigure¬†1 and Table Table¬†1 show the detailed timings The grand mean average time taken by all methods is ~2.12 seconds, but there are some differences.\n\nIt doesn‚Äôt really matter what function we use to merge data sets: both do.call and dplyr::bind_rows perform roughly similarly.\nWhat makes the biggest difference is what function we use to vectorise the importing operation across file names to import them. purrr::map is the fastest. Incredibly, is takes less than 0.3 seconds in all conditions. It is also the least sensitive to the format of the files and the function we use to import them.\nThe next vectorising function in terms of temporal efficiency is lapply, which takes ~1.5 seconds. It performs slightly better when working with .txt files, in that when working with .csv files its performance depends on what method we use to import them: data.table::fread is much faster than its base and readr competitors. This post by Daniele Cook sheds some light into the advantage of data.table over other importing functions, also covering the vroom package, which this post doesn‚Äôt cover.\nUsing for loops looks like the least efficient method for iterating across data sets when importing data. It also shows a similar profile than lapply: data.table::fread performs a bit better than the rest."
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#conclusion",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#conclusion",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Conclusion",
    "text": "Conclusion\nUnder the scenario under which I have simulated the data, it seems that using purrr::map in combination with do.call or dplyr::bind_rows to merge data sets is the most efficient method in terms of time. When using said combination, it doesn‚Äôt matter what function we use to import files, but data.table::fread seems like the best choice, as it is also the most flexible (take a look at the documentation of data.table to see all the features it offers).\nIf I have time, I may add another two dimensions: number of rows in the files and number of files, although I dare say similar results are to be expected. If anything, I would say that differences may become greater as file size and number of files increase. Also, it would be interesting to test if pre-allocating the elements of the vector in the for loop speeds up the process (see here what I mean). We shall see.\nHope this was useful, if not interesting!"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#code",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#code",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "blog/importing-data-multiple/importing-data-multiple.html#session-info",
    "href": "blog/importing-data-multiple/importing-data-multiple.html#session-info",
    "title": "Importing data from multiple files simultaneously in R",
    "section": "Session info",
    "text": "Session info\n\n\nR version 4.3.3 (2024-02-29 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.10.1         ggsci_3.2.0       lubridate_1.9.3   forcats_1.0.0    \n [5] stringr_1.5.1     readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   data.table_1.15.4 purrr_1.0.2      \n[13] dplyr_1.1.4      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      jsonlite_1.8.8    compiler_4.3.3    renv_1.0.5       \n [5] tidyselect_1.2.1  xml2_1.3.6        scales_1.3.0      yaml_2.3.9       \n [9] fastmap_1.2.0     R6_2.5.1          commonmark_1.9.1  labeling_0.4.3   \n[13] generics_0.1.3    knitr_1.48        htmlwidgets_1.6.4 munsell_0.5.1    \n[17] pillar_1.9.0      tzdb_0.4.0        rlang_1.1.4       utf8_1.2.4       \n[21] stringi_1.8.4     xfun_0.46         sass_0.4.9        timechange_0.3.0 \n[25] cli_3.6.3         withr_3.0.0       magrittr_2.0.3    digest_0.6.36    \n[29] grid_4.3.3        markdown_1.13     hms_1.1.3         lifecycle_1.0.4  \n[33] vctrs_0.6.5       evaluate_0.24.0   glue_1.7.0        farver_2.1.2     \n[37] fansi_1.0.6       colorspace_2.1-0  rmarkdown_2.27    tools_4.3.3      \n[41] pkgconfig_2.0.3   htmltools_0.5.8.1"
  }
]