{
  "hash": "0fc41b61fdd2a473517db5dd911b8dca",
  "result": {
    "markdown": "---\ntitle: \"Getting the most out of logistic regression\"\ndescription: \"Logistic regression models provide information way beyond a *p*-value. Using the {palmerpenguins} dataset, I review the relationship between the logistic and the logit functions, and how it relates to the outputs of a binomial regression model with an emphasis on marginal effects.\"\ndate: 2023-01-22\nimage: predictions-natural-probability-1.png\ncategories:\n  - r\n  - tidyverse\n  - regression\n  - statistics\n  - logistic\nformat: html\ncode-fold: true\nreference-location: margin\nwarning: false\nmessage: false\nfig-align: center\nfig-dpi: 600\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(scales)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(ggdist)\nlibrary(palmerpenguins)\n\ntheme_set(\n    theme_minimal() +\n        theme(\n            axis.line = element_line(colour = \"black\", size = 0.65),\n            panel.grid = element_blank()\n        ))\n\nclrs <- c(\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\")\n```\n:::\n\n\n\n\n# The logistic function\n\nThe logistic function is a mathematical function that defines how the values of an input variable $x$ that spans the range of real values ($x \\in (-\\infty, +\\infty)$) are associated to an output variable $y$ that spans an interval of real values contained between the $[0, 1]$ interval or narrower. This function is monotonic: for all values of $x$, $y$ increases for negative values of $x$ and increases for positive values of $x$, or the other way around. This functions takes the general form:\n\n$$\n\\text{logistic(x)} = \\frac{L}{1 + e^{(-\\beta \\ · \\ (x-x_0))}}\n$$\n\nWhere:\n\n* $L$ is the upper limit of $y$, which is commonly assumed to be 1, where $L \\in (0, +\\infty)$)\n* $\\beta$ is a coefficient that indicates the steepness of the logistic curve, or how fast the curve grows, where $\\beta \\in (-\\infty, +\\infty)$. Positive values of this coefficient lead to the logistic curve growing as $x$ gets more positive, to the logistic curve decays when $x$ gets more negative.\n* $x_0$ is the mid-point of the logistic curve, indicating the value of $x$ where the maximum of $\\beta$ is found, that is, when the logistic curve is steepest \n\nI first learnt about the logistic function and its parameters in [this blog post by TJ Mahr](https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/), in which he describes the different parameters of the function, and how they relate to its behaviour. Let's take a look at how the logistic curve looks like for different values of these parameters. We can implement the logistic curve like this:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlogistic <- function(x, slope, mid, upper) {\n    upper / (1 + exp(-slope * (x - mid)))\n}\n```\n:::\n\n\nNow we generate a logistic curve by applying `logistic()` to a vector of values `x`. We will set the steepness ($\\beta$, `slope`), mid-point ($x_0$, `mid`), and upper limit ($L$, `upper`) at different values for illustration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-5, 5, 0.1)\nslope <- c(-1, 1)\nmid <- c(-1, 1)\nupper <- c(0.7, 0.9, 1)\n\nlogistic_data <- expand_grid(x, slope, mid, upper) %>% \n    mutate(y = logistic(x, slope, mid, upper),\n           values = paste0(\"slope = \", round(slope, 2),\n                           \", mid = \", round(mid, 2)))\n\nggplot(logistic_data, aes(x, y, colour = values)) +\n    facet_wrap(~upper, labeller = labeller(upper = ~paste0(\"upper = \", .))) +\n    geom_line(size = 1.25) +\n    labs(colour = \"Parameter values\") +\n    scale_colour_manual(values = clrs[c(1, 3, 4, 5)]) +\n    theme(panel.grid = element_line(linetype = \"dotted\", colour = \"grey\"),\n          legend.position = \"top\",\n          legend.title = element_blank(),\n          legend.justification = c(\"left\", \"top\"))\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-logistic-simulation-1.png){#fig-logistic-simulation width=3600}\n:::\n:::\n\n\n\nYou might seem the logistic function in different forms. This might be because it can parametrised in slightly different ways (as we'll see later), or because someone decided to pick different symbols for the same parameters. parametrised in many ways. Sometimes, you might even find a formula for the logistic form that includes less parameters. This is because it is common to assume that some of the parameters we have seen are constant. For instance, whenever we assume that $y$ ranges from 0 to 1 (e.g., we are modelling proportions or probabilities), $L$, or the upper limit, can be assumed to be 1 (see @eq-up and note how we have replaced $L$ with 1).\n\n$$\n\\text{logistic(x)} = \\frac{1}{1 + e^{(-\\beta \\ (x-x_0))}}\n$$ {#eq-upper}\n\nIt is still not clear how @eq-upper relates to a regression model, right? The coefficients returned by our regression model---the intercept and the slope of `flipper_length_mm`---do not map perfectly into the coefficients of the logistic function in @eq-upper. These regression coefficients are meant to be in the exponential of $e$, which is $-\\beta \\ (x-x_0)$, but we must first make a simple change:\n\n$$\n\\begin{aligned}\n-\\beta \\ (x-x_0) = -(-\\beta x_0 + \\beta x) = -(\\beta_0 + \\beta_1x)\n\\end{aligned}\n$$\n\nWe substitute this element in @eq-upper, and we get:\n\n$$\n\\text{logistic(x)} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\n$$ {#eq-upper}\n\nHere, $\\beta_0 = -kx_0$ and $\\beta_1 = k$. This is how the intercept and the slope of the predictors map into the logistic regression function! The intercept indicates \n\n\n$$\n\\begin{aligned}\n\\text{logistic(x)} &= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\n\\end{aligned}\n$$ {#eq-mid}\n\nThis is called the one-parameter regression logistic function because, effectively, we only one parameter is left to be estimated: the steepness of the curve, or $\\beta$. The one-parameter logistic regression function is the one we usually encounter in logistic regression. The more general cases of the two-parameter (@eq-mid) and three-parameter (@eq-upper) logistic regression functions are more flexible and are used when one is particularly interested in estimating them explicitly in a statistical model. This [Wikipedia article](https://en.wikipedia.org/wiki/Logistic_function) suggests some historical uses of these functions. Perhaps importantly for Psychology and Cognitive Science is the use of the two- and three-parameters logistic functions in Item Response Theory (IRT).\n\n# The logit function\n\nThe *logit* scale, or the *log odds* scale, was created by Joseph Berkson in 1944 as a solution to apply linear regression to a variable whose values ranged from 0 to 1 (e.g., proportions). The name *logit* scale was derived from ***log**istic un**it***. The rationale behind this function was to transform the original variable contained in the $[0, 1]$ interval, to a set of values ranging from $-\\infty$ to $\\infty$. This way, it was possible to use linear regression to the transformed variable, and therefore hold its assumptions. You will frequently see the logit function expressed as in @eq-logit next to the clarification that $p$ is a value comprehended between $[0, 1]$, and that this function is the the inverse of the logistic function. \n\n$$\n\\text{logit(x)} = \\text{ln} \\bigg( \\frac{p}{1-p} \\bigg)\n$${#eq-logit}\n\n\nIt can be a bit unclear how we ended up with this formula. Let's try to reconstruct the process^[For a more detailed description of the derivation of the logit function from the logistic function---and also for a exhaustive explanation of logistic regression, see [this tutorial by Hause Lin](https://hausetutorials.netlify.app/posts/2019-04-13-logistic-regression/).]. The logistic function can be expressed in a slightly different way, from which the logit function, its inverse function, is derived. This is  how this alternative parametrisation can be found:\n\n$$\n\\begin{aligned}\n\\text{logistic}(x) &= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} \\\\\n&= \\bigg(\\frac{1}{1+e^{-\\beta x}} \\bigg) · \\bigg( \\frac{e^{\\beta_0 + \\beta_1x}}{e^{\\beta_0 + \\beta_1x}} \\bigg) \\\\\n& =\\frac{e^{\\beta_0 + \\beta_1x}}{e^{\\beta_0 + \\beta_1x}+1} \\\\\n&= \\frac{e^{-(\\beta_0 + \\beta_1x)}}{e^{-(\\beta_0 + \\beta_1x)} + 1} \\\\\n\\end{aligned}\n$$ {#eq-logistic-alternative}\n\nThe inverse of $e^{\\beta_0 + \\beta_1x}$ is the natural logarithm of $\\beta_0 + \\beta_1x$, that is, $\\text{ln}(\\beta_0 + \\beta_1x)$. The inverse of the logistic function as expressed in @eq-logistic-alternative, and therefore the logit function, is:\n\n$$\n\\text{logit}(x) = \\text{logistic}^{-1}(x) = \\text{ln} \\bigg( \\frac{p}{1-p} \\bigg)\n$${#eq-logistic-inverse}\n\nHere, $p$ is the outcome of the logistic function, or a value in the $[0, 1]$ interval:\n\n$$\np = \\text{logistic}(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\n$${#eq-logistic-logit}\n\n\nThere are more transformations available to make variables in the $[0, 1]$ interval suitable for linear regression, like *probit*. We won't get into them, since they fall out of the scope of this blog post. Let's see how this takes the form of logistic regression in real data.\n\n# Our dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- penguins %>% \n    filter(species==\"Adelie\") %>% \n    select(sex, flipper_length_mm, sex) %>% \n    rownames_to_column(\"id\") %>% \n    drop_na() %>% \n    mutate(sex = factor(sex, levels = c(\"female\", \"male\")),\n           flipper_length_std = scale(flipper_length_mm)[, 1])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data %>% \n    ggplot(aes(flipper_length_mm, sex, colour = sex, fill = sex)) +\n    stat_slab(trim = FALSE, colour = \"white\",\n              position = position_nudge(y = 0.3), scale = 0.5) +\n    geom_boxplot(width = 0.1, fill = \"white\", colour = \"black\", size  = 0.75,\n                 outlier.colour = NA, position = position_nudge(y = 0.3)) +\n    geom_point(shape = 1, stroke = 1, size = 2.5, alpha = 0.5,\n               position = position_jitter(height = 0.1), show.legend = FALSE) +\n    scale_colour_manual(values = clrs[c(1, 4)]) +\n    scale_fill_manual(values = clrs[c(1, 4)]) +\n    labs(x = \"Flipper length (mm)\", y = \"Sex\") +\n    guides(colour = \"none\", fill = \"none\") +\n    theme(axis.title.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-descriptives-1.png){#fig-descriptives width=4200}\n:::\n:::\n\n\n# Fitting the model\n\n$$\n\\ln \\bigg( \\frac{p}{1-p} \\bigg) = \\beta_{0} + \\beta_{1} \\times \\text{Flipper length}\n$${#eq-linear-regression}\n\nFollowing @eq-logistic-logit, we can express this regression model in the probability scale by using the logistic function and replacing the exponential bit in the denominator with our regression model:\n\n$$\np = \\text{logistic}(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 · \\ \\text{Flipper length)}}}\n$${#eq-logistic-regression}\n\nWe can implement this model using the base R function `glm(..., family = binomial(\"logit\"))` in which the `family` argument tells the function what type of generalised regression family we are interesting in using, and where `binomial(\"logit\")` indicates that we want the logistic function to be used with the logit link transformation (as opposed to probit).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfit <- glm(sex ~ flipper_length_mm, \n           data = my_data, \n           family = binomial(\"logit\"))\n```\n:::\n\n\n# Interpreting the coefficients\n\nWe have first fitted a binomial logit regression model in which we estimate the probability of a given penguin being female or male, adjusting for their flipper length in millimetres. This is a generalised linear model (GLM): after we transform the response variable (sex, coded as 0s and 1s) to the logit scale, the resulting response variable is modelled as a conventional linear model. This means that we can interpret the coefficients of this model the same way we would in a linear model. These are our coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncoef(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      (Intercept) flipper_length_mm \n      -24.1696179         0.1271495 \n```\n:::\n:::\n\n\n\nWe can generate and visualise the predictions of our model using the `predict` function. By setting `type = \"link\"`, we are asking the function to return the predictions in the function-link scale, which in our case is the logit function. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# model predictions (on the probability scale by default)\nmy_data <- mutate(my_data, sex_pred = predict(fit, type = \"link\"))\n```\n:::\n\n\n@fig-predictions-natural-logit shows the predictions of the model in the logit scale. The intercept informs us of the probability of an average penguin being male (`sex = 1`) in the logit scale when all predictors are set at zero: in our case, when `flipper_lenght_mm` = 0. The value of the intercept -24.17 is somewhat difficult to interpret for two reasons.\n\nFirst, it's in the logit scale, which is not interpretable by itself, as opposed to the probability scale. For instance, it is easy to understand what a 50% probability of a penguin being male means: if we pick one penguin at random from our population of penguins of interest, the chances of it being male is roughly the same as flipping a coin and getting tails. It is not possible to determine what a 0 logit (equivalent to 50% in the probability scale) of a penguin being male means if not by transforming it to the probability scale. For now, let's just agree that negative values on the logit scale indicate that the chance of a penguin being male is lower than it being female, that positive values on the logit scale mean that the chance of a penguin being male is higher than it being female, and than a logit of 0 indicates that a penguin is equally likely to be male or female. With this information, we can conclude that a penguin with 0mm flipper length is extremely likely to be female (-24.17 is a very large negative number in the logit scale).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    annotate(geom = \"text\", label = \"Male more likely\", x = 170, y = 2.2,\n             angle = 90, hjust = 1) +\n    annotate(geom = \"segment\", y = 0.75, yend = 2.2, x = 171, xend = 171,\n             size = 1, arrow = arrow(length = unit(0.2, \"cm\"))) +\n    annotate(geom = \"text\", label = \"Female more likely\", x = 170, y = -2.2,\n             angle = 90, hjust = 0) +\n    annotate(geom = \"segment\", y = -0.5, yend = -2.2, x = 171, xend = 171,\n             size = 1, arrow = arrow(length = unit(0.2, \"cm\"))) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male) [logit scale]\",\n         title = \"<span style = 'color:#003f5c;'>Slope (logit scale)</span>\") +\n    guides(linetype = \"none\") +\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-predictions-natural-logit-1.png){#fig-predictions-natural-logit width=4200}\n:::\n:::\n\n\nA second reason why interpreting the intercept of this model is troublesome stems precisely in that this value corresponds to the likelihood of a penguin being male **when flipper length is 0mm**. This value is uninterpretable because a flipper length of 0mm would mean that the penguin has no flippers. We are not really interested in finding out the probability of a penguin with no flippers being male. I'm not expert, but I assume that such a penguin would likely not be part of our population of interest here. Rather, we might be interested in getting the probability of being male of a penguin with some interesting flipper length, such as the **average flipper length** (190.1 in our sample). We will address how to do this later.\n\nInterpreting the slope of `flipper_length_mm` is also daunting. On the logit scale, its interpretation is fairly straightforward: the value of this coefficient (0.13) indicates how much the chance of a penguin being male increases for every unit increase in flipper length in the logit scale. Since `flipper_lenght_mm` is expressed in millimetres, we can conclude that for every millimetre increment in flipper length, the chance of the measure penguin being male increases in (0.13) logits. Again, we are not sure how to interpret this quantity, since the logit scale is not interpretable by itself. We need to transform the coefficient of `flipper_length_mm` to the probability scale to get a grasp on the actual meaning in a theoretically meaningful way.\n\n\n# The probability scale\n\nWe previously tried to interpret the coefficients of a binomial logit regression model, and encountered some difficulties. One of them related to the fact that coefficients are expressed in the logit scale. We can solve this by transforming these values---in our case the intercept and the coefficient of `flipper_length_mm`---to the probability scale. This way, the intercept will indicate the probability of a penguin being male when their flipper is 0mm long^[As we have already discussed, such penguin is unlikely to exist, and therefore this quantity will be uninteresting, but we will address this issue later.]. The coefficient of `flipper_length_mm` will tell us how much such probability increases for every millimetre increase in flipper length.\n\nTransforming values from the logit to the probability scale, however, is not very straightforward. This is because the relationship between probabilities and logits is non-linear. If you remember, we had transformed probabilities to logits for this reason precisely! In order to fit a linear model in a sensible way, we to transform our response variable (probabilities) to a scale with the right properties. One of such transformations was the logit function, which we employed. Now we have to deal with its consequences.\n\nTo transform logits to probabilities we can use the inverse logit function (see @eq-inv-logit).\n\n\n$$\n\\text{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}\n$${#eq-inv-logit}\n\nBase R conveniently offers the function `plogis()`, which we can use to transform the intercept of the model to an interpretable probability value:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nplogis(coef(fit)[\"(Intercept)\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) \n3.186166e-11 \n```\n:::\n:::\n\n\nThis quantity tells us that the probability of a penguin with flipper length equal 0mm (remember that the intercept tells us the value of the response variable when predictors are set at 0). This value, as we suspected is very close to zero: `percent(plogis(coef(fit)[\"(Intercept)\"]))`. Obviously, if male penguins tend to have longer flippers, the model will predict that penguins with zero-length flippers as extremely likely to be female! (see @fig-logit-prob-interecept). But aside from this issue that we'll deal with later, transforming the intercept from a logit model to the probability scale seems fairly simple.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob <- seq(0, 1, 0.001)\nlogit_fun <- function(x) log(x / (1 - x))\n\nd <- tibble(prob, logit = logit_fun(prob)) \n\ntxt <- str_wrap(\n    paste0(\n        \"The intercept in our model is \", \n        round(coef(fit)[\"(Intercept)\"], 2),\n        \" in the logit scale, which translates to an extremely \",\n        \"low value in the probability scale, very close to zero!\"),\n    width = 20)\n\nggplot(d, aes(logit, prob)) +\n    annotate(geom = \"segment\", x = -6, xend = -7.5, y = 0.2, yend = 0.2, size = 0.75,\n             arrow = arrow(length = unit(0.2, \"cm\"))) +\n    annotate(geom = \"text\", label = txt,\n             x = -7.5, y = 0.5, hjust = -0.1) +\n    geom_line(colour = clrs[1], size = 1) +\n    labs(x = \"Logit\", y = \"Probability\") +\n    scale_y_continuous(labels = percent) \n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-logit-prob-intercept-1.png){#fig-logit-prob-intercept width=4200}\n:::\n:::\n\n\nDoing the same with the coefficient of its predictors is a different story. This is because the intercept of the model maps into a single point in the logit scale, and therefore it does in the probability scale. There is a one-to-one correspondence between each value of the logit and the probability scales. But a regression coefficient does not refer to a value in either scale, but to a *difference in values*. Specifically, a regression coefficient tells us the estimated increase in the response variable for every unit increment in the predictor---while any other predictor is set at zero. This is true for any linear regression model, including ours. As long as we remain in the logit scale, the coefficient of the `flipper_length_mm` predictor will inform us of the increase in logits of being male for every millimetre increment in flipper length. But as soon as we move to the probability scale, this stops being true. This is because of the non-linear relationship between logits and probabilities: a difference of one logit of being male is not the same for the entire range of flipper lengths. Take a look at @fig-logit-prob-slope.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob <- seq(0, 1, 0.001)\nlogit_fun <- function(x) log(x / (1 - x))\n\nd <- tibble(prob, logit = logit_fun(prob)) \n\n# model predictions (on the probability scale by default)\nmy_data <- mutate(my_data, \n                  sex_pred = fitted(fit), \n                  sex_pred_logit = predict(fit, type = \"link\"))\n\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = plogis(predict(fit, .)))\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[2,2]-point_preds[1,2]), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[4,2]-point_preds[3,2]), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male)\",\n         title = \"<span style = 'color:#003f5c;'>Logistic curve</span>,\n        <span style = 'color:#ff6361;'>intercept</span>\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-logit-prob-slope-1.png){#fig-logit-prob-slope width=4200}\n:::\n:::\n\n\nAs you can see, a difference in the logit scale does not scale linearly to the probability scale. The difference in probability of being male between a penguin with 185mm-long flipper and a penguin with 190mm-long flipper is 0, whereas the difference in probability of being male between a penguin with 190 mm-long flipper and a penguin with 195 mm-long flipper is 0. In both cases, the difference in flipper length is the same (5 mm), but the estimated difference in the probability of being male is different!\n\nThis non-linearity in the relationship between logits and probabilities should be considered when interpreting the coefficients of a logistic regression model and, more generally, whenever the response variable has been transformed before entering the model. One strategy to ease the regression coefficients is to calculate the marginal effects of the model for specific values of the predictor.\n\n## Marginal effects\n\nDefining *marginal effect* is tricky. As it happens with many concepts and labels in statistics, the same label may be used to refer to different concepts, and several labels may be used interchangeably to refer to the same concept. Each subfield seems of science seems to use a somewhat intrinsic lexicon, which sometimes leads to some confusion. I will adopt the terminology in the documentation of the `marginaleffects` R package [@arel-bundock2023marginaleffects], in which a marginal effect is defined in the context of regression as:\n\n> [...] the association between a change in a regressor $x$ and a change in the response $y$. Put differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor $x$.\n\nAccording to this definition, calculating the marginal effect of our predictor of interest `flipper_length_mm` means extracting its slope for a specific value of the predictor. For linear regression models, this is trivial: since the relationship between the predictor and the response variable is assumed linear, the slope is considered constant across the whole range of the values of the predictor, and therefore the its marginal effect is identical for all of them. We can prove this by taking a look at the estimates of our model in the logit scale, which is linear. Let's say that we are interested in finding out the slope of `flipper_length_mm` for its average, 190.1027397. A slope is just a difference. And a difference is a derivative. And the linear regression function, $y = \\beta_0 + \\beta_1 x$, is a function that can be derived (see @eq-linear-derivative).\n\n$$\n\\begin{aligned}\ny &= \\beta_0 + \\beta_1 \\times \\text{Flipper length} \\\\\ny' &= \\beta_1 & \\text{First derivative}\n\\end{aligned}\n$$ {#eq-linear-derivative}\n\nThe derivative of the linear regression equation is a constant! This constant corresponds to the regression coefficient of `flipper_length_mm`. This means that the difference in probability of being a male penguin is going to be same for two penguins whose flippers are 180 mm and 185 mm, respectively, and for two penguins whose flippers are 190 and 195 mm, respectively. Take a look at @fig-linear-derivative. The difference in chances of being male between each pair of penguins, in the logit scale, is the same: 0.64, which corresponds to five times the `flipper_length_mm` regression coefficient because in both cases the difference in flipper length is not 1 mm but 5 mm ($5 \\times 0.1271 = 0.635$). but, again, any value in the logit scale is difficult to interpret by itself, so we are interested in translating this to the scale of probabilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predictions of model with unstandardised age\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = predict(fit, ., type = \"link\"))\n\nggplot(my_data, aes(flipper_length_mm, sex_pred_logit)) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred_logit),\n              size = 1,\n              colour = clrs[1]) +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(round(point_preds[2,2]-point_preds[1,2], 3), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(round(point_preds[4,2]-point_preds[3,2], 3), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male) [logit scale]\",\n         title = \"<span style = 'color:#003f5c;'>Slope (logit scale)</span>\") +\n    guides(linetype = \"none\") +\n    \n    \n    ggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    geom_point(colour = NA) +\n    geom_hline(yintercept = coef(fit)[\"flipper_length_mm\"],\n               size = 1, colour = \"#ffa600\") +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length [logit scale])\",\n         title = \"<span style = 'color:#ffa600;'>Derivative (logit scale)</span>\") +\n    scale_y_continuous(limits = c(0.05, 0.2)) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-linear-derivative-1.png){#fig-linear-derivative width=4200}\n:::\n:::\n\n\n\nWe have seen that regression coefficients do not behave identically for different values of their predictors when transformed to probabilities. The exact point of `flipper_length_mm` at which we calculate its marginal effect matters. This is because the derivative of the logistic function, which describes the behaviour of the probability scale we just moved to, is no longer a constant. See @eq-logistic-derivative.\n\n$$\n\\begin{aligned}\ny &= \\frac{1}{1 + e^{(-\\beta x)}} & \\text{Logistic function} \\\\\ny' &= \\frac{\\beta_1 · e^{\\beta_0 + \\beta_1 · \\ \\text{Flipper length}} }{(1 + e^{-(\\beta_0 + \\beta_1 · \\ \\text{Flipper length})})^2} & \\text{Derivative}\n\\end{aligned}\n$$ {#eq-logistic-derivative}\n\nThe derivative of the logistic function still considers the value of the predictor ($\\text{Flipper length}$), which means that the value of the derivative changes depending on such value. Let's try to visualise this. First, we are going to implement the derivative of the logistic function as an R function that computes it for the `fit` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_derivative <- function(object, x, value) {\n    slope <- coef(object)[x]\n    intercept <- coef(object)[\"(Intercept)\"]\n    numerator <- slope * exp(-(intercept + (slope * value)))\n    denominator <- (1 + exp(-(intercept + (slope * value))))^2\n    y <- numerator/denominator\n    names(y) <- NULL\n    return(y)\n}\n```\n:::\n\n\n@fig-logistic-derivative shows how the derivative changes for each value of the predictor. As you can see, the difference in probability of being male is largest at around 190 mm, while such difference decreases as `flipper_length_mm` shifts away from 190 mm.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model predictions (on the probability scale by default)\nmy_data <- mutate(my_data,\n                  derivative = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\npoint_preds <- data.frame(flipper_length_mm = c(180, 185, 190, 195)) %>% \n    mutate(sex_pred = plogis(predict(fit, .)),\n           derivate = logistic_derivative(fit, \"flipper_length_mm\", flipper_length_mm))\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    geom_segment(aes(x = point_preds[1,1], xend = point_preds[2,1],\n                     y = point_preds[1,2], yend = point_preds[1,2])) +\n    geom_segment(aes(x = point_preds[2,1], xend = point_preds[2,1], \n                     y = point_preds[1,2], yend = point_preds[2,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[2,2]-point_preds[1,2]), \" increment\"),\n             x = point_preds[2,1], y = mean(c(point_preds[1,2], point_preds[2,2])), hjust = -0.1) +\n    \n    geom_segment(aes(x = point_preds[3,1], xend = point_preds[4,1],\n                     y = point_preds[3,2], yend = point_preds[3,2])) +\n    geom_segment(aes(x = point_preds[4,1], xend = point_preds[4,1], \n                     y = point_preds[3,2], yend = point_preds[4,2])) +\n    annotate(geom = \"text\", label = paste0(percent(point_preds[4,2]-point_preds[3,2]), \" increment\"),\n             x = point_preds[4,1], y = mean(c(point_preds[4,2], point_preds[3,2])), hjust = -0.1) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"P(male)\",\n         title = \"<span style = 'color:#003f5c;'>Logistic curve</span>,\n        <span style = 'color:#ff6361;'>intercept</span>\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    \n    ggplot(my_data, aes(flipper_length_mm, \n                        logistic_derivative(fit, \"flipper_length_mm\", value = flipper_length_mm))) +\n    geom_point(colour = NA) +\n    geom_line(size = 1, colour = \"#ffa600\") +\n    geom_segment(aes(x = flipper_length_mm[which.max(derivative)],\n                     xend = flipper_length_mm[which.max(derivative)],\n                     y = min(derivative), yend = max(derivative)),\n                 colour = \"grey\", linetype = \"dotted\") +\n    annotate(geom = \"label\", label = paste0(\"Max = \", percent(max(my_data$derivative))),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = my_data$flipper_length_mm[which.max(my_data$derivative)],\n             y = max(my_data$derivative)*0.5) +\n    \n    labs(x = \"Flipper length (mm)\",\n         y = \"Slope of flipper length [logit scale])\",\n         title = \"<span style = 'color:#ffa600;'>Derivative</span>\") +\n    scale_y_continuous(labels = percent) +\n    \n    plot_layout() &\n    plot_annotation(tag_levels = \"A\") &\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/fig-logistic-derivative-1.png){#fig-logistic-derivative width=4200}\n:::\n:::\n\n\nThe maximum change in male probability is 3%, which occurs at around 190 mm flipper length. There is a smarter way of calculating the maximum slope of `flipper_length_mm`. This value will always occur at the mid-point of the logistic curve, and it turns out that to find the derivative of the logistic function at the mid-point (i.e., for $x = x_0$, go back to @eq-logistic for a reminder), we only need to find $\\beta /4$, where $\\beta$ is the regression coefficient of our predictor of interest! This is called the divide-by-four-rule, and is a simple trick to report the coefficients of a logistic regression model in the scale of probabilities, and for meaningful values of the predictors (the value at which the slope is maximum). This way, we could just divide the regression coefficient of `flipper_length_mm` by four to get the maximum probability difference of being male between two penguins with flipper lengths $x$ and $x + 1$.\n\n::: {.callout-info}\n\n## The divide-by-four rule\n\nWe have seen that dividing the coefficients of a logistic regression model (in the logit scale) gets us the maximum slope of the predictor in the probability scape. We mentioned that this has to do with the derivative of the logistic function at the mid-point. But since we dropped that term some equations ago after setting it at zero, it is no longer clear how one would derive the logistic function in such way that the divide-by-four-rule holds. Let's go back to @eq-upper, when the mid-point still appeared in our equation. We derive this formula:\n\n$$\n\\begin{aligned}\n\\text{logistic(x)} &= \\frac{1}{1 + e^{(-\\beta \\ · \\ (x-x_0))}} & \\text{Logistic function}\\\\\n\\text{logistic'(x)} &= \\frac{\\beta · e^{-\\beta (x-x_0)}}{(1 + e^{-\\beta (x-x_0)})^2} & \\text{Derivative}\n\\end{aligned}\n$$\n\nAnd since $x = x_0$, $x-x_0 = 0$. We can simplify the derivative now, knowing that $e^0 = 1$.\n\n$$\n\\begin{aligned}\n\\text{logistic'(x)} &= \\frac{\\beta · e^0}{(1 + e^0)^2} \\\\\n&= \\frac{\\beta}{(1 + 1)^2} \\\\\n&= \\frac{\\beta}{4} \\\\\n\\end{aligned}\n$$\n\nTake a look at the previously cited [blog post by TJ Mahr](https://www.tjmahr.com/anatomy-of-a-logistic-growth-curve/) for a derivation that also includes the asymptote term in the logistic function.\n\n:::\n\nLet's put the divide-by-four rule. The output of the `logistic_derivative()` that we defined before should, when solved for the mid-point of the logistic curve, return an equivalent value to $\\beta_1 / 4$, where $\\beta_1 = 0.1271495$, and therefore return something close to 0.0317874. We don't know what value of `flipper_length_mm` corresponds to the mid-point. In @fig-logistic-derivative A, we calculated it my finding the value of `flipper_length_mm` for which `logistic_derivative()` returned the maximum value:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nderivative_values <- logistic_derivative(fit, \"flipper_length_mm\", my_data$flipper_length_mm)\nmy_data$flipper_length_mm[which.max(derivative_values)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 190\n```\n:::\n:::\n\n\nFrom our data, we find that the maximum slope of `flipper_length_mm` occurs at 190. But finding the mid-point this way requires us to have already calculated the derivative of the logistic function. There is an alternative way to compute this mid-point from the estimated coefficients of the regression model. An additional benefit of computing the mid-point this way, is that we are doing so by relying on model-projections, and therefore in a way that does not entirely rely on the range of values of the predictor for which we have computed the derivative of the logistic function. This method consists in @eq-midpoint-coefs.\n\n$$\nx_0 = -\\beta_0 / \\beta_1\n$$\n\nWhere $\\beta$ is the intercept of the regression model, and $\\beta_1$ is the regression coefficient of the predictor we are calculating the mid-point for. We can implement this formula in R as:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# get point in x at the inflection point (where y = 0.5)\nget_mid <- function(x) {\n    coefs <- coef(x)[-1]\n    mid <- coef(x)[1]/-coefs\n    return(mid)\n}\n\nget_mid(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   190.0882 \n```\n:::\n:::\n\nUsing this function, we find that the mid-point is located at flipper length 190.0882 mm, pretty close to what we had estimated from our data. This value can sometimes be extremely interesting! A personal experience: in my PhD, I investigated how the age at which children learn particular words is affected by participant-level and word-level characteristics. I used logistic regression to model the probability of a given child having learnt a word, adjusting for my predictors of interest, the most important of them being the age of the child. Older children are more likely to have learnt a given word than younger children. My model returned, among others, a coefficient for `age` in the logit scale, but I wasn't specially interested in it, even after having transformed it to the probability scale. I was, however, more interested in finding the value of age at which most children were learning each word, which corresponded to the mid-point of the logistic curve for the age predictor!\n\nNow that we have calculated the mid-point of our logistic function, we can finally compare the divide-by-four rule against the actual value of the derivative of the logistic function. If we solve @eq-logistic-derivative for $x_0 = 190.0882$ using `logistic_derivative(fit, \"flipper_length_mm\", 190.0882)`, we get that the mid-point is located at 0.0317874. If we use the divide-by-four rule, we get that `coef(fit)[\"flipper_length_mm\"] / 4`. Exactly the same!\n\nSo far, we have seen that the divide-by-four rule is a simple way to obtain the slope at the mid-point, which is a meaningful value: it tells us the upper limit of the distribution of the regression slope of the predictor. However, there might be other values of the predictor for which we might be interested in finding the slope of the coefficient. We might even want to compute the average of all slopes! These, an others, are different perspectives to adopt when thinking, computing, and reporting marginal effects. I cannot say anything that has not already been explained better by Andrew Heiss in his blog post: [Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are](https://www.andrewheiss.com/blog/2022/05/20/marginalia/). I will only mention that to compute the marginal effects of your model, regardless of your strategy towards reporting marginal effects (maximum slope, average marginal effects, marginal effects at specific points), or the characteristics of your model (Gaussian or binomial, logit or probit, Bayesian or frequentist, etc.), chances are that the `marginaleffects` R package will be useful. Take a look at its documentation and functions, and play with them.\n\n\n\n# Combining our findings\n\nWe have learnt how logistic regression exploits the numerical properties of the logit function to use a simple linear regression model to model probabilities, or any other variable spanning the  $[0, 1]$ interval. We have learnt that due to the non-linear relationship between probabilities and logits, we need to be careful when interpreting the outcome of the model. We have found that depending on the value of the predictor for which we find the slope in the probability scale, we may find effects of different sizes, and that we can use the divide-by-four rule to report the maximum change in probabilities associated with any change in the predictor. We have also considered how to estimate the mid-point of the logistic curve. We can express all those interested values in the following graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get point in x at the intercept of a line with slope beta/4 that passes through 0.5\n# this function assumes that lower and upper bounds of the logistic curve are 0 and 1\n# under these conditions, 0.5 will always be the inflection point of the logistic curve\nget_mid_intercept <- function(x) {\n    mid <- get_mid(x)\n    slope <- coef(x)[-1]/4\n    mid_intercept <- (-slope*mid) + 0.5\n    return(mid_intercept)\n}\n\n# predictions of model with unstandardised age\nggplot(my_data, aes(flipper_length_mm, sex_pred)) +\n    # plot observations\n    geom_point(aes(y = as.numeric(sex)-1), \n               shape = 1,\n               size = 1.5,\n               stroke = 1,\n               position = position_jitter(height = 0.1),\n               alpha = 0.75) +\n    geom_hline(yintercept = 0.5,\n               colour = \"grey\",\n               linetype = \"dotted\") +\n    geom_line(aes(x = flipper_length_mm, y = sex_pred),\n              size = 1.5,\n              colour = clrs[1]) +\n    # plot intercept (y when x = 0)\n    geom_hline(yintercept = plogis(coef(fit)[\"(Intercept)\"]),\n               linetype = \"dashed\",\n               colour = clrs[4])  +\n    # plot mid point (x when y = 0.5)\n    geom_vline(xintercept = get_mid(fit),\n               linetype = \"dashed\",\n               colour = clrs[2]) +\n    # plot slope when x = mid point (approximated derivative of the logistic curve)\n    geom_abline(\n        slope = coef(fit)[\"flipper_length_mm\"] / 4,\n        intercept = get_mid_intercept(fit),\n        size = 0.75,\n        colour = clrs[5]\n    ) +\n    annotate(geom = \"label\", label = percent(coef(fit)[\"flipper_length_mm\"] / 4),\n             fill = \"#ffa600\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = get_mid(fit)+5, y = 0.5, label.r = unit(0, \"lines\")) +\n    annotate(geom = \"label\", label = percent(plogis(coef(fit)[\"(Intercept)\"])),\n             fill = \"#ff6361\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = 200, y = plogis(coef(fit)[\"(Intercept)\"])+0.1,\n             label.r = unit(0, \"lines\")) +\n    annotate(geom = \"label\", label = round(get_mid(fit), 2),\n             fill = \"#58508d\", alpha = 0.5, colour = \"black\", label.size = 0,\n             x = get_mid(fit)-1, y = 0.6, hjust = 1,\n             label.r = unit(0, \"lines\")) +\n    labs(x = \"Flipper length (mm)\",\n         y = \"Sex\",\n         title = \"<span style = 'color:#003f5c;'>Logistic curve</span>,\n<span style = 'color:#ff6361;'>intercept</span>,\n<span style = 'color:#58508d;'>mid-point</span>,\n<span style = 'color:#ffa600;'>derivative at mid-point</span>\") +\n    guides(linetype = \"none\") +\n    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.25)) +\n    theme(legend.title = element_blank(),\n          plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-html/predictions-natural-probability-1.png){width=4200}\n:::\n:::\n",
    "supporting": [
      "logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}